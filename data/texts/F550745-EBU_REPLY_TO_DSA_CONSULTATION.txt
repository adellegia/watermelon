Contribution ID: baead061-eda1-425c-9602-874ffbee1579

Date: 07/09/2020 17:41:42

Ref. Ares(2020)4722286 - 09/09/2020

Digital Services Act package: open public 
consultation

Fields marked with * are mandatory.

Introduction

The Commission recently 

announced

 a Digital Services Act package with two main pillars:

first, a proposal of new and revised rules to deepen the Single Market for Digital 

Services, by increasing and harmonising the responsibilities of online platforms and 

information service providers and reinforce the oversight over platforms’ content policies 

in the EU;

second, ex ante rules to ensure that markets characterised by large platforms with 

significant network effects acting as gatekeepers, remain fair and contestable for 

innovators, businesses, and new market entrants.

T h i s  

c o n s u l t a t i o n

The  Commission  is  initiating  the  present  open  public  consultation  as  part  of  its  evidence-

gathering exercise, in order to identify issues that may require intervention through the Digital 

Services Act, as well as additional  topics related to the environment of digital services and 

online platforms, which will be further analysed in view of possible upcoming initiatives, should 

the issues identified require a regulatory intervention. 

The consultation contains 6 modules (you can respond to as many as you like):

1.  

How to effectively keep users safer online?

2.  

Reviewing the liability regime of digital services acting as intermediaries?

3.  

What issues derive from the gatekeeper power of digital platforms?

4.  

Other emerging issues and opportunities, including online advertising and smart 

contracts

5.  

How  to  address  challenges  around  the  situation  of  self-employed  individuals 

offering services through online platforms?

6.  

What governance for reinforcing the Single Market for digital services?

Digital services and other terms used in the questionnaire

1

          
The  questionnaire  refers  to 

digital  services

  (or  ‘information  society  services’,  within  the 

meaning of the E-Commerce Directive), as 'services provided through electronic means, at a 

distance, at the request of the user'. It also refers more narrowly to a subset of digital services 

here  termed 

online  intermediary  services

.  By  this  we  mean  services  such  as  internet 

access providers, cloud services, online platforms, messaging services, etc., i.e. services that 

generally transport or intermediate content, goods or services made available by third parties.

Parts  of  the  questionnaire  specifically  focus  on 

online  platforms

  –  such  as  e-commerce 

marketplaces,  search  engines,  app  stores,  online  travel  and  accommodation  platforms  or 

mobility platforms and other collaborative economy platforms, etc.

Other 

terms  and  other 

technical  concepts  are  explained 

in 

a  glossary

. 

H o w  

t o  

r e s p o n d

Make  sure 

to 

save  tour  draft

  regularly  as  you 

fill 

in 

the  questionnaire. 

You 

can 

break 

off 

and 

return 

to 

finish 

it 

at 

any 

time. 

At the end, you will also be able to upload a document or add other issues not covered in 

d e t a i l  

i n  

t h e  

q u e s t i o n n a i r e .  

D e a d l i n e  

f o r  

r e s p o n s e s

8  

S e p t e m b e r  

2 0 2 0 .

L a n g u a g e s

You 

can 

submit 

your 

response 

in 

any 

official 

EU 

language.

The questionnaire is available in 23 of the EU's official languages. You can switch languages 

from the menu at the top of the page.

About you

*

1 Language of my contribution

Bulgarian

Croatian

Czech

Danish

Dutch

English

Estonian

Finnish

2

 
French

Gaelic

German

Greek

Hungarian

Italian

Latvian

Lithuanian

Maltese

Polish

Portuguese

Romanian

Slovak

Slovenian

Spanish

Swedish

*

2 I am giving my contribution as

Academic/research institution

Business association

Company/business organisation

Consumer organisation

EU citizen

Environmental organisation

Non-EU citizen

Non-governmental organisation (NGO)

Public authority

Trade union

Other

*

3 First name

Sarah

*

4 Surname

Turnbull

3

*

5 Email (this won't be published)

turnbull@ebu.ch

*

7 Organisation name

255 character(s) maximum

European Broadcasting Union

*

8 Organisation size

Micro (1 to 9 employees)

Small (10 to 49 employees)

Medium (50 to 249 employees)

Large (250 or more)

9 What is the annual turnover of your company?

<=€2m

<=€10m

<= €50m

Over €50m

10 Are you self-employed and offering services through an online platform?

Yes

No

11 Would you describe your company as :

a startup?

a scaleup?

a conglomerate offering a wide range of services online?

12 Is your organisation:

an online intermediary

an association representing the interests of online intermediaries

a digital service provider, other than an online intermediary

an association representing the interests of such digital services

a different type of business than the options above

an association representing the interest of such businesses

4

other

16 Does your organisation play a role in:

Flagging illegal activities or information to online intermediaries for removal

Fact checking and/or cooperating with online platforms for tackling harmful 

(but not illegal) behaviours

Representing fundamental rights in the digital environment

Representing consumer rights in the digital environment

Representing rights of victims of illegal activities online

Representing interests of providers of services intermediated by online 

platforms

Other

17 Is your organisation a

Law enforcement authority, in a Member State of the EU

Government, administrative or other public authority, other than law 

enforcement, in a Member State of the EU

Other, independent authority, in a Member State of the EU

EU-level authority

International level authority, other than at EU level

Other

18 Is your business established in the EU?

Yes

No

19 Please select the EU Member States where your organisation is established or 

currently has a legal representative in:

Austria

Belgium

Bulgaria

Croatia

Cyprus

Czechia

Denmark

Estonia

5

Finland

France

Germany

Greece

Hungary

Ireland

Italy

Latvia

Lithuania

Luxembourg

Malta

Netherlands

Poland

Portugal

Romania

Slovak Republic

Slovenia

Spain

Sweden

20 Transparency register number

255 character(s) maximum
Check if your organisation is on the 

making.

93288301615-56

transparency register

. It's a voluntary database for organisations seeking to influence EU decision-

*

21 Country of origin

Please add your country of origin, or that of your organisation.

Afghanistan

Djibouti

Libya

Saint Martin

Åland Islands

Dominica

Liechtenstein

Saint Pierre 

Dominican 

Republic

Ecuador

Egypt

Lithuania

and Miquelon

Saint Vincent 

and the 

Grenadines

Luxembourg

Samoa

Macau

San Marino

Albania

Algeria

American 

Samoa

6

Príncipe

Saudi Arabia

Senegal

Serbia

Seychelles

Sierra Leone

Singapore

Sint Maarten

Slovakia

Slovenia

Solomon 

Islands

Somalia

Andorra

El Salvador

Madagascar

São Tomé and 

Angola

Equatorial 

Malawi

Anguilla

Antarctica

Guinea

Eritrea

Estonia

Antigua and 

Eswatini

Malaysia

Maldives

Mali

Barbuda

Argentina

Armenia

Aruba

Australia

Austria

Azerbaijan

Bahamas

Bahrain

Bangladesh

Barbados

Belarus

Belgium

Belize

Benin

Bermuda

Bhutan

Ethiopia

Malta

Falkland Islands

Marshall 

Faroe Islands

Fiji

Finland

France

Islands

Martinique

Mauritania

Mauritius

Mayotte

French Guiana

Mexico

French 

Polynesia

French 

Southern and 

Antarctic Lands

Gabon

Georgia

Germany

Ghana

Gibraltar

Greece

Greenland

Bolivia

Grenada

Bonaire Saint 

Guadeloupe

Eustatius and 

Saba

Micronesia

South Africa

Moldova

Monaco

Mongolia

South Georgia 

and the South 

Sandwich 

Islands

South Korea

South Sudan

Montenegro

Spain

Montserrat

Morocco

Sri Lanka

Sudan

Mozambique

Suriname

Myanmar

/Burma

Namibia

Nauru

Svalbard and 

Jan Mayen

Sweden

Switzerland

7

Bosnia and 

Herzegovina

Botswana

Guam

Nepal

Syria

Guatemala

Netherlands

Taiwan

Bouvet Island

Guernsey

New Caledonia

Tajikistan

Brazil

Guinea

New Zealand

British Indian 

Guinea-Bissau

Nicaragua

Tanzania

Thailand

Ocean Territory

British Virgin 

Guyana

Niger

The Gambia

Islands

Brunei

Bulgaria

Haiti

Heard Island 

and McDonald 

Islands

Nigeria

Niue

Timor-Leste

Togo

Burkina Faso

Honduras

Norfolk Island

Tokelau

Burundi

Hong Kong

Northern 

Tonga

Mariana Islands

Cambodia

Hungary

North Korea

Trinidad and 

Cameroon

Iceland

Canada

India

Cape Verde

Indonesia

Cayman Islands

Iran

North 

Macedonia

Norway

Oman

Pakistan

Tobago

Tunisia

Turkey

Turkmenistan

Turks and 

Caicos Islands

Central African 

Iraq

Palau

Tuvalu

Republic

Chad

Chile

China

Ireland

Isle of Man

Israel

Palestine

Panama

Uganda

Ukraine

Papua New 

United Arab 

Christmas 

Italy

Island

Guinea

Paraguay

Emirates

United 

Kingdom

Clipperton

Jamaica

Peru

United States

8

Pitcairn Islands

Uruguay

Poland

Puerto Rico

Vanuatu

Cocos (Keeling) 

Japan

Philippines

Islands

Colombia

Comoros

Jersey

Jordan

Congo

Kazakhstan

Portugal

Cook Islands

Costa Rica

Côte d’Ivoire

Croatia

Cuba

Kenya

Kiribati

Kosovo

Kuwait

Kyrgyzstan

Qatar

Réunion

Romania

Russia

Curaçao

Laos

Rwanda

Cyprus

Latvia

Saint 

Barthélemy

United States 

Minor Outlying 

Islands

US Virgin 

Islands

Uzbekistan

Vatican City

Venezuela

Vietnam

Wallis and 

Futuna

Western 

Sahara

Yemen

Czechia

Lebanon

Saint Helena 

Zambia

Ascension and 

Tristan da 

Cunha

Democratic 

Lesotho

Saint Kitts and 

Zimbabwe

Republic of the 

Congo

Denmark

Nevis

Liberia

Saint Lucia

*

22 Publication privacy settings

The Commission will publish the responses to this public consultation. You can choose whether you would like your details to be made 

public or to remain anonymous.
Anonymous

Only your type of respondent, country of origin and contribution will be 

published. All other personal details (name, organisation name and size, 

transparency register number) will not be published.

Public 

Your personal details (name, organisation name and size, transparency 

register number, country of origin) will be published with your contribution.

9

I agree with the personal data protection provisions

I. How to effectively keep users safer online?

This module of the questionnaire is structured into several subsections:

First,  it  seeks  evidence,  experience,  and  data  from  the  perspective  of  different  stakeholders  regarding 

illegal  activities  online,  as  defined  by  national  and  EU  law.  This  includes  the  availability  online  of  illegal 

goods (e.g. dangerous products, counterfeit goods, prohibited and restricted goods, protected wildlife, pet 

trafficking, illegal medicines, misleading offerings of food supplements), content (e.g. illegal hate speech, 

child  sexual  abuse  material,  content  that  infringes  intellectual  property  rights),  and  services,  or  practices 

that  infringe  consumer  law  (such  as  scams,  misleading  advertising,  exhortation  to  purchase  made  to 

children) online. It covers all types of illegal activities, both as regards criminal law and civil law.

It then asks you about other activities online that are not necessarily illegal but could cause harm to users, 

such as the spread of online disinformation or harmful content to minors.

It also seeks facts and informed views on the potential risks of erroneous removal of legitimate content. It 

also asks you about the transparency and accountability of measures taken by digital services and online 

platforms  in  particular  in  intermediating  users’  access  to  their  content  and  enabling  oversight  by  third 

parties.  Respondents  might  also  be  interested  in  related  questions  in  the  module  of  the  consultation 

focusing on online advertising.

Second, it explores proportionate and appropriate responsibilities and obligations that could be required 

from online intermediaries, in particular online platforms, in addressing the set of issues discussed in the 

first sub-section.

This module does not address the liability regime for online intermediaries, which is further explored in the 

next module of the consultation.

1. Main issues and experiences

A. Experiences and data on illegal activities online

Illegal goods

1 Have you ever come across illegal goods on online platforms (e.g. a counterfeit 

product, prohibited and restricted goods, protected wildlife, pet trafficking, illegal 

medicines, misleading offerings of food supplements)?

No, never

Yes, once

Yes, several times

I don’t know

3 Please specify.

3000 character(s) maximum

10

4 How easy was it for you to find information on where you could report the illegal 

good?

Please rate from 1 star (very difficult) to 5 stars (very easy)

5 How easy was it for you to report the illegal good?

Please rate from 1 star (very difficult) to 5 stars (very easy)

6 How satisfied were you with the procedure following your report?

Please rate from 1 star (very dissatisfied) to 5 stars (very 

satisfied)

7 Are you aware of the action taken following your report?

Yes

No

8 Please explain

3000 character(s) maximum

9 In your experience, were such goods more easily accessible online since the 

outbreak of COVID-19?

No, I do not think so

Yes, I came across illegal offerings more frequently

I don’t know

10 What good practices can you point to in handling the availability of illegal goods 

online since the start of the COVID-19 outbreak?

5000 character(s) maximum

Illegal content

11 Did you ever come across illegal content online (for example illegal incitement to 

violence, hatred or discrimination on any protected grounds such as race, ethnicity, 

11

 
 
 
 
 
 
 
 
 
 
 
 
gender or sexual orientation; child sexual abuse material; terrorist propaganda; 

defamation; content that infringes intellectual property rights, consumer law 

infringements)?

No, never

Yes, once

Yes, several times

I don’t know

12 What measure did you take?

I reported it to the platform via its existing reporting procedure

I contacted the online platform by other means to report the illegal content

I contacted a national authority

I contacted a consumer organisation

I did not take any action

I took a different action. Please specify in the text box below

13 Please specify

3000 character(s) maximum

EBU Members frequently come across and report illegal content, such as content which harms their 

journalists or their business reputation. Members also regularly come across content infringing their 

intellectual property rights. We do not include IP infringing content in this response.

Some members have noticed a growth in “deepfakes” using extracts of content, with their meaning changed 

or inserting images (e.g. faces of journalists) within third party content.

Our experience is that content flagged as racist or homophobic, is rapidly taken down but that this is not the 

case for all content flagged as illegal.

Members use the general notification form provided by the platforms or their direct contacts. Not all platforms 

processes are clear at the moment, and it requires internal resources to keep across new services and 

content – resources that are taken away from the development of new services and content.  

These notification forms and procedures could be better harmonized to have a uniform approach across 

platforms.

14 How easy was it for you to find information on where you could report the illegal 

content/activity?

Please rate from 1 star (very difficult) to 5 stars (very easy)

15 How easy was it for you to report the illegal content/activity?

12

 
 
 
 
Please rate from 1 star (very difficult) to 5 stars (very easy)

16 How satisfied were you with the procedure following your report?

Please rate from 1 star (very dissatisfied) to 5 stars (very 

satisfied)

17 Are you aware of the action taken following your report?

Yes

No

18 How has the dissemination of illegal content changed since the outbreak 

of  COVID-19? Please explain.

3000 character(s) maximum

EBU members have seen an increase in certain illegal practices such as phishing and the dissemination of 

hoaxes/fake news in social media, chats and certain media about political affairs and coronavirus treatment. 

Such practices have become widespread. These have taken advantage of the pandemic, the uncertainty 

from the pandemic and the resultant move online for citizens. 

Some of the platforms claim they are taking a more proactive stance to identify illegal content during the 

pandemic. However, overall there appears to have been no significant improvement.

19 What good practices can you point to in handling the dissemination of illegal 

content online since the outbreak of COVID-19?

3000 character(s) maximum

Overall EBU Members consider that there has been no significant change and therefore it is difficult to 

identify good practices.

Improvements by platforms include fast track procedures for granting consumer rights, promoting fact 

checking systems, prohibitions on third party sellers from any promotional deals on COVID-19 related 

products or from charging excessively high prices on products exploiting an emergency.

20 What actions do online platforms take to minimise risks for consumers to be 

exposed to scams and other unfair practices (e.g. misleading advertising, 

exhortation to purchase made to children)?

3000 character(s) maximum

13

 
 
 
 
 
 
 
 
Platforms mainly have tried to create transparency but have not taken enough action to avoid these 

problems since their main interest is to commercialise content and user data.  

Some platforms responded, with moderate and unequal success, to the Commission’s call to take proactive 

measures to address and prevent scams and unfair practices with measures including: reporting offensive 

comments, fact-checking systems for fake news, policies and reminders to prohibit exploitative or deceptive 

advertisements and sales of healthcare products and services, temporary prohibition of the sale of certain 

products seeking to exploit peoples fear of COVID-19 to charge higher prices, measures to limit the spread 

of misinformation and false claims related to COVID-19 and remove as much of this content as they can. 

This includes human moderation.

21 Do you consider these measures appropriate?

Yes

No

I don't know

22 Please explain.

3000 character(s) maximum

Platforms should be more active to block/delete illegal content (and fake news) and develop technical and 

other measures to prevent abusive use of programmatic and other forms of automated advertising. Citizens 

are still clearly exposed to illegal practices so measures are still insufficient. There may have been some 

improvement but clearly scams and misleading adverts and other illegal content still appear on platforms.

B. Transparency

1 If your content or offering of goods and services was ever removed or blocked 

from an online platform, were you informed by the platform?

Yes, I was informed before the action was taken

Yes, I was informed afterwards

Yes, but not on every occasion / not by all the platforms

No, I was never informed

I don’t know

2 Were you able to follow-up on the information?

Yes, I complained to the platform

Yes, I escalated to an out-of-court dispute mechanism

No, but it was useful to learn about the platform’s policy

No

Other. Please specify in the text box below

14

3 Please explain.

3000 character(s) maximum

Platforms frequently take down content without prior information to the content provider. There are no 

effective out of court dispute mechanisms. Platforms do not always provide an efficient system to report 

erroneous removal. EBU members have generally received a standardized answer with a short general 

explanation with limited additional information after challenge:•Removal by YouTube in 2019 of RTVE 

content uploaded in 2017 due to nudity during a few seconds in a drama episode of 1 hour 8 minutes. •San 

Fermin bull runs have been uploaded for years without objection but in 2019, the last San Fermin bull run 

content was removed a few minutes after being uploaded with no explanation. RTVE complained the video 

was re-uploaded only 20 days later. One month later, it was removed again. It took 10 days for YouTube to 

reinstate.•Removal of content from ZDF that was lawful under German regulation (content showing a 

comedian wearing a Swastika, and content showing nudity as part of a satirical montage).•Google Play 

rejection of the DR Ramasjang-app (children’s content made for 4-8 years). This removal was explained as 

content violating Google’s family policy requirements. Google referenced a loading-screen picture of a child 

with a liquorice sweet shaped as a ‘pipe’ (traditional candy in Denmark). The pipe was deemed in violation of 

children content. DR only found out through user complaints. Google only reinstated the original app months 

later after political pressure.•Rejection of an update due to the word “lort,” (poo in English), as this was 

considered in conflict with Google’s policies. The word for poo was used in the context of a game, where 

children can find seeds in animal excrements to plant trees. To solve this DR removed the word poo and 

only mentioned that seeds can be found and sowed. •Google recently decided that NRK’s educational 

content about puberty was a “policy violation”. They considered the program to be "sexually offensive or 

include violent content", while in contrast is regarded as educational content by NRK, Norwegian audiences 

and regulatory authorities.•The Instagram account for SR’s youth-oriented satirical show Tankesmedjan was 

removed from Facebook-owned Instagram with just an alert that an individual post had violated the Terms 

and Conditions. SR contacted Facebook and after two weeks the account was reinstated. SR has still not 

received a full explanation.  

 These examples clearly show how decisions by platforms, based on unilateral global community standards 

and terms and conditions, can have major consequences for cultural pluralism, freedom of expression and 

media freedom. Within their own cultural contexts, these examples are considered educational, entertaining 

and age appropriate. If platform operators plan to modify or remove content belonging to editorially 

responsible media companies, they should immediately provide a local contact/case handler and a full 

transparent explanation.

4 If you provided a notice to a digital service asking for the removal or disabling of 

access to such content or offering of goods or services, were you informed about 

the follow-up to the request?

Yes, I was informed

Yes, but not on every occasion / not by all  platforms

No, I was never informed

I don’t know

5 When content is recommended to you - such as products to purchase on a 

platform, or videos to watch, articles to read, users to follow - are you able to obtain 

enough information on why such content has been recommended to you? Please 

explain.

15

3000 character(s) maximum

No, EBU Members generally find that there is a lack of transparency as regards the categorization, 

recommendation, and the selection of content by platforms. 

One example is YouTube which provides no information on the way the algorithm operates either for search 

or for recommendation. Facebook’s new function “why am I seeing that” gives some indications why the 

content is recommended but no information on the functioning of the algorithm.

The same is true for other platforms – EBU Members only receive indicators of the content most likely to 

circulate or on the data taken into account by the algorithm (viewing time, last content posted).

C. Activities that could cause harm but are not, in themselves, illegal

1 In your experience, are children adequately protected online from harmful 

behaviour, such as grooming and bullying, or inappropriate content?

3000 character(s) maximum

Providing children with the opportunity to enjoy quality programming tailored to their age and with safe 

spaces online where they can watch, engage and create is very much part of the DNA of public service 

media. Public service media organisations have an outstanding track record in providing suitable content for 

children and for doing everything within their sphere of control to provide a safe viewing/online environment. 

Their editorial policy and principles are the same in all cases, should the content be transmitted on television 

or online. They make no links to the open Internet unless the webpage is safe for children. PSM also work 

closely with educational partners to encourage creativity and reflective skills in children within a trusted 

online space. 

The protection of children online is indeed a special concern as many children access content and engage 

with each other on major online platforms for which no high editorial and quality audiovisual standards apply. 

Access to Youtube, for example, is in principle prohibited or subject to conditions for minors, but as access 

to the platform does not require the creation of an account, these limitations are practically without effect and 

cannot qualify as an effective parental control tool. Advertisements and trailers associated with content 

aimed at minors can also be problematic: even if parents control the selection of content on Youtube Kids, 

inappropriate images may be shown. Against this background, measures/tools to protect minors should be 

reinforced, including parental control tools, guides to protect privacy, reporting of inappropriate content, 

bullying, abuse etc. However, technological measures will not fully protect children. We need to firm up 

commitments to promote media literacy among children and ensure that content and services suitable and 

trustworthy for children can easily be found online by children. Overall media literacy initiatives should be 

further promoted. They play a key role in fighting the information disorder and in helping users of all age 

groups to have the necessary skills to navigate an ever-changing media landscape. Public service media are 

at the forefront of offering tools and services that raise awareness of the importance of developing adequate 

skills, strengthening media literacy.

In Germany, according to the latest annual reports of jugendschutz.net (jugendschutz.net monitors internet 

content, identifies violations of youth protection laws and forwards these cases to the German Commission 

for the Protection of Minors in the Media for further action.) there is too little protection for children and young 

people online and the risk of cyberbullying, harassment and inappropriate advertising is still high. (http://www.

jugendschutz.net/en/annual-reports/ - June 2020) 

16

2 To what extent do you agree with the following statements related to online 

disinformation?

Neither 

Fully 

agree

Somewhat 

agree 

Somewhat 

Fully 

agree

not 

disagree

disagree

disagree

I 

don't 

know/ 

No 

reply

Online platforms can easily 

be manipulated by foreign 

governments or other 

coordinated groups to 

spread divisive messages

To protect freedom of 

expression online, diverse 

voices should be heard

Disinformation is spread by 

manipulating algorithmic 

processes on online 

platforms

Online platforms can be 

trusted that their internal 

practices sufficiently 

guarantee democratic 

integrity, pluralism, non-

discrimination, tolerance, 

justice, solidarity and 

gender equality.

3 Please explain.

3000 character(s) maximum

Relying solely on self-regulatory measures to tackle disinformation is no longer tenable. Decisions taken by 

online platforms can have far-reaching consequences for the exercise of freedom of expression and 

information and for media freedom and pluralism. Recent assessments of the Code of Practice on 

Disinformation show that it is time to address the need to go beyond the Code’s voluntary actions and 

consider a co-regulatory model.

Any new legislation at EU level must respect fundamental rights and support pluralism and cultural diversity 

(e.g. smaller language groups and communities). Interventions by public authorities must be proportionate to 

the threat posed by disinformation, minimizing the damaging effect on freedom of expression. Any adequate 

response to disinformation must build on close collaboration between those on the front-line fighting 'fake 

news'.

EBU has identified at least three areas for which rules are needed. 

1. Increasing the visibility for quality media content online will help diluting disinformation. Easy access to 

and prominent display of general interest content must be guaranteed on all relevant platforms. Independent 

PSM play a key role in building informed citizenship. A clear brand attribution is instrumental in tackling 

disinformation. It helps citizens to decide for themselves if they can trust a certain news/source of 

17

information. With increased polarisation of societies and increasing levels of online disinformation, the role 

played by trustworthy independent media is becoming more important than ever before. 

2. Safeguarding editorial freedom and media independence is key to safeguard public trust. Allowing global 

platforms to ban and/or erroneously remove media content and services when they clash with their unilateral 

corporate community standards creates a serious threat for editorial freedom and media pluralism. When 

media content and services that are already subject to regulation and oversight are offered on major 

platforms, platform operators shall not subject these services to any form of control or interference.  Instead, 

they should be under a positive obligation to respect such services as conceived by the media provider. 

3. Platforms’ ability to control what content appears and when, in full opacity, has far-reaching consequences 

for freedom of expression. Enhancing algorithmic transparency as well as transparency of content policies 

will help media and media users to understand why which content appears in the ranking and the news feed 

and react to it accordingly. Similarly, providing trusted fact-checkers and academia with access to platform 

data would enable them to better monitor the algorithms’ functioning and facilitate independent 

assessments. See also further below on access to data generated by or related to media content.

In addition, media literacy initiatives should be further promoted (see above).

4 In your personal experience, how has the spread of harmful (but not illegal) 

activities online changed since the outbreak of  COVID-19? Please explain.

3000 character(s) maximum

The spread of fake news and harmful content online is an ongoing phenomenon and should be analyzed 

carefully.

News and information related to the COVID-19 pandemic were subject to misleading information and 

manipulation on platforms. SVT, EBU Swedish member, observed that false press articles circulating on 

social networks and illegally using their logo, were spreading false information about the COVID-19 crisis. 

EBU Members have, however, not noticed any significant changes in the way harmful activities were spread 

online. They have, though, noticed an increase in certain platform’s videos misusing their content (mainly the 

information content). 

Free use of social media, low control barriers and the roles blogs have played in the constant spread of fake 

news and misinformation during the COVID-19 emergency have been a principal cause. The explosion of 

news around COVID-19 with much of it questionable by science with extreme political polarization are an 

example of how quickly such news can spread and escalate in the digital environment.

5 What good practices can you point to in tackling such harmful activities since the 

outbreak of COVID-19?

3000 character(s) maximum

Major online platform operators refer to practices such as the removal of misleading information about 

COVID-19, implementation tools to detect and limit it fact checking and reporting. Unfortunately, there is no 

independent verification of how successful these initiatives are. 

Public service media have responded to the COVID-19 crisis by increasing their volume of news 

18

 
programming in order to keep citizens informed about developments related to the crisis. Information was 

provided through additional and extended news bulletins, as well as dedicated coronavirus current affairs 

shows and talk shows. Audiences for PSM evening news bulletins across Europe have gone up by 20% on 

average and by 44% among young viewers and the daily reach of PSM online news sites has increased by 

2.6 times during the crisis.

As the EBU, we have brought journalists and factcheckers together to share crucial information and real-time 

intelligence on misinformation and disinformation around the virus (EBU Flashlight COVID-19 initiative) and 

we teamed up with a number of international news organizations to create the Trusted News Initiative which 

enables alerting each other to disinformation about coronavirus so that content can be reviewed promptly by 

platforms, whilst publishers ensure they don’t unwittingly republish disinformation. Alerts will also flag up 

content that undermines trust in partner news providers by identifying imposter content which claims to come 

from trusted brand identities or sources.

Digital skills media and information literacy are essential means of tackling disinformation. Public service 

media help to increase levels of societal knowledge and participation and play a key role in strengthening 

societal resistance to manipulation, rumors and disinformation. This is not only by trustworthy and reliable 

content, but through transparency in relation to journalistic and production methods. See the recent EBU 

Report on PSM in the COVID-19 and their role in providing news revealing the massive efforts public service 

media across Europe are making to keep citizens informed.  PSM are top investors in news and current 

affairs and children content (see https://www.ebu.ch/publications/research/loginonly/report/public-service-

media-and-news)

D. Experiences and data on erroneous removals

This section covers situation where content, goods or services offered online may be removed erroneously 

contrary to situations where such a removal may be justified due to for example illegal nature of such 

content, good or service (see sections of this questionnaire above).

1 Are you aware of evidence on the scale and impact of erroneous removals of 

content, goods, services, or banning of accounts online? Are there particular 

experiences you could share?

5000 character(s) maximum

 Earlier on in our response, we listed a range of examples where platforms claimed that legal content from 

broadcasters infringed platforms’ community standards even though the content is culturally acceptable and 

lawful in the EU and/or national context.

Erroneous removals of EBU Members content are numerous and this has a direct impact on media pluralism 

and on the freedom of information. Amongst numerous examples include medical information content which 

Facebook has removed using the justification that the content included content of a sexual nature or 

contained nudity.

Content removed due to platforms community standards is not the only damaging action by platforms, they 

also remove advertisement from this content, or classify it in a category to downgrade its visibility.

A recent experience from "Sexy soucis", a sex education program distributed on France Televisions' "Slash" 

platform for young people, is significant. In November 2019 Snapchat published new very strict guidelines. 

These meant that in a program on contraception, it was not possible to show visuals (of the pill for example), 

19

nor to write words referring to contraceptive methods. Faced with restrictive content policies, EBU members 

find themselves forced to adapt their content to the general conditions of the platforms if they want it to 

remain accessible online and run the risk of being less relevant and less impactful for their target audience. 

Any reduction in the number of unjustified content withdrawals can therefore be explained by content 

providers' efforts to comply with the platforms' business rules (and not only by a relaxation of platforms' 

requirements). By acting in this way, platforms impede the right to freedom of expression of media providers 

and restrict media plurality.

There are numerous examples of political or social content (widespread content on PSM) being labelled as 

political advertising by platforms and blocked (by Facebook). 

Failure to attribute content to its publisher or erroneous attribution also violates freedom of information, 

depriving the user of an essential element of judgment for information.

There are many examples of non-attribution, erroneous or uncertain attribution of EBU Member’s content. 

For example: sometimes the France Télévisions logo is not displayed, sometimes it is but not very visible or 

it is not immediately recognizable as content distributors and aggregators like Apple TV follow Apple’s 

general conditions for displaying logos. 

EBU Members recognise that some of these examples of removals are due to the platform’s own terms and 

conditions and community standards (which EBU Members have to sign up to). These unilateral terms and 

community standards often have different criteria than the law/regulations in the EU or in individual Member 

States and cause unnecessary removals of content on 3rd party platforms. This is a direct threat to media 

organisations’ editorial independence and media pluralism. The DSA package should clarify that contractual 

agreements cannot prevail over sectoral rules and, in particular, that platforms’ terms and conditions and 

community standards cannot take precedence over current and future EU legislation (as is ensured in the 

Portability Regulation 2017/1128 in Article 7).

The cornerstone of independent public service media is exercising full editorial responsibility over content 

guided by strict national and European rules as well as journalistic and editorial principles. Public trust is built 

on maintaining this independence. When digital platforms and social networks are used to make 

independent content of societal general interest available to audiences, such content should never be 

subject to any undue form of secondary control or removal.

Platforms have become gatekeepers for what and how PSM content can reach its audience. This directly 

impacts the editorial independence of media and further underlines the need to limit third party influence of 

editorial decisions by content providers. Stronger safeguards for the editorial independence of editorial 

content are needed on all platforms. Content by a provider with editorial responsibility should not be 

removed or modified, if doing so this could negatively affect their independence or trustworthiness.

The following questions are targeted at organisations. 

Individuals responding to the consultation are invited to go to section 2 here below on 

responsibilities for online platforms and other digital services

3 What is your experience in flagging content, or offerings of goods or services you 

deemed illegal to online platforms and/or other types of online intermediary 

services? Please explain in what capacity and through what means you flag 

content.

20

3000 character(s) maximum

Complaints are usually handled tardily, and illegal content often reappears. EBU members also find it difficult 

to get in touch with a responsible person for follow-up questions after having submitted their request. Finally, 

little information on actions taken by the platform is accessible.

We elaborate on this further below.

4 If applicable, what costs does your organisation incur in such activities?

3000 character(s) maximum

It takes internal manpower of EBU Members to deal with the different mechanisms provided by the platforms 

and because of the lack of engagement by the platforms themselves.

5 Have you encountered any issues, in particular, as regards illegal content or 

goods accessible from the EU but intermediated by services established in third 

countries? If yes, how have you dealt with these? 

3000 character(s) maximum

Please see response above. We consider that the situation where global online platform operators ban and

/or erroneously remove services and content from providers of legal content (such as broadcasters) solely 

because these services clash with platform’s community standards deserves further scrutiny and action at 

EU level. Media, including broadcast media, in Europe abide by European and/or national legal standards. 

Allowing unilateral global community standards to take precedence creates a serious threat to editorial 

freedom, cultural diversity and media pluralism in Europe. Online platform providers’ freedom to conduct a 

business (Art. 16 EU Charter) is not absolute and finds its limits in other fundamental rights and general 

interest objectives.

Illegal/harmful content is not uniformly defined at EU level and Member States have different rules and 

cultural norms. Defining illegal and harmful content and related enforcement should be within the 

competence of individual EU members states in respect of their cultural norms and respecting fundamental 

EU values.

6 If part of your activity is to send notifications or orders for removing illegal content 

or goods or services made available through online intermediary services, or taking 

other actions in relation to content, goods or services, please explain whether you 

report on your activities and their outcomes:

Yes, through regular transparency reports

Yes, through reports to a supervising authority

Yes, upon requests to public information

Yes, through other means. Please explain

No , no such reporting is done

7 Please provide a link to publicly available information or reports.

21

1000 character(s) maximum

8 Does your organisation access any data or information from online platforms?

Yes, data regularly reported by the platform, as requested by law

Yes, specific data, requested as a competent authority

Yes, through bilateral or special partnerships

On the basis of a contractual agreement with the platform

Yes, generally available transparency reports

Yes, through generally available APIs (application programme interfaces)

Yes, through web scraping or other independent web data extraction 

approaches

Yes, because users made use of their right to port personal data

Yes, other. Please specify in the text box below

No

9 Please indicate which one(s). What data is shared and for what purpose, and are 

there any constraints that limit these initiatives?

3000 character(s) maximum

The quality and quantity of data provided by internet service providers vary. In France, most comprehensive 

audience measurements are those provided by operators who accept integration of the “Médiamétrie” 

marker in the broadcast signal. Only a few operators do. Most operators prefer to use their own 

measurement system. However, these are not sufficient to provide an accurate overview of media services' 

consumption online. Some operators refuse or limit sharing of audience information: Orange recently 

stopped transmitting audience data for its programs to France Télévisions. In 2019, SFR ended the detailed 

program performance report to market it. Distributors can use audience data from publishers distributed for 

the benefit of companies in the same group or to market them.

The same issue arises with online platforms which do currently not provide meaningful access to data 

related to or generated by PSM content and services. If they do provide access, data remains fragmented 

and may not be relevant. Online platforms use their own measurement systems.

The same applies to app stores such as Apple. Apple refuses to transmit usage data to third parties (via 

Médiamétrie) for performance measurement purposes via the app store of France Télévisions Okoo's app, 

intended for young audiences. France Télévisions cannot therefore measure the success of its service and 

adapt it accordingly. An example of the need for greater cooperation with platforms is clearer communication 

and transparency regarding changes to terms of service. On occasion, EBU Members have discovered 

changes only at the point at which apps are rejected from Apple’s app stores. 

This growing tendency to refuse or limit audience information is a worrying trend meaning that media editors 

cannot measure the performance of their services. Media need access to data in a usable form generated by 

or related to making their content available on third party platforms, in line with data protection and privacy 

rules. This ensures further innovation and enhancement of services for the long-term benefit of audiences. 

Full and usable return of data when making content available on 3rd party platforms is crucial for PSM’s 

ability to understand audiences and innovate further on content and services. Transparency around what 

data the platforms gather and what content providers can access is increasingly important to continue 

22

development of services in the future. This must be done within the framework of the obligations of the 

platforms and corporate users respecting the GDPR. 

The degree of human interaction dialogue with the global platforms varies from good relationships, to 

nonexistent or merely automated messages. This results in lengthy case processing and restricts 

development of a partnership. Global platforms increasingly take part in distributing PSM content and in such 

matter acts as a distributor, it is important to establish good conditions for healthy and fair business relations. 

10 What sources do you use to obtain information about users of online platforms 

and other digital services – such as sellers of products online, service providers, 

website holders or providers of content online? For what purpose do you seek this 

information?

3000 character(s) maximum

Analytics provided by platforms (see Q9) and general market research.

11 Do you use WHOIS information about the registration of domain names and 

related information?

Yes

No

I don't know

12 Please specify for what specific purpose and if the information available to you 

sufficient, in your opinion?

3000 character(s) maximum

In the majority of cases WHOIS information does not deliver the contact details of the responsible person or 

entity of the content on websites, only the registrar of the domain. Registrars often refuse to disclose 

personal information with respect to the GDPR. Often the information is not sufficient.

13 How valuable is this information for you?

Please rate from 1 star (not particularly important) to 5 (extremely 

important)

14 Do you use or ar you aware of alternative sources of such data? Please explain.

3000 character(s) maximum

No

The following questions are targeted at online intermediaries.

A. Measures taken against illegal goods, services and content online shared by users

23

 
 
 
 
1 What systems, if any, do you have in place for addressing illegal activities 

conducted by the users of your service (sale of illegal goods -e.g. a counterfeit 

product, an unsafe product, prohibited and restricted goods, wildlife and pet 

trafficking - dissemination of illegal content or illegal provision of services)?

A notice-and-action system for users to report illegal activities

A dedicated channel through which authorities report illegal activities

Cooperation with trusted organisations who report illegal activities, following 

a fast-track assessment of the notification

A system for the identification of professional users (‘know your customer’)

A system for penalising users who are repeat offenders

A system for informing consumers that they have purchased an illegal good, 

once you become aware of this

Multi-lingual moderation teams

Automated systems for detecting illegal activities. Please specify the 

detection system and the type of illegal content it is used for

Other systems. Please specify in the text box below

No system in place

2 Please explain.

5000 character(s) maximum

3 What issues have you encountered in operating these systems?

5000 character(s) maximum

4 On your marketplace (if applicable), do you have specific policies or measures for 

the identification of sellers established outside the European Union ?

Yes

No

5 Please quantify, to the extent possible, the costs of the measures related to 

‘notice-and-action’ or other measures for the reporting and removal of different 

types of illegal goods, services and content, as relevant.

5000 character(s) maximum

24

6 Please provide information and figures on the amount of different types of illegal 

content, services and goods notified, detected, removed, reinstated and on the 

number or complaints received from users. Please explain and/or link to publicly 

reported information if you publish this in regular transparency reports.

5000 character(s) maximum

7 Do you have in place measures for detecting and reporting the incidence of 

suspicious behaviour (i.e. behaviour that could lead to criminal acts such as 

acquiring materials for such acts)?

3000 character(s) maximum

B. Measures against other types of activities that might be harmful but are not, in 

themselves, illegal

1 Do your terms and conditions and/or terms of service ban activities such as:

Spread of political disinformation in election periods?

Other types of coordinated disinformation e.g. in health crisis?

Harmful content for children?

Online grooming, bullying?

Harmful content for other vulnerable persons?

Content which is harmful to women?

Hatred, violence and insults (other than illegal hate speech)?

Other activities which are not illegal per se but could be considered harmful?

2 Please explain your policy.

5000 character(s) maximum

3 Do you have a system in place for reporting such activities? What actions do they 

trigger?

3000 character(s) maximum

4 What other actions do you take? Please explain for each type of behaviour 

considered.

25

5000 character(s) maximum

5 Please quantify, to the extent possible, the costs related to such measures.

5000 character(s) maximum

6 Do you have specific policies in place to protect minors from harmful behaviours 

such as online grooming or bullying?

Yes

No

7 Please explain.

3000 character(s) maximum

C. Measures for protecting legal content goods and services

1 Does your organisation maintain an internal complaint and redress mechanism to 

your users for instances where their content might be erroneously removed, or their 

accounts blocked?

Yes

No

2 What action do you take when a user disputes the removal of their goods or 

content or services, or restrictions on their account? Is the content/good reinstated?

5000 character(s) maximum

3 What are the quality standards and control mechanism you have in place for the 

automated detection or removal tools you are using for e.g. content, goods, 

services, user accounts or bots?

3000 character(s) maximum

4 Do you have an independent oversight mechanism in place for the enforcement 

of your content policies?

26

Yes

No

5 Please explain.

5000 character(s) maximum

D. Transparency and cooperation

1 Do you actively provide the following information:

Information to users when their good or content is removed, blocked or 

demoted

Information to notice providers about the follow-up on their report

Information to buyers of a product which has then been removed as being 

illegal

2 Do you publish transparency reports on your content moderation policy?

Yes

No

3 Do the reports include information on:

Number of takedowns and account suspensions following enforcement of 

your terms of service?

Number of takedowns following a legality assessment?

Notices received from third parties?

Referrals from authorities for violations of your terms of service?

Removal requests from authorities for illegal activities?

Number of complaints against removal decisions?

Number of reinstated content?

Other, please specify in the text box below

4 Please explain.

5000 character(s) maximum

27

5 What information is available on the automated tools you use for identification of 

illegal content, goods or services and their performance, if applicable? Who has 

access to this information? In what formats?

5000 character(s) maximum

6 How can third parties access data related to your digital service and under what 

conditions?

Contractual conditions

Special partnerships

Available APIs (application programming interfaces) for data access

Reported, aggregated information through reports

Portability at the request of users towards a different service

At the direct request of a competent authority

Regular reporting to a competent authority

Other means. Please specify

7 Please explain or give references for the different cases of data sharing and 

explain your policy on the different purposes for which data is shared.

5000 character(s) maximum

The following questions are open for all respondents.

2. Clarifying responsibilities for online platforms and other digital services

1 What responsibilities (i.e. legal obligations) should be imposed on online 

platforms and under what conditions? 

Should such measures be taken, in your view, by all online platforms, or only by 

specific ones (e.g. depending on their size, capability, extent of risks of exposure to 

illegal activities conducted by their users)? If you consider that some measures 

should only be taken by large online platforms, please identify which would these 

measures be.

Yes, by all online 

platforms, based 

on the activities 

Yes, 

only by 

Yes, only 

platforms 

Such 

at 

measures 

particular 

risk of 

should 

not be 

28

Maintain an effective ‘notice and action’ 

system for reporting illegal goods or 

content

Maintain a system for assessing the 

risk of exposure to illegal goods or 

content

Have content moderation teams, 

appropriately trained and resourced

Systematically respond to requests 

from law enforcement authorities

Cooperate with national authorities and 

law enforcement, in accordance with 

clear procedures

Cooperate with trusted organisations 

with proven expertise that can report 

illegal activities for fast analysis 

('trusted flaggers')

Detect illegal content, goods or services

In particular where they intermediate 

sales of goods or services, inform their 

professional users about their 

obligations under EU law

Request professional users to identify 

themselves clearly (‘know your 

customer’ policy)

Provide technical means allowing 

professional users to comply with their 

obligations (e.g. enable them to publish 

on the platform the pre-contractual 

information consumers need to receive 

in accordance with applicable 

consumer law)

Inform consumers when they become 

aware of product recalls or sales of 

illegal goods

Cooperate with other online platforms 

for exchanging best practices, sharing 

information or tools to tackle illegal 

activities

they intermediate 

(e.g. content 

larger 

online 

exposure 

required 

to illegal 

by law

hosting, selling 

platforms

activities 

goods or services)

by their 

users

29

Be transparent about their content 

policies, measures and their effects

Maintain an effective ‘counter-notice’ 

system for users whose goods or 

content is removed to dispute 

erroneous decisions

Other. Please specify

2 Please elaborate, if you wish to further explain your choices.

5000 character(s) maximum

We can no longer tolerate that online platform providers determine content policies (and thus influence 

access to and the visibility of content) and moderate speech far removed from public scrutiny. The DSA thus 

needs to improve transparency and accountability of platforms’ content policies and practices in relation to 

users (including PSM and other content providers), society at large as well as academic researchers and 

regulators.

Effective transparency measures should include: 

•        Effective notice-and-action procedures with clear deadlines, 

•        Information about platform and content providers (see our answer to question 17 in this chapter)

•        The publication of platforms’ content policies, including clear, easily understandable and sufficiently 

detailed explanations of their policies

•        The transparency of platforms’ content-related decisions in individual cases (e.g. confirmation of 

receipt to notice provider, clear and transparent notices including reasoning)

•        The publication of regular reports on activities related to the removal of content, 

•        The consultation of users and other stakeholders before changes to content policies are made

•        Informing content providers before substantial changes to content policies are made, 

•        The Provision of relevant data to researchers or, as appropriate, competent regulatory authorities (see 

our answer to questions 20 and 21 in this chapter),  

•        The submission of regular reports to competent regulatory authorities on the application of content 

policies, on measures taken to fight illegal and harmful content and on safeguards applied to protect 

fundamental rights, including on the resources deployed

3 What information would be, in your view, necessary and sufficient for users and 

third parties to send to an online platform in order to notify an illegal activity (sales 

of illegal goods, offering of services or sharing illegal content) conducted by a user 

of the service?

Precise location: e.g. URL

Precise reason why the activity is considered illegal

Description of the activity

Identity of the person or organisation sending the notification. Please explain 

under what conditions such information is necessary:

Other, please specify

30

4 Please explain

3000 character(s) maximum

Information on all these parameters are crucial to identify and effectively act against illegal content. The level 

of detail setting out why the activity is considered illegal must be proportionate and a summary reasoning 

should suffice as long as it gives sufficient detail to understand. It is also key that there is a system providing 

direct and easy contact for users to discuss questions and problems with the platforms.

5 How should the reappearance of illegal content, goods or services be addressed, 

in your view? What approaches are effective and proportionate?

5000 character(s) maximum

It is a legitimate objective to prevent the reappearance of illegal content, which, due to its dangerous nature, 

may cause wide-spread harm, sometimes to a significant proportion of the general public. Online platforms 

should be required to put in place appropriate and practicable measures to ensure the stay-down of illegal 

content. Such measures should be specific to the nature of the content and may differ accordingly: the 

reappearance of manifestly illegal material such as terrorist content may require different actions/tools

/systems than, for instance, copyright infringements or privacy violations. It is important that these measures 

be targeted and proportionate, balancing all interests at stake and ensuring the highest level of protection for 

parties’ fundamental rights. 

We consider that automated tools may be used to detect and prevent the reappearance of content 

previously found to be illegal, provided that there are relevant safeguards. The use of automated or 

algorithmic tools should be accompanied by appropriate human oversight (see also Q 6 below). What is 

more, effective and user-friendly redress mechanisms should be provided to allow content providers to 

contest online platforms’ content-related decisions. As a rule, PSM and other media content should be 

presumed to be complying with the law and should therefore stay online, unless, ultimately, a court finds 

otherwise (see our answer to question 6 below). 

In the exceptional case that online platforms take decisions affecting PSM and other media providers’ 

content, it is essential that online platforms provide a contact point for the national market to clarify for 

example, the nature of the content and adapt necessary action (e.g. suspension, removal or reinstatement of 

content). Moreover, such mechanisms should be harmonised across all online platforms and be clear and 

transparent in order to raise online platforms’ level of responsibility and ensure that they take swift and 

effective action in line with national law and cultural perceptions. 

6 Where automated tools are used to detect illegal content, goods or services, what 

opportunities and risks does their use present as regards different types of illegal 

activities and the particularities of the different types of tools?

3000 character(s) maximum

We agree that the use of automated tools brings with it opportunities and risks. Given the size, scale and 

volume of illegal content on online platforms, automated tools certainly constitute an effective and efficient 

method to swiftly and rapidly detect illegal content. 

While they represent an affordable means, such systems may not appropriately recognise or understand the 

context within which speech is iterated or they may not recognise the precise meaning of it (for example if 

material constitutes a parody or contains irony). Lawful speech, as demonstrated above, is already being 

blocked. Over-removal of lawful content is a severe threat to freedom of expression that PSM aim to foster. 

31

Hence, care needs to be taken that such tools do not interfere with content that is under the editorial control 

of media providers (including PSM) who are subject to regulatory standards and oversight. In respecting 

PSM’s and other content providers’ editorial freedom, their content should be presumed to be lawful and 

safe. No content should therefore be removed without the platform provider giving notice and offering the 

content provider the opportunity to defend itself. Content by PSM and other content providers should stay 

online until a court finds otherwise. 

In addition, the use of automated tools should be combined with appropriate human oversight (see Q 5 

above). Such human component should include a specific point of contact within a given Member State. For 

content providers such as PSM, it is vital to have such a contact point specifically dedicated to the national 

market, for instance, to clarify platforms’ content-related decisions.

7 How should the spread of illegal goods, services or content across multiple 

platforms and services be addressed? Are there specific provisions necessary for 

addressing risks brought by:

a. Digital services established outside of the Union?

b.  Sellers  established  outside  of  the  Union,  who  reach  EU  consumers 

through online platforms?

3000 character(s) maximum

Service providers established outside the EU should also be in the scope of the future DSA package. This is 

in line with other legal instruments and regulatory fields, such as the General Data Protection Regulation, the 

Platform-to-Business Regulation and competition law. The DSA should cover non-European providers who 

offer digital services to users in the EU. 

8 What would be appropriate and proportionate measures for digital services acting 

as online intermediaries, other than online platforms, to take – e.g. other types of 

hosting services, such as web hosts, or services deeper in the internet stack, like 

cloud infrastructure services, content distribution services, DNS services, etc.?

5000 character(s) maximum

9 What should be the rights and responsibilities of other entities, such as 

authorities, or interested third-parties such as civil society organisations or equality 

bodies in contributing to tackle illegal activities online?

5000 character(s) maximum

32

 
10 What would be, in your view, appropriate and proportionate measures for online 

platforms to take in relation to activities or content which might cause harm but are 

not necessarily illegal?

5000 character(s) maximum

Online platforms have greatly facilitated the free circulation of information and have become important 

channels to access and exchange information. But the abundance of content and information shared and 

spread online also comes with serious challenges as it created new ways to disseminate illegal and harmful 

content. 

This matters to public service media organizations. We produce a diverse range of content and information; 

offer our own trusted digital services to audiences and we also use digital service offerings by major third-

party global platforms, especially to reach young audiences or to challenge disinformation at source. 

PSM are concerned about the proliferation of harmful content, such as disinformation (as has been forcefully 

shown during the coronavirus pandemic), content unsuitable for minors or hateful and defamatory 

comments, particularly towards journalists.

Online platform providers currently shape public discourse by moderating speech and even restricting free 

speech through their terms and conditions/community standards. Online platform providers define these 

standards in a unilateral and non-transparent manner, unaccountable to the public and with an interest to 

augmenting traffic and maximizing profits.

Whenever platforms engage in the distribution of content, organize or moderate content, there is a need for 

effective safeguards to protect the general interest, fundamental rights and European values. Leaving it 

solely to the discretion of global online platforms to define what is in the public interest and to apply content 

display and removal policies and automated tools without any form of regulatory oversight is unacceptable. 

This endangers public interest objectives and values in EU, such as the freedom of expression and 

information, media pluralism (Art. 11 EU Charter of Fundamental Rights with Art. 10 European Convention 

on Human Rights), cultural diversity as well as the protection of consumers and vulnerable audiences such 

as children. Opaque decision-taking about the legality of content without adequate redress mechanisms may 

also be seen as violating the fundamental right to fair trial (Art. 47 EU Charter in conjunction with Art. 6 

ECHR). Online platform providers’ freedom to conduct a business (Art. 16 EU Charter) is not absolute and 

finds its limits in other fundamental rights and general interest objectives. 

Current online platforms’ practices also encroach on Member States’ ability to define what they consider 

harmful and this ability should be preserved. This notion has not been harmonised at EU level and remains 

subject to socio-cultural perceptions that evolve over time. Standards of harm are therefore different across 

Member States and so are the legal requirements linked to its distribution. This unity in diversity is part of the 

EU’s very fabric and it should continue to be respected. 

To stop platform providers from acting as governors of all online communication space, we believe that 

online platform providers should be subjected to clear duties of care. Such requirements should include, at 

least, enhanced transparency about their content policies as well as information obligations, vis-à-vis content 

providers and users ("Know-your-business-customer-principle”, see also our answer to question 2 above) as 

well as the implementation of user-friendly tools to flag and/or report harmful activities and content. The 

provision of flagging and/or reporting systems should be accompanied by appropriate explanations about the 

effect that has been given to flagged/reported content as well as by transparent, easy-to-use and effective 

procedures for the handling and resolution of complaints. Platforms should also regularly publish reports on 

their activities relating to the fight against harmful content. Such duties should be clearly outlined in law and 

be flanked by measures that ensure their effective enforcement, including regulators competence to sanction 

non-compliance, in particular by imposing deterrent fines. Policy interventions in relation to activities and 

content that might cause harm must take due account of the impact on fundamental rights and the diverse 

interests at stake. Such measures should therefore be designed in a careful, targeted manner, paying 

utmost attention to the proportionality principle. (see Q16 below).Enhanced transparency requirements 

applicable to business users ("Know-your-business-customer-principle”) will help online platforms to adapt 

33

their duty of care depending on the business user’s identity. For media services and content which are 

subject to editorial/regulatory standards, platforms should respect media’s existing editorial standards and 

regulations and do not interfere with the media’s content that is already subject to independent oversight. 

This is crucial to uphold public trust in media and democracy and it would also incentivise platforms to render 

content from PSM and other trusted media sources appropriately prominent

11 In particular, are there specific measures you would find appropriate and 

proportionate for online platforms to take in relation to potentially harmful activities 

or content concerning minors? Please explain.

5000 character(s) maximum

For PSM, providing all groups of society (in particular children) with dedicated programmes and services is 

at the heart of their national remit. EBU Members currently make available 233 unique dedicated children 

and youth services (including linear avms and radio, as well as standalone online brands) in their own safe 

online environments (EBU MIS data 2020). PSM also provide innovative and immersive online content, 

including on online platforms, operating more than 127 official children and youth dedicated social media 

offers (EBU MIS data 2020). PSM abide by broadcast regulations and apply the highest editorial standards 

to all services however consumed.

Current broadcast standards are outlined in the Audiovisual Media Services Directive (AVMSD), a sector-

specific instrument. The AVMSD provides a high level of protection in relation to audiovisual media services 

(avms). It needs to be ensured, in designing the future EU legal framework for online platforms, that in case 

of conflict, the AVMSD prevails over any horizontal legislation. Applying a graduated and harms-based 

approach, the AVMSD requires avms providers to protect minors from harmful and extremely harmful 

content (Art. 6a (1) AVMSD). The revised AVMSD also empowers audiences by requiring avms providers to 

give sufficient information about the harm associated with a particular programme (Art. 6a (3) AVMSD). In 

addition, the AVMSD imposes substantial advertising standards on avms providers, including rules that 

protect minors (Art. 9(1)(g) AVMSD).

Although the revised AVMSD introduces new rules for video-sharing platform services (VSPs), regulatory 

asymmetries persist between heavily regulated avms providers and lightly regulated VSP providers. 

Likewise, the current basic rules applicable to information society service providers according to the E-

Commerce Directive do not protect minors on online platforms. Users are generally unaware of distinctions 

between services online that are regulated or not (ERGA Position Paper on the DSA, p. 5, para. 6). The new 

rules applicable to VSPs under the AVMSD may serve as a useful blueprint for protecting users, particularly 

minors against harmful content.

While PSM do their utmost to provide dedicated and suitable programmes and services for children that they 

can enjoy in a safe online space (oftentimes going beyond the AVMSD’s legal requirements), the viewing 

environment on online platforms is less regulated and provides more opportunities for harm to occur. This is 

deplorable as current consumption trends show that minors and young adolescents (below 24 years) 

increasingly watch content on-demand and on online platforms, particularly on YouTube (see, e.g. p. 11 of 

Ofcom’s Children and Parents’ media attitude report 2019 and p. 41 of Ofcom’s Online Nation report 2019), 

but also through other VSPs like TikTok, Twitch or Snapchat as well as social media networks like Facebook 

and Twitter. 

Apart from the lack of safeguards for content harmful to children, PSM are likewise disturbed by the 

deterioration of public discourse as well as the proliferation of disinformation and hate speech online.

We have witnessed the damage that disinformation may cause during election times to our democracies (e.

g. during the 2019 European elections) or to public health (during the ongoing international sanitary crisis). 

Self-regulatory initiatives have failed and it is time to consider structured co-regulatory responses in the 

future (see our answer to question 3 above).

Increasingly, journalists are victims of aggression, harassment (cyber-bullying), hateful comments and in 

extreme cases this even amounts to illegal hate speech (see Media Pluralism Monitor 2020, p. 122). This 

34

may have a chilling effect on the freedom of expression and the watchdog role PSM play in our societies. 

The safety and integrity of journalists is crucial for PSM and their ability to produce investigative reporting 

and to provide independent, high quality news and current affairs programmes which are trusted by the 

public.

PSM have particular responsibilities towards their audiences, whether they air content on TV or make it 

available on online platforms. Audiences must be able to rely on a safe online space when they consume 

PSM content on online platforms. It is necessary that online platform providers meet additional obligations 

(see question 10 above).

12 Please rate the necessity of the following measures for addressing the spread of 

disinformation online. Please rate from 1  (not at all necessary) to 5 (essential) 

each option below.

1 (not at 

all 

2

necessary)

3 

(neutral)

4

5 

I don't 

know / 

(essential)

No 

answer

Transparently inform consumers 

about political advertising and 

sponsored content, in particular during 

election periods

Provide users with tools to flag 

disinformation online and establishing 

transparent procedures for dealing 

with user complaints

Tackle the use of fake-accounts, fake 

engagements, bots and inauthentic 

users behaviour aimed at amplifying 

false or misleading narratives

Transparency tools and secure 

access to platform data for trusted 

researchers in order to monitor 

inappropriate behaviour and better 

understand the impact of 

disinformation and the policies 

designed to counter it

Transparency tools and secure 

access to platform data for authorities 

in order to monitor inappropriate 

behaviour and better understand the 

impact of disinformation and the 

policies designed to counter it

Adapted risk assessments and 

mitigation strategies undertaken by 

online platforms

35

Ensure effective access and visibility 

of a variety of authentic and 

professional journalistic sources

Auditing systems for platform actions 

and risk assessments

Regulatory oversight and auditing 

competence over platforms’ actions 

and risk assessments, including on 

sufficient resources and staff, and 

responsible examination of metrics 

and capacities related to fake 

accounts and their impact on the 

manipulation and amplification of 

disinformation.

Other (please specify)

13 Please specify

3000 character(s) maximum

14 In special cases, where crises emerge and involve systemic threats to society, 

such as a health pandemic, and fast-spread of illegal and harmful activities online, 

what are, in your view, the appropriate cooperation mechanisms between digital 

services and authorities?

3000 character(s) maximum

15 What would be effective measures service providers should take, in your view, 

for protecting the freedom of expression of their users? Please rate from 1 (not at 

all necessary) to 5 (essential).

1 (not at 

all 

necessary)

2

3 

(neutral)

4

5 

I don't 

know / 

(essential)

No 

answer

High standards of transparency on 

their terms of service and removal 

decisions

Diligence in assessing the content 

notified to them for removal or blocking

Maintaining an effective complaint and 

redress mechanism

36

Diligence in informing users whose 

content/goods/services was removed 

or blocked or whose accounts are 

threatened to be suspended

High accuracy and diligent control 

mechanisms, including human 

oversight, when automated tools are 

deployed for detecting, removing or 

demoting content or suspending 

users’ accounts

Enabling third party insight – e.g. by 

academics – of main content 

moderation systems

Other. Please specify

16 Please explain.

3000 character(s) maximum

Public service media organisations in Europe are heavily regulated and subject to independent oversight. 

They bear editorial responsibility for the content they publish and may be held liable for it under national 

laws. Liability should remain to be limited to the content PSM produce or commission and for which they 

bear editorial responsibiliy, excluding any liability for user comments on social media platforms (as 

insinuated by Australian Justice Rothman in a heavily criticised decision holding media companies liable for 

defamatory comments posted by users in the comments section of the media organisations’ Facebook 

pages (See https://www.caselaw.nsw.gov.au/decision/5d0c5f4be4b08c5b85d8a60d). 

While we understand online platforms should act in relation to certain types of content, they should not 

subject media services and content, for which editorial responsibility is already exercised, to any form of 

control or interference. Any additional control by platforms over media’s content already subject to oversight 

would be inappropriate and interfere with the right to freedom of expression and information. Any decisions 

on suspension or removal of such content should be left to independent judicial authorities. When content is 

taken down or suspended, effective remedy and redress mechanisms should be available to content 

providers, granting them an effective right to defence.

Platforms’ algorithms used for ranking content can also have far-reaching consequences on users’ right to 

freedom of expression. By their community standards and more practically, through their algorithms, 

platforms control what content appears, where and when. They do so unilaterally and in full opacity, 

unaccountable to regulatory authorities and the general public. To promote a flourishing online space where 

free speech is possible within acceptable limits, it is inevitable to enhance algorithmic transparency. To 

reach audiences through online platforms and understand why each content item appears in the ranking or 

news feed, PSM and other content providers need to understand platforms’ content policies (including 

ranking) and be aware of any changes. 

17 Are there other concerns and mechanisms to address risks to other 

fundamental rights such as freedom of assembly, non-discrimination, gender 

equality, freedom to conduct a business, or rights of the child? How could these be 

addressed?

37

5000 character(s) maximum

18 In your view, what information should online platforms make available in relation 

to their policy and measures taken with regard to content and goods offered by 

their users? Please elaborate, with regard to the identification of illegal content and 

goods, removal, blocking or demotion of content or goods offered, complaints 

mechanisms and reinstatement, the format and frequency of such information, and 

who can access the information.

5000 character(s) maximum

It is crucial to have full transparency and information available for content and services available on 

platforms including on the online platform provider and the content provider to include:

1. Information on content provider uploading content (know your (business) customer)

Information obligations should be outlined that enhance transparency of content providers. The E-Commerce 

Directive’s information requirements for information society service providers (Art. 5 ECD) could be 

broadened and complemented. Information should at least include: 

-        The name and the address at which they are established (for legal entities, the legal form, the 

authorized representative),

-        Possibly, the name and address of the responsible editor,

-        Information that enables quick electronic contact and direct communication, including email and 

website,

-        If the service is offered as part of an activity that requires official approval (such as notification), 

information on the responsible supervisory authority

2. Information of the responsible person within the online platform

Information about the responsible persons for the blocking and deleting of content, whether the content is 

illegal or not, must be easily accessible. 

3. Clear information about the complaint mechanism and reinstatement:

The complaint mechanism must be easy to handle.

All information in points 1-3 must be  

-        easily recognizable, 

-        immediately and readily accessible and 

-        always available for all users at any time and in the same place.

19 What type of information should be shared with users and/or competent 

authorities and other third parties such as trusted researchers with regard to the 

use of automated systems used by online platforms to detect, remove and/or block 

illegal content, goods, or user accounts?

5000 character(s) maximum

38

20 In your view, what measures are necessary with regard to algorithmic 

recommender systems used by online platforms?

5000 character(s) maximum

It is without doubt that online platforms have facilitated information sharing and enabled free circulation of 

views and expressions. Their services enhance participation in public debate and thereby contribute to the 

public good. For PSM, online platforms have become an indispensable partner in reaching certain audiences 

and PSM are experimenting with new, innovative ways of distributing content (to the extent that their national 

remit allows). Yet, the proliferation of illegal content like hate speech and other harmful material such as 

disinformation on such platforms has forcefully revealed the dangers and risks platforms’ practices may have 

on the coherence of our societies. 

Online platforms have also become powerful gatekeepers, by determining the way content is accessed, 

made available, found, suspended or removed. Online platforms are essentially moderating speech (in line 

with their community standards/terms of service) and they do so as profit-driven companies. They manage 

the way content is displayed on their platforms, including by ranking, personalising and recommending 

content to users, based on a significant amount of data they gather of and about every single user. 

A means to curating/managing/moderating the vast amounts of content available on online platforms, is 

algorithmic recommender systems that, very often, attribute preference to the online platforms’ own content 

(by way of self-preferencing, see below on gatekeeper power of online platforms), to affiliated third parties or 

to the party that pays the most. Online platforms gain huge profits by trading users’ attention to the highest 

bidder (be it for editorial or commercial content) while platform users lack the necessary transparency on 

how these systems work and how their data are exploited and lack the tools to empower them.  

Major online platforms are driven by their own global, commercial motivations and this has propelled them to 

much success, but they have become powerful gatekeeping platforms through which users access content, 

control vast amounts of data and are not subject to robust regulation. If we want to secure the ability to offer 

great and diverse European content to audiences the DSA needs to establish the right legal framework, that 

(i) promotes content by authentic/trustworthy media providers and (ii) enhances transparency about 

recommender systems. 

To promote a safe online space within which public discourse and opinion forming can take place within 

acceptable limits, it is necessary that online platforms take measures to promote and render appropriately 

prominent general interest content (see Q 7 on the liability regime). This is especially vital in situations of 

crises, like the coronavirus pandemic (also dubbed infodemic due to the sheer amount of disinformation that 

has proliferated on the virus). 

More generally, online platforms’ opaque content moderation policies are problematic as they put at risk 

general interest objectives like the freedom of expression and the right to information as well as media 

pluralism and media freedom, all of which are expressly protected by Art. 11 of the EU Charter of 

Fundamental Rights and cultural diversity, which is inherent in the EU’s sui generis nature. More 

transparency is needed for content providers, especially with regard to the effects of recommender systems 

on the visibility/accessibility of content. Without prejudice to platform providers’ obligation to ensure that 

general interest content is prominently displayed, it is vital that  online platforms be transparent and inform 

content providers about changes to ranking and recommendation systems so that content providers can 

react and adapt their services accordingly, minimising the negative impact of such changes/decisions on 

content visibility/findability and access/availability

39

21 In your view, is there a need for enhanced data sharing between online 

platforms and authorities, within the boundaries set by the General Data Protection 

Regulation? Please select the appropriate situations, in your view:

For supervisory purposes concerning professional users of the platform - e.

g. in the context of platform intermediated services such as accommodation 

or ride-hailing services, for the purpose of labour inspection, for the purpose 

of collecting tax or social security contributions

For supervisory purposes of the platforms’ own obligations – e.g. with regard 

to content moderation obligations, transparency requirements, actions taken 

in electoral contexts and against inauthentic behaviour and foreign 

interference

Specific request of law enforcement authority or the judiciary

On a voluntary and/or contractual basis in the public interest or for other 

purposes

22   Please  explain.  What  would  be  the  benefits?  What  would  be  concerns 

for  companies, consumers or other third parties?

5000 character(s) maximum

Content providers, such as PSM, are contingent on the algorithms deployed by online platforms. Whether 

content is surfaced or found by users depends on the way the algorithm is programmed. In addition to 

enhancing transparency of online platforms’ content policies, we believe that more enhanced data sharing 

between online platforms and competent authorities would promote algorithmic accountability. While online 

platforms may not be obliged to disclose their algorithms publicly (due to trade secrets regulation), they 

should at least grant access to independent regulators/authorities to allow for effective control and 

enforcement. 

23 What types of sanctions would be effective, dissuasive and proportionate for 

online platforms which systematically fail to comply with their obligations (See also 

the last module of the consultation)?

5000 character(s) maximum

Experience from European competition and data protection law shows that the only sanction providing 

incentives for platforms to respect rules are deterrent financial sanctions

24 Are there other points you would like to raise?

3000 character(s) maximum

II. Reviewing the liability regime of digital services acting as intermediaries?

40

The liability of online intermediaries is a particularly important area of internet law in Europe and worldwide. 

The E-Commerce Directive harmonises the liability exemptions applicable to online intermediaries in the 

single market, with specific provisions for different services according to their role: from Internet access 

providers and messaging services to hosting service providers.

The previous section of the consultation explored obligations and responsibilities which online platforms 

and other services can be expected to take – i.e. processes they should put in place to address illegal 

activities which might be conducted by users abusing their service. In this section, the focus is on the legal 

architecture for the liability regime for service providers when it comes to illegal activities conducted by their 

users. The Commission seeks informed views on hos the current liability exemption regime is working and 

the areas where an update might be necessary.

2 The liability regime for online intermediaries is primarily established in the E-

Commerce Directive, which distinguishes between different types of services: so 

called ‘mere conduits’, ‘caching services’, and ‘hosting services’. 

In your understanding, are these categories sufficiently clear and complete for 

characterising and regulating today’s digital intermediary services? Please explain.

5000 character(s) maximum

PSM rely on a diverse range of distributors that make PSM’s programmes and services available to the 

general public. PSM, as avms providers, assume full editorial responsibility for and control over the content 

they provide. Editorial control, however, has its limits and PSM and other content providers cannot be made 

liable for comments posted by users on social media networks, as they do not have any control (including no 

control over settings) over the comments section. We therefore sincerely regret the recent preliminary 

decision by Australian Justice Rothman who found that publishers had legal liability for comments on their 

Facebook pages (https://www.caselaw.nsw.gov.au/decision/5d0c5f4be4b08c5b85d8a60d). 

In general, it is noteworthy that the character and role of the intermediary, i.e. the actor placed between PSM 

and its audiences, has become more fluid and varied than at the time the E-Commerce Directive was 

adopted. We are especially concerned that the category of “hosting services” as referred to in the E-

Commerce Directive is too broad and does not adequately reflect the multi-functional nature of online 

platforms. We therefore recommend that the DSA establish meaningful and future-proof sub-categories. 

Whatever future denominations, it is important that one such sub-category captures hosting service 

providers whose business centers around the commercialization of content by providing access, making 

available tools for sharing content and by promoting/categorizing/organizing content (the future definition 

could draw inspiration from the definition of “video-sharing platform service” as set out in Art. 1(1)(aa) 

Audiovisual Media Services Directive or that of “online content-sharing service provider” as introduced by 

Art. 2(6) of the Copyright in the Digital Single Market Directive). 

Apart from the problematic definition of “hosting service”, the E-Commerce Directive’s underlying assumption 

upon which the liability regime is based is no longer up to date. This is because certain online platforms, 

notably video-sharing platforms (VSPs) and social media networks do not merely passively host or assemble 

(third-party) audiovisual content, but they organize, rank, label, personalize, monetize or otherwise moderate 

or commercialize content for public use and thus take content-related decisions (see also Cole/Etteldorf

/Ullrich, Cross-border dissemination of online content, p. 44-45). Their business models are geared towards 

maximizing profits through the sale of space and visibility (for editorial and commercial content) on their 

platforms. 

We see an urgent need to act as online platform providers determine what users see/find and what they do 

not/are unlikely to see/find. By moderating and curating content (primarily through algorithmic means), they 

41

decide upon the degree of visibility and access to online content. In accordance with the principle of 

universality that underpins PSM’s public service mission and on account of the shift in consumption patterns, 

PSM offer services to reach audiences on online platforms. However, PSM can only effectively fulfill its 

democratic, social and cultural role in today’s society if there is a safe online environment and if their 

programmes and services are clearly visible and easily findable in the online space – and not buried under 

an avalanche of sponsored content and disinformation that becomes viral (see also our answer to question 7 

below). 

For hosting services, the liability exemption for third parties’ content or activities is conditioned by a 

knowledge standard (i.e. when they get ‘actual knowledge’ of the illegal activities, they must ‘act 

expeditiously’ to remove it, otherwise they could be found liable).

3 Are there aspects that require further legal clarification?

5000 character(s) maximum

4 Does the current legal framework dis-incentivize service providers to take 

proactive measures against illegal activities? If yes, please provide your view on 

how disincentives could be corrected.

5000 character(s) maximum

The current legal framework is insufficient in that it does not provide incentives for online platform providers 

to take proactive measures against illegal activities. This should be corrected in the DSA which should set 

out binding harmonised rules tackling the spread of illegal content online, including effective notice-and-

action procedures as well as redress mechanisms. 

5 Do you think that the concept characterising intermediary service providers as 

playing a role of a 'mere technical, automatic and passive nature' in the 

transmission of information (

recital 42 of the E-Commerce Directive

) is sufficiently 

clear and still valid? Please explain. 

5000 character(s) maximum

Today, certain distributors still retain a passive role. These are, for example, providers of electronic 

communications services as defined by Art. 2(4) EECC, such as traditional/legacy telecommunications or 

cloud service providers. As long as these providers do not interfere with the content (including by automated

/algorithmic tools), they should be regarded as neutral or passive intermediaries within the meaning of 

Recital 42 E-Commerce Directive and consequently benefit from the E-Commerce Directive’s liability 

exemptions. 

If, however, providers interfere with the content/information they transmit, they must be considered as active 

and cannot be allowed to escape potential claims for secondary liability (provided that the DSA retains the E-

Commerce Directive’s link between a provider’s passiveness and the liability exemptions). This is also 

relevant in case a telecommunications provider offers several services and performs diverse roles: (i) the 

one of a passive intermediary transmitting audiovisual content as well as (ii) the one of an active content 

provider. Providers offering several services must therefore assume an appropriate degree of (secondary) 

responsibility for (the part of) the service for which it assumes an active role. 

42

We note, however, that, in the future, the active/passive dichotomy may no longer be suitable to capture the 

complexities of activities/services provided by a same provider. An updated liability regime may instead be 

based on clearer definitions of services (see Q 2 above) that the DSA will apply to, attributing the appropriate 

level of responsibility to each service category. 

Should the DSA retain the active/passive distinction, it is necessary to acknowledge that hosting service 

providers do no longer play a role of a “mere technical, automatic and passive nature” in the transmission of 

content/information. In fact, online platforms take content-related decision in line with their community 

standards (see Q 2 above), which override the media’s editorial standards and decisions. In combination 

with the dominant position of certain platforms, such secondary control by platforms does not only threaten 

media pluralism but also severely limit media freedom, a fundamental right expressly protected by Art. 11(2) 

EU Charter of Fundamental Rights (see also our answer to question 7 below).

6 The E-commerce Directive also prohibits Member States from imposing on 

intermediary service providers general monitoring obligations or obligations to seek 

facts or circumstances of illegal activities conducted on their service by their users. 

In your view, is this approach, balancing risks to different rights and policy 

objectives, still appropriate today? Is there further clarity needed as to the 

parameters for ‘general monitoring obligations’? Please explain.

5000 character(s) maximum

The availability of illegal content - including illegal hate speech against journalists - on online platforms is 

increasing. Online platform providers must take more responsibility with respect to the content available on 

their platforms, the current E-Commerce Directive as well as additional self-regulatory initiatives fighting 

illegal content are no longer sufficient. The existing liability regime allows them to hide from responsibility 

based on self-perception as being passive and neutral. The DSA must include a comprehensive legal 

framework including clear responsibilities, duty of care rules as well as effective oversight and enforcement 

(see our answers to questions 2 and 10 in the chapter on responsibilities and our answer to question 4 

above and 7 below).

Indeed, the ban on general monitoring obligations should not prevent specific monitoring obligations : For 

example, in case of manifestly illegal content (inciting terrorism, child sexual abuse material) platforms must 

put more effort into monitoring the availability of this kind of content on their platforms. In this case they 

should not be able to use the liability exemption. 

In addition to counter current information asymmetries, platform providers need to put measures in place that 

allow for a more reliable identification of uploaders/content providers and enhance transparency (Know-your-

business-customer principle). In the case of editorial content provided by regulated media providers 

(including PSM content), such specific monitoring obligations should not be applicable (see our answer to 

question 7 below). 

7 Do you see any other points where an upgrade may be needed for the liability 

regime of digital services acting as intermediaries?

5000 character(s) maximum

It is high time that the DSA, like the revised AVMSD has done, recognizes hosting service providers’ actual 

role and their business model whereby they compete for users’ attention. The DSA must establish rules that 

43

 
correspond to online platform providers’ content management activity, taking into account their societal 

impact and their importance for democratic opinion forming. Three regulatory solutions are indispensable: 

1.        Respecting media providers’ editorial decisions. The DSA must bring an end to online platforms’ 

private enforcement of unilaterally imposed content standards and must ensure that independent courts or 

other public oversight, on the basis of an assessment of relevant fundamental rights, are competent to 

decide upon the legality of content and thus on the inclusion or suspension/removal of content on online 

platforms. We also consider it necessary that online platform providers are under a positive obligation to 

respect media providers’ editorial decisions. As a result, they are prohibited from exercising a second layer 

of control over content that is already under a media provider’s editorial control and subject to specific 

standards/media regulation and independent oversight (such as PSM’s services which are governed by 

sector-specific regulation and compliance of which is monitored by independent national regulatory 

authorities/supervisory bodies) (see also Schibsted Report Ensuring democracy and freedom of speech 

online). The beneficiaries of this positive obligation could be determined at the Member State level, either 

through a multi-stakeholder process, or through an open and objective procedure under the responsibility of 

national media regulators. Importantly, this positive obligation to respect existing content standards should 

encompass a requirement to leave intact media provider’s branding and to correctly attribute the source. In 

turn, online platforms should be exempt from liability for content which has been provided by media 

providers bearing editorial responsibility for content distributed online.

2.        Ensuring prominence of general interest content.Online platforms’ opaque content policies are 

problematic because they determine access/availability and visibility/findability of content. These put at risk 

general interest objectives like freedom of expression, the right to information, media pluralism, media 

freedom (Art. 11 EU Charter of Fundamental Rights) and cultural diversity which is inherent in the EU’s sui 

generis nature (see Q10 and 11 Responsibilities).To promote a safe online space where public discourse 

and opinion forming can take place within acceptable limits, active online platforms need to take measures to 

promote and render appropriately prominent general interest content. This is especially vital in crises like 

COVID-19 (also dubbed infodemic due to the sheer amount of disinformation published on the virus). The 

trustworthiness of a provider may be established through different ways, such as using the machine-

readable CEN standard of the Journalism Trust Initiative (see Q 5 below), which is a checklist of commonly 

agreed criteria for high quality and independent journalism to which media providers may subscribe, ranging 

from ownership and sources of revenue to the editorial process, including topics like correction policies, 

labelling of opinion or sponsored content, and ensuring accuracy. Alternatively, trusted providers may be 

defined at the national level, through a multi-stakeholder dialogue or by the competent national media 

regulators based on open and transparent criteria. Such prominence measures should be placed within the 

appropriate regulatory framework and take due account of the media’s specificities, like the new findability 

rule applicable to user interfaces introduced in Art. 84 German Medienstaatsvertrag. A recent report from 

Mediatique published by Ofcom also demonstrates the importance of prominence in the Connected TV 

context (https://www.ofcom.org.uk/__data/assets/pdf_file/0019/201493/connected-gateways.pdf). VUB-SMIT 

conducted similar research on the Belgian market and the study, which has not yet been published, likewise 

advocates for national prominence.

3.        Enhancing procedural responsibility  The DSA should outline clear and effective procedures 

enhancing online platforms’ accountability. This includes efficient and effective notice-and-action 

mechanisms in relation to fighting the (re)appearance of illegal content including effective complaints 

mechanisms (e.g. respecting content providers’ rights in contesting online platforms’ content-related 

decisions), as well as enhanced transparency rules (about content moderation and curation as well as about 

business customers). As enforcement of the new rules is key, an effective mechanism should be 

established, including administrative sanctions (notably dissuasive fines possibly indexed to platform 

providers’ global turnover as in the case of the GDPR) in case of non-compliance.

III. What issues derive from the gatekeeper power of digital platforms?

44

There is wide consensus concerning the benefits for consumers and innovation, and a wide-range of 

efficiencies, brought about by online platforms in the European Union’s Single Market. Online platforms 

facilitate cross-border trading within and outside the EU and open entirely new business opportunities to a 

variety of European businesses and traders by facilitating their expansion and access to new markets. At 

the same time, regulators and experts around the world consider that large online platforms are able to 

control increasingly important online platform ecosystems in the digital economy. Such large online 

platforms connect many businesses and consumers. In turn, this enables them to leverage their 

advantages – economies of scale, network effects and important data assets- in one area of their activity to 

improve or develop new services in adjacent areas. The concentration of economic power in then platform 

economy creates a small number of ‘winner-takes it all/most’ online platforms. The winner online platforms 

can also readily take over (potential) competitors and it is very difficult for an existing competitor or potential 

new entrant to overcome the winner’s competitive edge. 

The Commission

 announced

 that it ‘will further explore, in the context of the Digital Services Act package, 

ex ante rules to ensure that markets characterised by large platforms with significant network effects acting 

as gatekeepers, remain fair and contestable for innovators, businesses, and new market entrants’.

This module of the consultation seeks informed views from all stakeholders on this framing, on the scope, 

the specific perceived problems, and the implications, definition and parameters for addressing possible 

issues deriving from the economic power of large, gatekeeper platforms. 

The Communication ’Shaping Europe’s Digital Future’

 also flagged that ‘competition policy alone cannot 

address all the systemic problems that may arise in the platform economy’. Stakeholders are invited to 

provide their views on potential new competition instruments through a separate, dedicated open public 

consultation that will be launched soon.

In parallel, the Commission is also engaged in a process of reviewing EU competition rules and ensuring 

they are fit for the modern economy and the digital age. As part of that process, the Commission has 

launched a consultation on the proposal for a New Competition Tool aimed at addressing the gaps 

identified in enforcing competition rules. The initiative intends to address as specific objectives the 

structural competition problems that prevent markets from functioning properly and that can tilt the level 

playing field in favour of only a few market players. This could cover certain digital or digitally-enabled 

markets, as identified in the report by the Special Advisers and other recent reports on the role of 

competition policy, and/or other sectors. As such, the work on a proposed new competition tool and the 

initiative at stake complement each other. The work on the two impact assessments will be conducted in 

parallel in order to ensure a coherent outcome. In this context, the Commission will take into consideration 

the feedback received from both consultations. We would therefore invite you, in preparing your responses 

to the questions below, to also consider your response to the parallel consultation on a new competition tool

.

1 To what extent do you agree with the following statements?

Neither 

Fully 

agree

Somewhat 

agree 

Somewhat 

Fully 

agree

not 

disagree

disagree

disagree

I 

don't 

know/ 

No 

reply

Consumers have sufficient 

choices and alternatives to 

the offerings from online 

platforms.

45

It is easy for consumers to 

switch between services 

provided by online platform 

companies and use same or 

similar services provider by 

other online platform 

companies (“multi-home”).

It is easy for individuals to 

port their data in a useful 

manner to alternative 

service providers outside of 

an online platform.

There is sufficient level of 

interoperability between 

services of different online 

platform companies.

There is an asymmetry of 

information between the 

knowledge of online 

platforms about consumers, 

which enables them to 

target them with commercial 

offers, and the knowledge of 

consumers about market 

conditions.

It is easy for innovative SME 

online platforms to expand 

or enter the market.

Traditional businesses are 

increasingly dependent on a 

limited number of very large 

online platforms.

There are imbalances in the 

bargaining power between 

these online platforms and 

their business users.

Businesses and consumers 

interacting with these online 

platforms are often asked to 

accept unfavourable 

conditions and clauses in 

the terms of use/contract 

with the online platforms.

Certain large online platform 

companies create barriers 

46

to entry and expansion in 

the Single Market 

(gatekeepers).

Large online platforms often 

leverage their assets from 

their primary activities 

(customer base, data, 

technological solutions, 

skills, financial capital) to 

expand into other activities.

When large online platform 

companies expand into 

such new activities, this 

often poses a risk of 

reducing innovation and 

deterring competition from 

smaller innovative market 

operators.

Main features of gatekeeper online platform companies and the 

main  criteria for assessing their economic power

1 Which characteristics are relevant in determining the gatekeeper role of large 

online platform companies? Please rate each criterion identified below from 1 (not 

relevant) to 5 (very relevant):

Large user base

Wide geographic coverage in the EU

They capture a large share of total revenue of the market you are 

active/of a sector

Impact on a certain sector

They build on and exploit strong network effects

They leverage their assets for entering new areas of activity

47

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
They raise barriers to entry for competitors

They accumulate valuable and diverse data and information

There are very few, if any, alternative services available on the 

market

Lock-in of users/consumers

Other

2 If you replied "other", please list

3000 character(s) maximum

-User behavior (e.g. users do not ‘multi-home’; users’ trust in the platform)

-Many widely used platforms are vertically and/or diagonally integrated

Due to several traits characterizing the markets under consideration (e.g. network effects, high production

/low distribution costs, the ability to reap large economies of scale and scope), certain platforms are vertically 

and/or diagonally integrated. As a result, those platforms have the ability and incentive to control entire 

digital ecosystems/the entire value chain. We believe that this is a characteristic that should be taken into 

account in determining whether a platform is a gatekeeper. Vertical integration is very common in ‘mature’ 

digital markets, such as online search, social networking and news aggregation, as well as in emerging 

audio markets (consider, for instance, a popular music streaming platform expanding into content 

development by acquiring podcast production companies and by offering advertising services that would 

facilitate monetization of the content concerned. Another example is a company producing smart phones 

that expands into content aggregation by offering its own application store and audio streaming service). The 

greater the degree of vertical integration in a specific sector is, the greater the impact of a platform on that 

sector will be.    

In addition to the criteria identified by the Commission and vertical/diagonal integration, we find that an 

assessment of whether a large online platform is a gatekeeper must not be restricted to supply-side 

considerations. It should further consider how users behave. This is clearly illustrated by the Commission’s 

Google Shopping decision. In its decision, the Commission found that, though it is easy for users to switch 

from one search engine to another, only a minority of users multi-home (paragraphs 307-311). Moreover, 

and perhaps more importantly, the Commission relied on experiments showing that a significant number of 

online users trusted Google to such an extent that they would be highly unlikely to use a different search 

engine even if Google were to deliver less relevant search results (paragraph 312 and fn. 333). Taking 

account of user attitude enabled the Commission to gain a more complete understanding of Google’s 

gatekeeper position in the affected market. 

3 Please explain your answer. How could different criteria be combined to 

accurately identify large online platform companies with gatekeeper role?

48

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3000 character(s) maximum

In some cases, a combination of the above criteria may be necessary to conduct relevant assessments. For 

example, in the case of a nascent market, there may be very few services to which the consumer may 

switch. If combined with practices that lock the consumer into its services without the consumers switching to 

another service, this would arguably be an indication that the platform under consideration acts as a 

gatekeeper. Similarly, if a platform that benefits from strong network effects controls a large user base, 

rendering it difficult for other providers to enter or expand into the market concerned, that would also be an 

indication that the platform acts as a gatekeeper. 

However, we believe that a combination of the above criteria is not necessary in all cases. For example, 

based on the 2020 Reuters Institute Report, 72% of online users access news through a gate, including a 

social network (26%) or a search engine (25%) (p. 23). Another example concerns PSM organizations that 

are required to use as many platforms as possible in order to reach all segments of society, including 

younger generations, to fulfil their public service obligations. In such cases, the platform in question may 

possess intermediation power and become an ‘unavoidable partner’ for content providers that may not 

otherwise reach the audiences concerned. In other words, in cases such as those described above, the user 

base that a platform controls may be sufficient to determine that a platform acts as a gatekeeper. A ‘large’ 

user base should not be defined with reference to an absolute number. Other parameters, such as the 

demographics of the user base, how users consume content (e.g. do they use the platform as their primary 

source of news?), should define whether the platform is a gatekeeper. 

As regards the criterion of wide geographic coverage, the presence of a platform in several Member States 

could, in principle, determine whether that platform is a gatekeeper. Yet, in many cases, including in the 

case of audiovisual and audio markets, markets are mainly national in nature. In such cases, other criteria, 

such as the user base that a platform controls and the degree of vertical integration, should define whether a 

platform acts as a gatekeeper.  

Finally, it is our understanding that, in this Part, the Commission focuses on ‘large, gatekeeper’ platforms. 

However, we find that rules must be set that apply to platforms irrespective of their size. For example, the 

P2B Regulation imposes on all platforms a set of transparency obligations. Yet, the regulatory framework 

has many gaps that need to be filled (e.g. the P2B Regulation does not establish the platforms’ obligation to 

disclose whether they engage in retaliatory practices). Put differently, though the focus on certain large 

platforms may be justified to address certain issues, appropriate rules capturing all platforms are needed in 

order to promote fairness in the platform economy. 

4 Do you believe that the integration of any or all of the following activities within a 

single company can strengthen the gatekeeper role of large online platform 

companies (‘conglomerate effect’)? Please select the activities you consider to 

steengthen the gatekeeper role:

online intermediation services (i.e. consumer-facing online platforms such as 

e-commerce marketplaces, social media, mobile app stores, etc., as per Reg

ulation (EU) 2019/1150

 - see glossary)

search engines

operating systems for smart devices

consumer reviews on large online platforms

49

network and/or data infrastructure/cloud services

digital identity services

payment services (or other financial services)

physical logistics such as product fulfilment services

data management platforms

online advertising intermediation services

other. Please specify in the text box below.

5 Other - please list

1000 character(s) maximum

Emerging issues

The following questions are targeted particularly at businesses and business users of large online 

platform companies.

2 As a business user of large online platforms, do you encounter issues concerning 

trading conditions on large online platform companies?

Yes

No

3 Please specify which issues you encounter and please explain to what types of 

platform these are related to (e.g. e-commerce marketplaces, app stores, search 

engines, operating systems, social networks).

5000 character(s) maximum

PSM organizations are bound by ‘universality’ obligations, which require them to reach all segments of 

society. Given that a vast amount of content is now consumed on platforms, PSM rely on platforms to reach 

their audiences. Being present on different platforms is linked to the different purposes which those platforms 

serve. For example, app stores and smart TVs are used to distribute applications; social networks are mainly 

used for marketing purposes and community building; communications apps are used to make content ‘go 

viral’; audio streaming platforms are used for podcast distribution; and  VOD platforms are used for content 

monetization. 

In dealing with platforms, many problems arise from practices that prevent PSM from reaching their 

audiences. For example, lack of transparency about how a social media network works in practice does not 

allow PSM to assess how the relevant algorithm impacts the newsfeed. The same applies to how apps are 

promoted on popular app stores. Other issues concern the preferential treatment that platforms grant to their 

own services (or the services provided by advertising partners), lack of access to (reliable) data, and lack of 

attribution.

50

4 Have you been affected by unfair contractual terms or unfair practices of very 

large online platform companies? Please explain your answer in detail, pointing to 

the effects on your business, your consumers and possibly other stakeholders in 

the short, medium and long-term?

5000 character(s) maximum

The main source of the issues we have encountered in dealing with platforms is lack of bargaining power. 

Combined with the large user base platforms control, lack of bargaining power translates into accepting 

unfair contractual terms or being subject to unfair practices. 

In many cases, PSM organizations are forced to grant a non-exclusive, sublicensable and royalty-free 

worldwide licence of their content. 

Another example concerns retaliatory and bundling practices. Certain platforms bundle subscription-based 

and free services. If PSM do not agree to the distribution of their content through the subscription-based 

channel, they are not allowed to distribute their content for free. This is a ‘take-it-or-leave-it offer’, which 

requires PSM to either refrain from reaching the young audiences that such platforms control or acquire the 

relevant rights in order to be able to distribute their content through the subscription-based channel. 

Another issue concerns unilateral (and often unannounced) modifications to the T&Cs imposed by platforms. 

Such modifications concern a range of issues, including the prices that platforms charge for their services 

and de-referencing of PSM sub-domains that led to a significant decline in audiences. 

Other practices which we have experienced and consider unfair include: 

-The removal of legal content; 

-Platforms’ self-preferencing; 

-Platforms’ refusal to grant access to data; 

-Other data-related practices, including: 

-An opportunistic interpretation of the GDPR (e.g. a platform’s insistence that it qualifies as a ‘data controller’ 

whereas it could be regarded as ‘data processor’); 

-Practices to limit the possibilities of data processing by business users on their own applications, even if 

such processing is carried out in compliance with applicable data protection regulation. For example, a 

platform may impede in-app tracking (even when it is compatible with the GDPR and the e-privacy rules) to 

prevent the transmission of data to the app owner. In the case of PSM, this practice impedes the ability to 

engage in audience measurement.

-Practices related to the consent granted by users. Platforms may impose a mechanism of ‘double’ consent 

to trackers, which reduces the quality of the user’s experience. This practice, which may lead to users 

refusing permission, is not a standard required by law in the EU. 

-Lack of transparency regarding ranking (and, more broadly, recommendation mechanisms) and, in the case 

of voice assistants, lack of transparency regarding the parameters that determine the outcome of the user’s 

query. 

-Lack of brand attribution: Platforms may remove logos and/or other distinctive features of their business 

users, including content providers. As a result, the online user is not in the position to assess who offers the 

content she consumes and the content provider cannot establish a relationship with its audiences. 

-Practices concerning monetization, including: 

-Platforms’ refusal to apply to buyers of ad space the general conditions of sale of their business users, 

including content providers. As a result, content providers are prevented from setting their standards of 

compliance with advertising ethics. Moreover, the control exercised by platforms is generally insufficient, 

rendering it possible to publish unsuitable advertisements on the sites of content providers. For example, an 

advertiser (or a buyer acting on behalf of an advertiser) may replace the ad content with other (unsuitable) 

content.      

-Practices raising barriers to monetizing content offered on platforms. Broadly speaking, platforms require 

the use of their own ad tools. This requires a content provider/publisher to either entrust the platform with 

carrying out the ad campaign or implement the technical solution chosen by the platform. In such cases, the 

51

sales process becomes lengthier and the content provider/publisher must cope with the difficulties involved 

in the installation of the tools required by the platforms, the optimization of targeted advertising, etc.

-The placement of ads next to PSM content. In many cases, PSM are legally required to offer online content 

that is ad-free. Yet, platforms may place advertisements next to it without the authorization of the PSM 

concerned and in breach of applicable media regulation. Such practices have significant effects on the 

economy and the society. Recent cases illustrate, self-preferencing prevents services of higher quality from 

reaching the consumer. For PSM, platforms’ preferential treatment of own services may restrict the 

consumption of public interest content. Removal of legal content and bundling/retaliation restrict the fair and 

free flow of information, interfering with freedom of expression. Platforms’ refusal to grant access to data 

deprives the user of the benefits of competition, including a wide range of services to choose from. 

Platforms’ interpretation of the GDPR may undermine data protection.

The following questions are targeted particularly at consumers who are users of large online 

platform companies.

6  Do you encounter issues concerning commercial terms and conditions when 

accessing services provided by large online platform companies?

Please specify which issues you encounter and please explain to what types of 

platform these are related to (e.g. e-commerce marketplaces, app stores, search 

engines, operating systems, social networks).

5000 character(s) maximum

Please see our replies to Questions 3 and 4 above. 

7 Have you considered any of the practices by large online platform companies as 

unfair? Please explain.

3000 character(s) maximum

Please see our replies to Questions 3 and 4 above. 

The following questions are open to all respondents.

9 Are there specific issues and unfair practices you perceive on large online 

platform companies?

5000 character(s) maximum

Please see our replies to Questions 3 and 4 above. 

10 In your view, what practices related to the use and sharing of data in the 

platforms’ environment are raising particular challenges?

5000 character(s) maximum

We are concerned that certain platforms have little incentive to share their data with any firm that might 

compete with them for users’ attention or that may compromise their ability to drive revenues from paid 

52

placement and/or promotion. This is particularly true of vertically integrated platforms where content 

providers such as PSM act as both business users of the online platform and downstream competitors. In 

order to reach a certain number or type of audience, it is difficult for content providers to avoid putting their 

content on them (in other words, these platforms have market power). 

There is an emergence of gatekeeper platforms that commercialize the real estate inventory on their 

platforms. Platforms will sell inventory and package analytics into such deals. These analytics are not made 

available to non-paying parties. There is not a level playing field for PSM when competing with global 

organizations with bountiful resources or the ability to leverage significant debt in a way that is out of reach 

for PSM. 

This concentration of market power enables these platforms to impose broad Terms of Service on their 

users, which can be used to further strengthen their market power. This can occur in two ways:

•        Terms of Service may allow for the collection of vast amounts of user data. The way in which some 

platforms’ business models work (in particular, those that are ad-funded) provides them with an incentive to 

amass large volumes of user data which can be monetized, for example through targeted advertising; and

•        Given that this data is valuable, these platforms are incentivized not to share their data with Third 

Parties. As a result, Terms of Service may include restrictive clauses that prevent data from being shared 

with Third Parties (even where the product being used is provided by the Third Party to the platform)  and we 

note the CMA’s findings in the UK for example that platforms such as Google and Facebook may have 

incentives to take a stricter interpretation of GDPR than is necessary.

The above reality is recognized in the Commission’s Communication on A European Strategy for Data where 

it notes that: ‘data sharing between companies has not taken off at sufficient scale. This is due to a lack of 

economic incentives (including the fear of losing a competitive edge) [and] imbalances in negotiating power. 

[…] A case in point comes from large online platforms, where a small number of players may accumulate 

large amounts of data, gathering important insights and competitive advantages from the richness and 

variety of the data they hold. The high degree of market power resulting from the “data advantage” can 

enable large players to set the rules on the platform and unilaterally impose conditions for access and use of 

data or, indeed, allow leveraging of such “power advantage” when developing new services and expanding 

towards new markets’ (pp. 7-8). 

For more information about our views on what could be effective measures related to data held by platform 

companies beyond those laid down in the General Data Protection Regulation, please see our reply to 

Question 15 below.

11 What impact would the identified unfair  practices can have on innovation, 

competition and consumer choice in the single market?

3000 character(s) maximum

Please see our reply to Question 4 above. 

12 Do startups or scaleups depend on large online platform companies to access 

or expand? Do you observe any trend as regards the level of dependency in the 

last five years (i.e. increases; remains the same; decreases)? Which difficulties in 

your view do start-ups or scale-ups face when they depend on large online platform 

companies to access or expand on the markets?

53

3000 character(s) maximum

13 Which are possible positive and negative societal (e.g. on freedom of 

expression, consumer protection, media plurality) and economic (e.g. on market 

contestability, innovation) effects, if any, of the gatekeeper role that large online 

platform companies exercise over whole platform ecosystem?

3000 character(s) maximum

Please see our reply to Question 4 above. 

14 Which issues specific to the media sector (if any) would, in your view, need to 

be addressed in light of the gatekeeper role of large online platforms? If available, 

please provide additional references, data and facts.

3000 character(s) maximum

The remarks that follow are by no means limited to large, gatekeeper platforms.  

First, legal content offered by PSM organizations, which are editorially responsible for the content they 

produce and disseminate, should not be subject to ‘secondary’ control. Where platforms exercise such 

secondary control, which may lead to the removal of the content concerned, freedom of expression is 

unjustifiably undermined, media pluralism is harmed, and online users are deprived of the ability to access 

public interest content. 

Secondly, public interest content should be made easily findable, prominent and fully attributed.We have 

already referred to certain practices (e.g. self-preferencing, retaliatory/bundling practices) that may limit the 

user’s ability to access public interest content.Measures to support prominence of public interest content 

would increase exposure diversity, thereby promoting media pluralism. 

Moreover, media organizations need access to data generated by or related to their content and services on 

third party platforms.Such data is essential to understand the evolving needs of the audiences they are 

meant to serve.  

Furthermore, platforms must ensure that the brand or other distinctive features of the media organization 

providing content to the online user is clearly visible.Lack of brand attribution not only prevents media 

organizations from establishing a direct relationship with their audiences, but it also prevents users from 

assessing whether the content they consume is distributed by a trustworthy provider. 

In addition, attention must be drawn to issues concerning smart TVs, which act as platforms/intermediaries 

between the viewer and the content provider. For example, consumers using a smart TV can access content 

via a series of apps on the home screen. The order in which these apps are presented will be decided by the 

TV set manufacturer and/or the provider of the TV set’s OS. A recent report prepared for Ofcom shows, the 

order in which apps are presented reinforce ‘the importance of prominence while establishing a role for the 

operating system and user interface as key loci for enabling user choice’ (see Mediatique (2020). Connected 

TV gateways: review of market dynamics, pp. 3-4).It must be noted that many TV manufacturers now use a 

standard OS offered by large platforms. Another striking example of intermediaries is voice assistants. Voice 

assistants usually deliver one search result following the user’s query.This deprives the user of choosing the 

content that may be more relevant to her query or content that is of higher quality. Lack of transparency 

regarding the parameters determining the outcome of the user’s query does not solely raise competition 

concerns.In the case of consumption of news content, it also raises concerns over opinion-forming. 

Finally, media literacy initiatives are essential to raise awareness about how platforms influence the variety 

and quality of content to which users are exposed

54

Regulation of large online platform companies acting as gatekeepers

1 Do you believe that in order to address any negative societal and economic 

effects of the gatekeeper role that large online platform companies exercise over 

whole platform ecosystems, there is a need to consider dedicated regulatory rules?

I fully agree

I agree to a certain extent

I disagree to a certain extent

I disagree

I don’t know

2 Please explain

3000 character(s) maximum

The current regulatory framework is inadequate to address issues arising from platform practices; 

instruments that apply to platforms do not capture concerns that have emerged in recent years. For 

example, we have explained above why the distinction between ‘active’ and ‘passive’ service providers does 

not adequately reflect the role that platforms play in today’s information society. 

Recent attempts to regulate platforms, most notably the P2B Regulation and the New Deal for Consumers, 

focus on transparency. Though lack of transparency is pervasive in the platform economy, we are concerned 

that relevant obligations are not sufficient to address concerns arising from harmful practices. For example, 

while the platform-to-business Regulation imposes on platforms the obligation to disclose whether they grant 

preferential treatment to their own services, large platforms are not prevented from engaging in preferential 

treatment. Yet, as explained above, self-preferencing may significantly harm competition and artificially 

reduce consumer choice. 

General competition law is sufficiently flexible to address some concerns arising from platform practices. 

However, competition law applies only if the companies under scrutiny hold market power and, if a decision 

finding an infringement of the competition rules is adopted, it is binding only on the companies under 

investigation (please see our replies to Questions  below regarding the New Competition Tool). 

Other instruments are inadequate to address concerns arising from platform practices because of the 

subjects they protect (which essentially limits their scope). For example, obligations under the GDPR cover 

only ‘personal data’, that is, information relating to an identifiable individual. As a result, data-related issues 

facing the business users of the platforms, such as access to data and restrictions on the portability of non-

personal data, remain unregulated. 

The EU telecoms rules, meanwhile, regulate access to network infrastructure and do not cover online 

platforms.

In the light of the above and given that consumption of (audiovisual) content online is only expected to 

increase, PSM organizations would clearly benefit from revised and/or new rules that effectively address any 

unfair practices curtailing their ability to reach their audiences.

Even if such rules were limited to platforms acting as gatekeepers, we would encourage the Commission to 

consider establishing obligations for all platforms that act as gateways, despite the fact that their user base is 

not on the same global scale as some of the current tech giants; gatekeeping can occur regardless of size 

and the impact on business users and citizens can be significant. The rules should be applicable to all 

platforms that are being used to a significant extent by their target groups, provided that their customers 

cannot  be effectively accessed any other way (see also our reply to Question III.3. above)

55

3 Do you believe that such dedicated rules should prohibit certain practices by 

large online platform companies with gatekeeper role that are considered 

particularly harmful for users and consumers of these large online platforms?

Yes

No

I don't know

4 Please explain your reply and, if possible, detail the types of prohibitions that 

should in your view be part of the regulatory toolbox.

3000 character(s) maximum

One of the reasons why platforms must be regulated is that, as a result of the large user base they control, 

they have the ability and incentive to engage in harmful practices, exploiting firms that have come to depend 

on them. This is the problem the P2B Regulation seeks to address. The Regulation states ‘Given that 

increasing dependence, [platforms] often have superior bargaining power, which enables them to, in effect 

behave unilaterally in a way that can be unfair and harmful to the legitimate interests of their businesses 

users and, indirectly, also of consumers in the Union’ (Recital (2)). 

However, the P2B Regulation focuses on promoting transparency. 

Competition law is also insufficient to address concerns arising from an increasing dependence on platforms. 

The goal of competition law is to protect competition. The goal of competition law is not to protect individual 

competitors. At best, competition authorities may intervene to protect competitors that are ‘as efficient’ as the 

dominant company under investigation. However, the ‘as efficient competitor test’ is made complicated in the 

digital space by other effects that favor declining costs like the existence of economies of scale and scope, 

learning curve effects, or first mover advantages.

In view of the above, we find that the current legal framework is not adequate to protect platforms’ business 

users against abuses of economic dependence. Given the fast-moving character of platform markets, we 

would encourage the Commission to consider an ‘umbrella provision’ prohibiting the abuse of economic 

dependence, which could set out a non-exhaustive list of abusive practices, such as self-preferencing, 

refusal to grant access to data, and lack of interoperability. This is an approach adopted in several Member 

States (e.g. Greece, Belgium, Germany) in order to fill the lacunae of the legal framework described above. 

Since the problem we are considering here is pan-European and the criteria set by the relevant national laws 

have limited the effectiveness of such ‘umbrella provisions’, a well-designed obligation prohibiting the abuse 

of economic dependence that is established in an EU instrument could address concerns arising from 

harmful platform practices. 

Another solution is to adopt an approach similar to the Directive on unfair trading practices in business-to-

business relationships in the agricultural and food supply chain, which includes a non-exhaustive list of unfair 

trading practices that are prohibited. This is similar to proposals made in a report recently published by the 

UK Consumer and Markets Authority (CMA). The CMA proposes the establishment of an enforceable code 

of conduct for platforms that would be based around three high-level objectives. One of those objectives is 

fair trading, which ‘is intended to address concerns around the potential for exploitative behavior on the part 

of the […] platform’ (CMA (2020). Online platforms and digital advertising, para 7.76

5 Do you believe that such dedicated rules should include obligations on large 

online platform companies with gatekeeper role?

Yes

No

56

I don't know

6 Please explain your reply and, if possible, detail the types of obligations that 

should in your view be part of the regulatory toolbox.

3000 character(s) maximum

Please see our reply to Question 2 above. 

7 If you consider that there is a need for such dedicated rules setting prohibitions 

and obligations, as those referred to in your replies to questions 3 and 5 above, do 

you think there is a need for a specific regulatory authority to enforce these rules?

Yes

No

I don't know

8 Please explain your reply.

3000 character(s) maximum

We find that, in order to ensure the effective implementation of the regulatory rules we are considering here, 

their interpretation and application should not be left only to the courts. There is a need for specific 

authorities with the relevant expertise to enforce these rules. We also find that existing authorities could 

carry out this task. For example, National Competition Authorities (NCAs) and the Directorate General for 

Competition of the European Commission could be entrusted with this mission. Given that platform markets 

are rather complex, there may be a need for the establishment of specific units within the respective 

authorities that have relevant expert knowledge. A similar solution was followed recently in the UK (Digital 

Markets Taskforce within the CMA) and the US (Technology Task Force within the Federal Trade 

Commission). 

Clearly, competition authorities would benefit from cooperation with other authorities, including especially 

data protection authorities and media regulators. This is necessary because, as mentioned in our replies to 

other questions, platform practices may have negative effects on both the economy and the society at large, 

which a regulatory authority must consider holistically.   

9 Do you believe that such dedicated rules should enable regulatory intervention 

against specific large online platform companies, when necessary, with a case by 

case adapted remedies?

Yes

No

I don't know

10 If yes, please explain your reply and, if possible, detail the types of case by case 

remedies.

3000 character(s) maximum

57

11 If you consider that there is a need for such dedicated rules, as referred to in 

question 9 above, do you think there is a need for a specific regulatory authority to 

enforce these rules?

Yes

No

12 Please explain your reply

3000 character(s) maximum

Please see our reply to Question 6 above. 

13 If you consider that there is a need for a specific regulatory authority to enforce 

dedicated rules referred to questions 3, 5 and 9 respectively, would in your view 

these rules need to be enforced by the same regulatory authority or could they be 

enforced by different regulatory authorities? Please explain your reply.

3000 character(s) maximum

Please see our reply to Question 6 above. 

14 At what level should the regulatory oversight of platforms be organised?

At national level

At EU level

Both at EU and national level.

I don't know

15 If you consider such dedicated rules necessary, what should in your view be the 

relationship of such rules with the existing sector specific rules and/or any future 

sector specific rules?

3000 character(s) maximum

We find that the regulatory rules we are considering here could complement existing and future sector-

specific rules. We distinguish between rules that seek to address economic concerns and those that seek to 

address non-economic concerns. 

As regards rules that seek to address economic concerns, we have already referred to the platform-to-

business Regulation and the New Deal for Consumers, which impose transparency obligations on platforms 

in order to address the information asymmetries between platforms and business users/consumers. We 

believe that the rules that would be included in the DSA package should go beyond transparency with a view 

to promoting fairness and competition in the platform economy. 

Rules that seek to address non-economic concerns, including especially rules that promote freedom of 

58

expression and media pluralism (e.g. measures to support prominence of public interest content) must 

prevail over any new horizontal rules in the DSA package. This is because the non-economic objectives are 

different from those aimed at promoting fairness and competition in the platform economy. 

16 Should such rules have an objective to tackle both negative societal and 

negative economic effects deriving from the gatekeeper role of these very large 

online platforms? Please explain your reply.

3000 character(s) maximum

Whilst the principle of subsidiarity generally means that Member States should be primarily responsible for 

adopting regulation dealing with negative societal effects, it is clear that there is an overlap here with 

negative economic effects.  Therefore, the DSA can certainly aim to promote certain societal objectives (and 

therefore to address negative societal effects, such as damage to media pluralism) by addressing negative 

economic effects (for example, by prohibiting self-preferencing, which may have the effect of addressing 

reduced media pluralism).NB: Under the Charter of Fundamental Rights of the EU, both the EU institutions 

and the Member States are bound to implement EU law, including instruments that address economic 

effects with a view to completing the single market, in a way that respects the principles enshrined in the 

Charter (e.g. media pluralism).

Addressing negative societal effects could be achieved by ensuring that certain terms are interpreted in a 

way that takes into account the specificities of digital markets. For example, the revenue generated by large 

platforms (or the share they hold in advertising markets) are not the sole indicators of their ability to produce 

negative economic and societal effects. The user base they control and their ability to shape public opinion 

should also be taken into account in an attempt to tackle the aforementioned effects. 

17 Specifically, what could be effective measures related to data held by very large 

online platform companies with a gatekeeper role beyond those laid down in the 

General Data Protection Regulation in order to promote competition and innovation 

as well as a high standard of personal data protection and consumer welfare?

3000 character(s) maximum

We believe that ex ante regulation is needed to ensure that platforms’ business users have access to data 

generated by or relating to their own services and content. For more details on our position, please see our 

reply to the Commission’s consultation on A European Strategy for Data, which we attach to this document. 

•        Lack of access to data is not addressed by current regulation. In particular: 

-Under the P2B Regulation, access to data remains entirely contingent on a platform’s goodwill or 

commercial strategy. As the Commission itself noted in its Communication on A European Strategy for Data 

(pp. 7-8), powerful platforms are reluctant to share data because they fear they might lose a strong 

competitive advantage.  

-It is becoming increasingly clear that the right to data portability, enshrined in Article 20 GDPR, is not 

adequate to ensure access to data (see, for instance, Special Advisers Report on ‘A Competition Policy for 

the Digital Era’, p. 9).

•        Why do PSM organizations need access to data? 

Data is a key input for PSM as it allows us to tailor our services to users and adapt to the evolving needs of 

59

our audiences. It also helps support value for money through informing where resources are best allocated. 

The refusal of platforms to provide data to PSM continues to have an adverse impact on the ability of PSM to 

develop digital products and to understand content performance, which also impacts on PSM ability to 

commission the best and most relevant content. The refusal of platforms to provide data therefore inhibits 

the industry’s ability to exploit the full range of relevant data on its services which would otherwise be 

available.This in turn may hinder dynamic investment, to the detriment of users.

•        The form of access to data needed 

We have consistently advocated for regulation mandating platforms to grant access to data generated by or 

related to the services and content of platforms’ business users, in full respect of data protection regulation.

We believe that such a right: 

-Respects the right to the protection of personal data enshrined in Article 8 CFREU. Data on consumption of 

the relevant PSM services would need to be transferred in a manner consistent with the GDPR; 

-Is proportionate to the objective it seeks to achieve, as access would be restricted to the data generated by 

or related to the services and content offered by the business user concerned; 

-Is founded on the principle of ‘shared value creation’ which the Commission advocates for in its 

Communication Towards a Common European Data Space (p. 10). This principle is founded on the 

acknowledgment that, where data is generated as a by-product of using a service, several parties have 

contributed to creating the data.

18 What could be effective measures concerning large online platform companies 

with a gatekeeper role in order to promote media pluralism, while respecting the 

subsidiarity principle?

3000 character(s) maximum

Effective measures to promote media pluralism include: 

(1)        prohibiting gatekeeper platforms from self-preferencing in their services, particularly their news, 

search and social media services, so that news from other platforms can be surfaced more easily (NB: We 

believe that, to capture the fast-moving pace of digital markets, the definition of self-preferencing should be 

as broad as possible – see, for instance, Article 7(3) of the platform-to-business Regulation); 

(2)        where appropriate, imposing measures to ensure that gatekeeper platforms take a fair, reasonable 

and non-discriminatory (FRND) approach to allocating search results, ordering display areas and deciding 

on default settings. As a result, larger media companies would not be able to buy out all the relevant slots 

and squeeze out the smaller (often national, rather than international) news providers. For the avoidance of 

confusion, the aforementioned measures should not preclude any measures taken to promote prominence of 

public interest content; and

(3)        Measures to promote algorithmic transparency, especially transparency of algorithms that determine 

access to news content, that go beyond the requirements set by the platform-to-business Regulation (please 

see our reply to Section I.1.B.5 of the Questionnaire). 

Finally, we note that any measures to promote media pluralism must respect the principle of subsidiarity; 

while the EU may take initiatives to advance media pluralism, this should not impede the ability of the 

Member States to adopt relevant rules to serve the social, democratic and cultural needs of the society they 

serve.

60

19 Which, if any, of the following characteristics are relevant when considering the 

requirements for a potential regulatory authority overseeing the large online 

platform companies with the gatekeeper role:

Institutional cooperation with other authorities addressing related sectors – e.

g. competition authorities, data protection authorities, financial services 

authorities, consumer protection authorities, cyber security, etc.

Pan-EU scope

Swift and effective cross-border cooperation and assistance across Member 

States

Capacity building within Member States

High level of technical capabilities including data processing, auditing 

capacities

Cooperation with extra-EU jurisdictions

Other

21 Please explain if these characteristics would need to be different depending on 

the type of ex ante rules (see questions 3, 5, 9 above) that the regulatory authority 

would be enforcing?

3000 character(s) maximum

No, please see our reply to Question 6 above. 

22 Which, if any, of the following requirements and tools could facilitate regulatory 

oversight over very large online platform companies (multiple answers possible):

Reporting obligation on gatekeeping platforms to send a notification to a 

public authority announcing its intention to expand activities

Monitoring powers for the public authority (such as regular reporting)

Investigative powers for the public authority

Other

24 Please explain if these requirements would need to be different depending on 

the type of ex ante rules (see questions 3, 5, 9 above) that the regulatory authority 

would be enforcing?

3000 character(s) maximum

61

25 Taking into consideration 

the parallel consultation on a proposal for a New Competition Tool

 focusing on addressing 

structural competition problems that prevent markets from functioning properly and tilt the level playing field in favour of 

only a few market players. Please rate the suitability of each option below to address market issues arising in online 

platforms ecosystems. Please rate the policy options below from 1 (not effective) to 5 (most effective).

1 (not 

effective)

2 

3 

(somewhat 

(sufficiently 

effective)

effective)

4 (very 

5 (most 

effective)

effective)

Not 

applicable

/No 

relevant 

experience 

or 

knowledge

1. Current competition rules are enough to address issues raised in 

digital markets

2. There is a need for an additional regulatory framework imposing 

obligations and prohibitions that are generally applicable to all large 

online platforms with gatekeeper power

3. There is a need for an additional regulatory framework allowing for 

the possibility to impose tailored remedies on individual large online 

platforms with gatekeeper power, on a case-by-case basis

4. There is a need for a New Competition Tool allowing to address 

structural risks and lack of competition in (digital) markets on a case-by-

case basis.

5. There is a need for combination of two or more of the options 2 to 4.

62

26 Please explain which of the options, or combination of these, would be, in your 

view, suitable and sufficient to address the market issues arising in the online 

platforms ecosystems.

3000 character(s) maximum

As already explained above, though flexible, the current competition rules are not sufficient to address 

concerns arising from platform practices that have emerged in recent years. For example, Article 102 TFEU 

only applies to dominant companies. However, the ‘dominance’ threshold is very high and platforms may 

pose concerns to competition without necessarily being dominant. One of the policy options the Commission 

is currently considering is a New Competition Tool that would apply to non-dominant platforms. We would 

strongly support such an approach (for a more detailed analysis of the reasons why we support the 

aforementioned policy option, please see our reply to the consultation on the New Competition Tool). 

However, the ongoing process of reform of competition policy, including the adoption of a New Competition 

Tool, is not sufficient to promote fairness in the platform economy. Though several steps have been (and will 

be) taken to ensure that competition law is adapted to the specificities of digital markets, the reform of 

competition policy will not challenge the foundations of competition law. As already mentioned above, the 

goal of competition law is to protect competition. The goal of competition law is not to protect individual 

competitors. At best, competition authorities may intervene to protect competitors that are ‘as efficient’ as the 

dominant company under investigation. However, the ‘as efficient competitor test’ is made complicated in the 

digital space by other effects that favor declining costs like the existence of economies of scale and scope, 

learning curve effects, or first mover advantages. Moreover, competition law is ex post and therefore is often 

engaged only after substantial harm has already been done to firms operating in the market. Furthermore, 

competition law is expensive to enforce and investigations take time to reach a conclusion. Further 

regulation is required to mitigate the harm of online platform and ensure third parties can operate and 

innovate within digital markets. 

Similarly, in order to understand whether a conduct or transaction is harmful to competition, competition 

authorities must assess its effects on competition/the market as a whole. In other words, competition 

authorities do not assess its effects on individual competitors or customers. And yet, to ensure a well-

functioning marketplace, those competitors or customers may merit protection even if there are no actual or 

potential effects on the competitive process. In view of the above, competition rules must be complemented 

with effective regulatory rules that promote fairness, to the benefit of competition and consumers/citizens 

alike

27 Are there other points you would like to raise?

3000 character(s) maximum

IV. Other emerging issues and opportunities, including online advertising 

and smart contracts

Online advertising has substantially evolved over the recent years and represents a major revenue source 

for many digital services, as well as other businesses present online, and opens unprecedented 

opportunities for content creators, publishers, etc. To a large extent, maximising revenue streams and 

optimising online advertising are major business incentives for the business users of the online platforms 

63

and for shaping the data policy of the platforms. At the same time, revenues from online advertising as well 

as increased visibility and audience reach are also a major incentive for potentially harmful intentions, e.g. 

in online disinformation campaigns.

Another emerging issue is linked to the conclusion of ‘smart contracts’ which represent an important 

innovation for digital and other services, but face some legal uncertainties.

This section of the open public consultation seeks to collect data, information on current practices, and 

informed views on potential issues emerging in the area of online advertising and smart contracts. 

Respondents are invited to reflect on other areas where further measures may be needed to facilitate 

innovation in the single market. This module does not address privacy and data protection concerns; all 

aspects related to data sharing and data collection are to be afforded the highest standard of personal data 

protection.

Online advertising

1 When you see an online ad, is it clear to you who has placed it online?

Yes, always

Sometimes: but I can find the information when this is not immediately clear

Sometimes: but I cannot always find this information

I don’t know

No

64

2 As a publisher online (e.g. owner of a website where ads are displayed), what types of advertising systems do you use 

for covering your advertising space? What is their relative importance?

% of ad space

% of ad revenue

Intermediated programmatic advertising 

though real-time bidding

Private marketplace auctions

Programmatic advertising with guaranteed 

impressions (non-auction based)

Behavioural advertising (micro-targeting)

Contextual advertising

Other

65

3 What information is publicly available about ads displayed on an online platform 

that you use?

3000 character(s) maximum

4 As a publisher, what type of information do you have about the advertisement 

placed next to your content/on your website?

3000 character(s) maximum

5 To what extent do you find the quality and reliability of this information 

satisfactory for your purposes?

Please rate your level of satisfaction

66

 
 
 
 
6 As an advertiser or an agency acting on behalf of the advertiser (if applicable), what types of programmatic advertising 

do you use to place your ads? What is their relative importance in your ad inventory?

% of ad inventory

% of ad expenditure

Intermediated programmatic advertising 

though real-time bidding

Private marketplace auctions

Programmatic advertising with guaranteed 

impressions (non-auction based)

Behavioural advertising (micro-targeting)

Contextual advertising

Other

67

7 As an advertiser or an agency acting on behalf of the advertiser (if applicable), 

what type of information do you have about the ads placed online on your behalf?

3000 character(s) maximum

8 To what extent do you find the quality and reliability of this information 

satisfactory for your purposes?

Please rate your level of satisfaction

The following questions are targeted specifically at online platforms.

10 As an online platform, what options do your users have with regards to the 

advertisements they are served and the grounds on which the ads are being 

served to them? Can users access your service through other conditions than 

viewing advertisements? Please explain.

3000 character(s) maximum

11 Do you publish or share with researchers, authorities or other third parties 

detailed data on ads published, their sponsors and viewership rates? Please 

explain.

3000 character(s) maximum

12 What systems do you have in place for detecting illicit offerings in the ads you 

intermediate?

3000 character(s) maximum

The following questions are open to all respondents.

14 Based on your experience, what actions and good practices can tackle the 

placement of ads next to illegal content or goods, and/or on websites that 

disseminate such illegal content or goods, and to remove such illegal content or 

goods when detected?

3000 character(s) maximum

68

 
 
 
 
15 From your perspective, what measures would lead to meaningful transparency 

in the ad placement process?

3000 character(s) maximum

16 What information about online ads should be made publicly available?

3000 character(s) maximum

17 Based on your expertise, which effective and proportionate auditing systems 

could bring meaningful accountability in the ad placement system?

3000 character(s) maximum

18 What is, from your perspective, a functional definition of ‘political advertising’? 

Are you aware of any specific obligations attached to 'political advertising' at 

national level ?

3000 character(s) maximum

19 What information disclosure would meaningfully inform consumers in relation to 

political advertising? Are there other transparency standards and actions needed, 

in your opinion, for an accountable use of political advertising and political 

messaging?

3000 character(s) maximum

First of all, we wish to point out that there are huge differences in national rules applicable to political 

advertising. A wide array of requirements are imposed on broadcast media, in particular during elections. 

These may include, inter alia, the requirement to cover elections in a fair, balanced and impartial manner, 

rules on the allocation of airtime for political parties/candidates, rules regarding paid political advertising or 

rules on reflection/silence periods. The rationale for such rules is to facilitate the pluralistic expression of 

opinions during electoral campaigns, the very motor of democratic societies. 

While a great proportion of the population continues to inform itself about elections and follows election 

coverage on TV, citizens, in particular young users, increasingly access news via social media networks, 

particularly young users aged between 18 and 24 (Reuters Institute Digital News Report 2020, p. 10/11). 

The strict requirements imposed on broadcasters are therefore in stark contrast to the lack of rules in relation 

to online platforms and no longer reflect citizens’ opinion-forming processes as well as online platform’s 

impact on society, particularly in crucial times of elections. Self-regulatory initiatives or voluntary measures 

69

like the Code of Practice on Disinformation no longer suffice to ensure that citizens are properly informed 

about the sponsors and reasons why they see political commercial messages (see Kirk/Culloty/Casey

/Teeling/Park/Kearns/Suiter, Elect Check 2019, pp.38-40, ERGA Report on Disinformation, pp.18-19). 

Disinformation coupled with secluded methods to target the electorate with political advertisements can have 

a devastating effect on elections’ impartiality and integrity. 

This should not be tolerated and this is why we believe that the DSA should set out binding minimum 

standards for political advertising displayed on online platforms. It needs to be ensured that Member States 

can impose stricter standards in their territories, on account of the significance of elections for democratic 

societies. At the very least, online platforms should clearly separate editorial from commercial content, 

allowing users to quickly identify political advertisements. Political advertisements should thus be presented 

in such a way as to be readily recognisable as a paid-for communication or labelled as such. In addition, 

online platforms should be transparent about the identity of the sponsor and possibly disclose the amounts 

spent. Importantly, the EU’s minimum set of rules should be without prejudice to existing national rules, in 

particular those applicable to broadcast media. 

20 What impact would have, in your view, enhanced transparency and 

accountability in the online advertising value chain, on the gatekeeper power of 

major online platforms and other potential consequences such as media pluralism?

3000 character(s) maximum

21 Are there other emerging issues in the space of online advertising you would 

like to flag?

3000 character(s) maximum

Smart contracts

1 Is there sufficient legal clarity in the EU for the provision and use of “smart 

contracts” – e.g. with regard to validity, applicable law and jurisdiction?

Please rate from 1 (lack of clarity) to 5 (sufficient clarity)

2 Please explain the difficulties you perceive.

3000 character(s) maximum

3 In which of the following areas do you find necessary further regulatory clarity?

Mutual recognition of the validity of smart contracts in the EU as concluded 

in accordance with the national law

Minimum standards for the validity of “smart contracts” in the EU

70

 
 
 
 
Measures to ensure that legal obligations and rights flowing from a smart 

contract and the functioning of the smart contract are clear and 

unambiguous, in particular for consumers

Allowing interruption of smart contracts

Clarity on liability for damage caused in the operation of a smart contract

Further clarity for payment and currency-related smart contracts.

4 Please explain.

3000 character(s) maximum

5 Are there other points you would like to raise?

3000 character(s) maximum

V. How to address challenges around the situation of self-employed 

individuals offering services through online platforms?

Individuals providing services through platforms may have different legal status (workers or self-employed). 

This section aims at gathering first information and views on the situation of self-employed individuals 

offering services through platforms (such as ride-hailing, food delivery, domestic work, design work, micro-

tasks etc.). Furthermore, it seeks to gather first views on whether any detected problems are specific to the 

platform economy and what would be the perceived obstacles to the improvement of the situation of 

individuals providing services through platforms. This consultation is not intended to address the criteria by 

which persons providing services on such platforms are deemed to have one or the other legal status. 

The issues explored here do not refer to the selling of goods (e.g. online marketplaces) or the sharing of 

assets (e.g. sub-renting houses) through platforms.

The following questions are targeting self-employed individuals offering services through online 

platforms.

Relationship with the platform and the final customer

1 What type of service do you offer through platforms?

Food-delivery

Ride-hailing

Online translations, design, software development or micro-tasks

On-demand cleaning, plumbing or DIY services

Other, please specify

71

2 Please explain.

3 Which requirements were you asked to fulfill in order to be accepted by the 

platform(s) you offer services through, if any?

4 Do you have a contractual relationship with the final customer?

Yes

No

5 Do you receive any guidelines or directions by the platform on how to offer your 

services?

Yes

No

7 Under what conditions can you stop using the platform to provide your services, 

or can the platform ask you to stop doing so?

8 What is your role in setting the price paid by the customer and how is your 

remuneration established for the services you provide through the platform(s)?

9 What are the risks and responsibilities you bear in case of non-performance of 

the service or unsatisfactory performance of the service?

Situation of self-employed individuals providing services through platforms

10 What are the main advantages for you when providing services through 

platforms?

3000 character(s) maximum

72

11 What are the main issues or challenges you are facing when providing services 

through platforms? Is the platform taking any measures to improve these?

3000 character(s) maximum

12 Do you ever have problems getting paid for your service? Does/do the platform 

have any measures to support you in such situations?

3000 character(s) maximum

13 Do you consider yourself in a vulnerable or dependent situation in your work 

(economically or otherwise), and if yes, why?

14 Can you collectively negotiate vis-à-vis the platform(s) your remuneration or 

other contractual conditions?

Yes

No

15 Please explain.

The following questions are targeting online platforms.

Role of platforms

17 What is the role of your platform in the provision of the service and the 

conclusion of the contract with the customer?

18 What are the risks and responsibilities borne by your platform for the non-

performance of the service or unsatisfactory provision of the service?

19 What happens when the service is not paid for by the customer/client?

73

20 Does your platform own any of the assets used by the individual offering the 

services?

Yes

No

22 Out of the total number of service providers offering services through your 

platform, what is the percentage of self-employed individuals?

Over 75%

Between 50% and 75%

Between 25% and 50%

Less than 25%

Rights and obligations

23 What is the contractual relationship between the platform and individuals 

offering services through it?

3000 character(s) maximum

24 Who sets the price paid by the customer for the service offered?

The platform

The individual offering services through the platform

Others, please specify

25 Please explain.

3000 character(s) maximum

26 How is the price paid by the customer shared between the platform and the 

individual offering the services through the platform?

3000 character(s) maximum

27 On average, how many hours per week do individuals spend offering services 

through your platform?

3000 character(s) maximum

74

28 Do you have measures in place to enable individuals providing services through 

your platform to contact each other and organise themselves collectively? 

Yes

No

29 Please describe the means through which the individuals who provide services 

on your platform contact each other.

3000 character(s) maximum

30 What measures do you have in place for ensuring that individuals offering 

services through your platform work legally - e.g. comply with applicable rules on 

minimum working age, hold a work permit, where applicable - if any? 

(If you replied to this question in your answers in the first module of the 

consultation, there is no need to repeat your answer here.)

3000 character(s) maximum

The following questions are open to all respondents

Situation of self-employed individuals providing services through platforms

32 Are there areas in the situation of individuals providing services through 

platforms which would need further improvements? Please rate the following issues 

from 1 (no improvements needed) to 5 (substantial issues need to be addressed).

1 (no 

5 (substantial 

improvements 

2

3

4

improvements 

needed)

needed)

I don't 

know / 

No 

answer

Earnings

Flexibility of choosing when and /or 

where to provide services

Transparency on remuneration

Measures to tackle non-payment of 

remuneration

Transparency in online ratings

75

Ensuring that individuals providing 

services through platforms can 

contact each other and organise 

themselves for collective purposes

Tackling the issue of work carried 

out by individuals lacking legal 

permits

Prevention of discrimination of 

individuals providing services 

through platforms, for instance 

based on gender, racial or ethnic 

origin

Allocation of liability in case of 

damage

Other, please specify

33 Please explain the issues that you encounter or perceive.

3000 character(s) maximum

34 Do you think individuals providing services in the 'offline/traditional' economy 

face similar issues as individuals offering services through platforms? 

Yes

No

I don't know

35 Please explain and provide examples.

3000 character(s) maximum

36 In your view, what are the obstacles for improving the situation of individuals 

providing services

1.  

through platforms?

2.  

in the offline/traditional economy?

3000 character(s) maximum

76

37 To what extent could the possibility to negotiate collectively help improve the 

situation of individuals offering services:

through online platforms?

in the offline/traditional economy?

38 Which are the areas you would consider most important for you to enable such 

collective negotiations?

3000 character(s) maximum

39 In this regard, do you see any obstacles to such negotiations?

3000 character(s) maximum

40 Are there other points you would like to raise?

3000 character(s) maximum

VI. What governance for reinforcing the Single Market for digital services?

The EU’s Single Market offers a rich potential for digital services to scale up, including for innovative 

European companies. Today there is a certain degree of legal fragmentation in the Single Market . One of 

the main objectives for the Digital Services Act will be to improve opportunities for innovation and ‘deepen 

the Single Market for Digital Services

’. 

This section of the consultation seeks to collect evidence and views on the current state of the single 

market and steps for further improvements for a competitive and vibrant Single market for digital services. 

This module also inquires about the relative impact of the COVID-19 crisis on digital services in the Union.

It then focuses on the appropriate governance and oversight over digital services across the EU and means 

to enhance the cooperation across authorities for an effective supervision of services and for the equal 

protection of all citizens across the single market. It also inquires about specific cooperation arrangements 

such as in the case of consumer protection authorities across the Single Market, or the regulatory oversight 

and cooperation mechanisms among media regulators. This section is not intended to focus on the 

enforcement of  EU data protection rules (GDPR).

Main issues

1 How important are - in your daily life or for your professional transactions - digital 

services such as accessing websites, social networks, downloading apps, reading 

news online, shopping online, selling products online?

77

 
 
 
 
 
 
 
 
Overall

Those offered from outside of your Member State of 

establishment

The following questions are targeted at digital service providers

3 Approximately, what share of your EU turnover is generated by the provision of 

your service outside of your main country of establishment in the EU?

Less than 10%

Between 10% and 50%

Over 50%

I cannot compute this information

78

 
 
 
 
 
 
 
 
4 To what extent are the following obligations a burden for your company in providing its digital services, when expanding 

to one or more EU Member State(s)? Please rate the following obligations from 1 (not at all burdensome) to 5 (very 

burdensome).

1 (not at all 

burdensome)

2

3 

(neutral)

4

5 (very 

I don't 

know / 

burdensome)

No 

answer

Different processes and obligations imposed by Member States for notifying, 

detecting and removing illegal content/goods/services

Requirements to have a legal representative or an establishment in more than one 

Member State

Different procedures and points of contact for obligations to cooperate with authorities

Other types of legal requirements. Please specify below

79

6 Have your services been subject to enforcement measures by an EU Member 

State other than your country of establishment?

Yes

No

I don't know

8 Were you requested to comply with any ‘prior authorisation’ or equivalent 

requirement for providing your digital service in an EU Member State?

Yes

No

I don't know

10 Are there other issues you would consider necessary to facilitate the provision 

of cross-border digital services in the European Union?

3000 character(s) maximum

11 What has been the impact of COVID-19 outbreak and crisis management 

measures on your business’ turnover

Significant reduction of turnover

Limited reduction of turnover

No significant change

Modest increase in turnover

Significant increase of turnover

Other

13 Do you consider that deepening of the Single Market for digital services could 

help the economic recovery of your business?

Yes

No

I don't know

14 Please explain

3000 character(s) maximum

80

The following questions are targeted at all respondents.

Governance of digital services and aspects of enforcement

The ‘country of origin’ principle is the cornerstone of the Single Market for digital services. It ensures that 

digital innovators, including start-ups and SMEs, have a single set of rules to follow (that of their home 

country), rather than 27 different rules. 

This is an important precondition for services to be able to scale up quickly and offer their services across 

borders. In the aftermath of the COVID-19 outbreak and effective recovery strategy, more than ever, a 

strong Single Market is needed to boost the European economy and to restart economic activity in the EU. 

At the same time, enforcement of rules is key; the protection of all EU citizens regardless of their place of 

residence, will be in the centre of the Digital Services Act.

The current system of cooperation between Member States foresees that the Member State where a 

provider of a digital service is established has the duty to supervise the services provided and to ensure 

that all EU citizens are protected. A cooperation mechanism for cross-border cases is established in the E-

Commerce Directive.

1 Based on your experience, how would you assess the cooperation in the Single 

Market between authorities entrusted to supervise digital services?

5000 character(s) maximum

2 What governance arrangements would lead to an effective system for supervising 

and enforcing rules on online platforms in the EU in particular as regards the 

intermediation of third party goods, services and content (See also Chapter 1 of the 

consultation)? 

Please rate each of the following aspects, on a scale of 1 (not at all important) to 5 

(very important).

1 (not at 

all 

2

important)

3 

(neutral)

4

5 (very 

I don't 

know / 

important)

No 

answer

Clearly assigned competent national 

authorities or bodies as established by 

Member States for supervising the 

systems put in place by online platforms

Cooperation mechanism within 

Member States across different 

competent authorities responsible for 

the systematic supervision of online 

platforms and sectorial issues (e.g. 

81

consumer protection, market 

surveillance, data protection, media 

regulators, anti-discrimination 

agencies, equality bodies, law 

enforcement authorities etc.)

Cooperation mechanism with swift 

procedures and assistance across 

national competent authorities across 

Member States

Coordination and technical assistance 

at EU level

An EU-level authority

Cooperation schemes with third parties 

such as civil society organisations and 

academics for specific inquiries and 

oversight

Other: please specify in the text box 

below

3 Please explain

5000 character(s) maximum

4 What information should competent authorities make publicly available about 

their supervisory and enforcement activity?

3000 character(s) maximum

5 What capabilities – type of internal expertise, resources etc. - are needed within 

competent authorities, in order to effectively supervise online platforms?

3000 character(s) maximum

6 In your view, is there a need to ensure similar supervision of digital services 

established outside of the EU that provide their services to EU users?

Yes, if they intermediate a certain volume of content, goods and services 

provided in the EU

Yes, if they have a significant number of users in the EU

No

82

Other

I don’t know

7 Please explain

3000 character(s) maximum

8 How should the supervision of services established outside of the EU be set up in 

an efficient and coherent manner, in your view?

3000 character(s) maximum

Given the inherent international dimension of the provision of digital services and similar to other regulatory 

fields and instruments (e.g. General Data Protection Regulation, Platform-to-Business Regulation, 

competition law), the future legal framework should cover providers established outside the EU offering 

digital services to users in the EU (see our answer to question 6 above). The lack of an establishment in the 

EU should not relieve foreign platform providers from complying with the future DSA. In this respect, it is 

important to establish a supervisory scheme that ensures effective enforcement of the new rules, especially 

in relation to non-European providers. 

The supervisory system under the DSA may build on elements from other EU legal instruments, notably the 

General Data Protection Regulation (GDPR), while at the same time learning from its weaknesses. To 

guarantee the uniform application and implementation of the DSA, competent regulatory authorities should 

closely cooperate, where possible within existing bodies (i.e. ERGA in the field of media regulation, see our 

answer to question 11). Within these bodies, international cooperation mechanisms could be established, 

including engagement of relevant stakeholders, for example as regards content-related responsibilities (see 

for example Art. 50 GDPR). 

In addition, regulators should provide mutual assistance and be empowered to conduct joint operations. In 

relation to providers established outside the EU, the GDPR’s “one-stop-shop" or “lead supervisory authority” 

concepts (see for example Art. 60-62 GDPR) could be employed, while making sure that all relevant 

regulators collaborate closely and prevent a situation where one regulator’s capacities are over-stretched. 

Non-European service providers should also designate a representative in the EU or within each Member 

State where the platform provider pursues significant activities. This would not only greatly facilitate 

interaction with regulatory authorities (for the purpose of supervision) but the representative could also serve 

as contact point for content providers (see our answer to questions 5 and 6 in the chapter on 

Responsibilities). To facilitate communication, the representative’s identify and contact details should be 

made easily accessible alongside other information about platform providers (see our answer to question 17 

in the Responsibilities chapter). While the designation of a representative should not amount to an 

establishment understood under EU law, it should imply a meaningful presence that is more than just a 

letterbox

9 In your view, what governance structure could ensure that multiple national 

authorities, in their respective areas of competence, supervise digital services 

coherently and consistently across borders?

3000 character(s) maximum

83

 
 
10 As regards specific areas of competence, such as on consumer protection or 

product safety, please share your experience related to the cross-border 

cooperation of the competent authorities in the different Member States.

3000 character(s) maximum

11 In the specific field of audiovisual, the Audiovisual Media Services Directive 

established a regulatory oversight and cooperation mechanism in cross border 

cases between media regulators, coordinated at EU level within European 

Regulators’ Group for Audiovisual Media Services (ERGA). In your view is this 

sufficient to ensure that users remain protected against illegal and harmful 

audiovisual content (for instance if services are offered to users from a different 

Member State)? Please explain your answer and provide practical examples if you 

consider the arrangements may not suffice.

3000 character(s) maximum

Member States are competent to establish and organise the regulatory authorities that oversee media 

markets. They thus determine, among others, the authorities’ status, structure, funding and competencies. 

Specialized media regulators or otherwise media units of converged regulators have ample experience as 

well as vast expertise in content regulation and are thus the best placed to protect users against illegal and 

certain forms of harmful content disseminated online. In exercising their competence under national law, 

media regulators take due account of the fundamental rights (above all, the freedom of expression and right 

to information) and general interest objectives (e.g. media pluralism and media freedom, cultural and 

linguistic diversity) that lie at the heart of media regulation. When enlarging their field of activities to digital 

services, which is necessary given these providers importance in distributing and granting access to content 

services, media regulators should improve and facilitate cross-border or even pan-European coordination 

within ERGA. This is necessary to improve understanding of national sensitivities and priorities, but also 

because the provision of digital services is inherently cross-border, due to the borderless nature of the 

Internet and the technologies underlying it. 

The internal market permits businesses from inside and outside Europe to offer services to users in the EU 

or to locate in one Member State and to target users in (an)other Member State(s). The country-of-origin 

principle that has facilitated the growth of digital services is not, however, absolute and reasons for 

derogating from it need to be qualified for the purpose of securing above-mentioned fundamental rights and 

general interest objectives. The derogations must, in particular, safeguard Member States’ competence to 

promote cultural and linguistic diversity and to ensure the defence of pluralism – including, where necessary, 

in a cross-border manner.  

To fully protect these values, the future legal framework must also grant precedence to EU sectoral law (e.g. 

the Audiovisual Media Services Directive or the Directive on Copyright in the Digital Single Market). To 

respect the distribution of competences between the EU and the Member States, the DSA must include a 

clear statement that sector-specific legislation prevails over EU-wide harmonised horizontal rules, in line with 

84

 
 
current conflict clauses that have characterized the relationship of sectoral instruments to the E-Commerce 

Directive (e.g. Art. 4(7) AVMSD or first sentence of Art. 17(3) DSM Directive).

12 Would the current system need to be strengthened? If yes, which additional 

tasks be useful to ensure a more effective enforcement of audiovisual content 

rules?

Please assess from 1 (least beneficial) – 5 (most beneficial). You can assign the 

same number to the same actions should you consider them as being equally 

important.

Coordinating the handling of cross-border cases, including jurisdiction 

matters

Agreeing on guidance for consistent implementation of rules under the 

AVMSD

Ensuring consistency in cross-border application of the rules on the 

promotion of European works

Facilitating coordination in the area of disinformation

Other areas of cooperation

13 Other areas of cooperation - (please, indicate which ones)

3000 character(s) maximum

14 Are there other points you would like to raise?

3000 character(s) maximum

Final remarks

If you wish to upload a position paper, article, report, or other evidence and data for the attention of the 

European Commission, please do so.

1 Upload file

85

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The maximum file size is 1 MB

Only files of the type pdf,txt,doc,docx,odt,rtf are allowed

eb446d01-e634-496f-b67a-3be9041eeee0

/EBU_executive_summary_DSA_response_and_EBU_Reply_to_EU_Strategy_for_Data.pdf

2 Other final comments

3000 character(s) maximum

Useful links

Digital Services Act package (https://ec.europa.eu/digital-single-market/en/digital-services-act-package )

Background Documents

(BG) Речник на термините

(CS) Glosř

(DA) Ordliste

(DE) Glossar

(EL) ά

(EN) Glossary

(ES) Glosario

(ET) Snastik

(FI) Sanasto

(FR) Glossaire

(HR) Pojmovnik

(HU) Glosszrium

(IT) Glossario

(LT) Žodynėlis

(LV) Glosārijs

(MT) Glossarju

(NL) Verklarende woordenlijst

(PL) Słowniczek

(PT) Glossrio

(RO) Glosar

(SK) Slovnk

(SL) Glosar

(SV) Ordlista

Contact

86

CNECT-consultation-DSA@ec.europa.eu

87

