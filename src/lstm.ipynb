{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Long Short-Term Memory Network (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a step-by-step tutorial on how to approach the text classification task using the LSTM model, which belongs to the family of Recurrent Neural Networks.\n",
    "\n",
    "LSTM models handle long term dependencies in text through a RNN architecture that uses three different types of gates - input, output, and forget gates. The gates operate together to decide which information to retain in the LSTM cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sn\n",
    "\n",
    "# data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 143 entries, 0 to 142\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   Reference                     143 non-null    object\n",
      " 1   Feedback date                 143 non-null    object\n",
      " 2   User type                     112 non-null    object\n",
      " 3   Scope                         4 non-null      object\n",
      " 4   Organisation name             112 non-null    object\n",
      " 5   Transparency register number  93 non-null     object\n",
      " 6   Organisation size             112 non-null    object\n",
      " 7   label_132                     143 non-null    object\n",
      " 8   label_134                     143 non-null    object\n",
      " 9   submit                        143 non-null    int64 \n",
      " 10  file_name                     73 non-null     object\n",
      " 11  language                      143 non-null    object\n",
      " 12  text                          143 non-null    object\n",
      " 13  text_clean                    143 non-null    object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 16.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_json(r\"../data/df_final_document.json\") \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf submissions only\n",
    "df = df[df['submit']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['label.132'] = le.fit_transform(df['label_132'])\n",
    "df['label.134'] = le.fit_transform(df['label_134'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Prepare input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(df['text_clean'], df['label.132'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label.132'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataset \n",
    "train_dataset = list(zip(train_labels, train_text))\n",
    "test_dataset = list(zip(test_labels, test_text))\n",
    "\n",
    "# convert pd series to list\n",
    "train_text = train_text.tolist()\n",
    "test_text = test_text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data into training and test set, build the corpus vocabulary by tokenizing all texts and assigning each word to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def tokenize(datasets):\n",
    "    for dataset in datasets:\n",
    "        for text in dataset:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenize([train_text, test_text]), min_freq=1, specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12540"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1273, 691, 5528, 1]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tokens = tokenizer(\"This is an example.\")\n",
    "index = vocab(tokens)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary is built, we create several batches of text sequences and map the tokens to indices. We also pad the sequence of words so all are of the same length. This returns a tensor of the sequence length and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\"0\", \"1\"]\n",
    "max_words = 100\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X] # map tokes to index using vocab\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] # pad sequences\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=100, collate_fn=vectorize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 100]) torch.Size([51])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "embed_len = 50\n",
    "hidden_dim = 20\n",
    "n_layers=1\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_len = embed_len\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim), torch.randn(n_layers, len(X_batch), hidden_dim)\n",
    "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
    "        return self.linear(output[:,-1])\n",
    "\n",
    "    def init_hidden(self):\n",
    "      return (\n",
    "               torch.zeros(n_layers, 1, self.hidden_dim, device=device),\n",
    "               torch.zeros(n_layers, 1, self.hidden_dim, device=device)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(12540, 50)\n",
       "  (lstm): LSTM(50, 20, batch_first=True)\n",
       "  (linear): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_classifier = LSTMClassifier()\n",
    "lstm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer : Embedding(12540, 50)\n",
      "Parameters : \n",
      "torch.Size([12540, 50])\n",
      "\n",
      "Layer : LSTM(50, 20, batch_first=True)\n",
      "Parameters : \n",
      "torch.Size([80, 50])\n",
      "torch.Size([80, 20])\n",
      "torch.Size([80])\n",
      "torch.Size([80])\n",
      "\n",
      "Layer : Linear(in_features=20, out_features=2, bias=True)\n",
      "Parameters : \n",
      "torch.Size([2, 20])\n",
      "torch.Size([2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in lstm_classifier.children():\n",
    "    print(\"Layer : {}\".format(layer))\n",
    "    print(\"Parameters : \")\n",
    "    for param in layer.parameters():\n",
    "        print(param.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 2])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = lstm_classifier(torch.randint(0, len(vocab), (1024, max_words)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "\n",
    "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            Y_preds = model(X)\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y) \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            ## back propagation\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        evaluate(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.706\n",
      "Valid Loss : 0.733\n",
      "Valid Acc  : 0.318\n",
      "Confusion matrix:\n",
      "[[ 6  4]\n",
      " [11  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.700\n",
      "Valid Loss : 0.732\n",
      "Valid Acc  : 0.318\n",
      "Confusion matrix:\n",
      "[[ 6  4]\n",
      " [11  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.693\n",
      "Valid Loss : 0.732\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [9 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.687\n",
      "Valid Loss : 0.731\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.681\n",
      "Valid Loss : 0.730\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.674\n",
      "Valid Loss : 0.730\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.668\n",
      "Valid Loss : 0.730\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.662\n",
      "Valid Loss : 0.729\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.656\n",
      "Valid Loss : 0.729\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.649\n",
      "Valid Loss : 0.728\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.643\n",
      "Valid Loss : 0.728\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[6 4]\n",
      " [8 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.637\n",
      "Valid Loss : 0.727\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[5 5]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.630\n",
      "Valid Loss : 0.727\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[5 5]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.624\n",
      "Valid Loss : 0.727\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[5 5]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.618\n",
      "Valid Loss : 0.726\n",
      "Valid Acc  : 0.455\n",
      "Confusion matrix:\n",
      "[[5 5]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.611\n",
      "Valid Loss : 0.726\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[4 6]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.605\n",
      "Valid Loss : 0.726\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[4 6]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.598\n",
      "Valid Loss : 0.725\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[4 6]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.592\n",
      "Valid Loss : 0.725\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[4 6]\n",
      " [7 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.585\n",
      "Valid Loss : 0.725\n",
      "Valid Acc  : 0.409\n",
      "Confusion matrix:\n",
      "[[4 6]\n",
      " [7 5]]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_classifier = LSTMClassifier()\n",
    "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "train(lstm_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* https://www.kaggle.com/code/mehmetlaudatekman/lstm-text-classification-pytorch/notebook\n",
    "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('watermelon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d937f221309e89928364c8f95c616932ec42e1a35308f59658ba617ce90f29fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
