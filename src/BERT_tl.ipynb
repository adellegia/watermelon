{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json(r\"../data/df_final_para.json\") # paragraphs - 3776\n",
    "df = pd.read_json(r\"../data/df_final_document.json\") # document - 143 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 143 entries, 0 to 142\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   Reference                     143 non-null    object\n",
      " 1   Feedback date                 143 non-null    object\n",
      " 2   User type                     112 non-null    object\n",
      " 3   Scope                         4 non-null      object\n",
      " 4   Organisation name             112 non-null    object\n",
      " 5   Transparency register number  93 non-null     object\n",
      " 6   Organisation size             112 non-null    object\n",
      " 7   label_132                     143 non-null    object\n",
      " 8   label_134                     143 non-null    object\n",
      " 9   submit                        143 non-null    int64 \n",
      " 10  file_name                     73 non-null     object\n",
      " 11  language                      143 non-null    object\n",
      " 12  text                          143 non-null    object\n",
      " 13  text_clean                    143 non-null    object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 16.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label.132'] = le.fit_transform(df['label_132'])\n",
    "df['label.134'] = le.fit_transform(df['label_134'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    73\n",
       "0    70\n",
       "Name: submit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['submit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use pdf submissions\n",
    "df = df[df['submit']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(by='count', ascending=False)[['Reference', 'count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(df['count']).hist(bins = 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import yaml\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, test, validation sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text_clean'], df['label.132'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label.132'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text))\n",
    "print(len(val_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# bert-base-uncased\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# legal-bert-base-uncased\n",
    "bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "seq_len = [len(str(i).split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 2\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      self.relu =  nn.ReLU()\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to freeze all the parameters if freeze = T\n",
    "def set_parameter_requires_grad(model, freeze):\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters\n",
    "set_parameter_requires_grad(model=bert, freeze=True)\n",
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5) # learning rate\n",
    "\n",
    "# define the loss function\n",
    "criterion  = nn.NLLLoss() \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "  \n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  running_total_correct = 0.0\n",
    "  total_preds=[]\n",
    "  \n",
    "  for i, inputs in enumerate((dataloader)):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [r.to(device) for r in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    model.zero_grad()        \n",
    "\n",
    "    # forward + backward + optimize \n",
    "    preds = model(sent_id, mask)\n",
    "    loss = criterion(preds, labels)\n",
    "    total_loss = total_loss + loss.item()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
    "    optimizer.step()\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # epoch loss and accuracy\n",
    "  epoch_loss = total_loss / len(dataloader)\n",
    "  # reshape from (no. of batches, size of batch, no. of classes) to (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "  print(f\"Train Loss: {epoch_loss:.2f}\")\n",
    "\n",
    "  return epoch_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, dataloader, criterion):\n",
    "  \n",
    "  model.eval()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  total_preds = []\n",
    "\n",
    "  for i, inputs in enumerate((dataloader)):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [t.to(device) for t in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "      preds = model(sent_id, mask)\n",
    "      loss = criterion(preds,labels)\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # epoch loss and accuracy\n",
    "  epoch_loss = total_loss / len(dataloader) \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "  print(f\"Validation Loss: {epoch_loss:.2f}\")\n",
    "\n",
    "  return epoch_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "Train Loss: 0.68\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 2 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 3 / 20\n",
      "Train Loss: 0.70\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 4 / 20\n",
      "Train Loss: 0.70\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 5 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 6 / 20\n",
      "Train Loss: 0.68\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 7 / 20\n",
      "Train Loss: 0.68\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 8 / 20\n",
      "Train Loss: 0.70\n",
      "Validation Loss: 0.65\n",
      "\n",
      " Epoch 9 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 10 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 11 / 20\n",
      "Train Loss: 0.70\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 12 / 20\n",
      "Train Loss: 0.66\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 13 / 20\n",
      "Train Loss: 0.67\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 14 / 20\n",
      "Train Loss: 0.71\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 15 / 20\n",
      "Train Loss: 0.68\n",
      "Validation Loss: 0.68\n",
      "\n",
      " Epoch 16 / 20\n",
      "Train Loss: 0.71\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 17 / 20\n",
      "Train Loss: 0.68\n",
      "Validation Loss: 0.66\n",
      "\n",
      " Epoch 18 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 19 / 20\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.67\n",
      "\n",
      " Epoch 20 / 20\n",
      "Train Loss: 0.70\n",
      "Validation Loss: 0.66\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss, _ = train(model, train_dataloader, criterion, optimizer)\n",
    "    valid_loss, _ = evaluate(model, val_dataloader, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.20      0.25         5\n",
      "           1       0.50      0.67      0.57         6\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.42      0.43      0.41        11\n",
      "weighted avg       0.42      0.45      0.43        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/?\n",
    "\n",
    "https://trishalaneeraj.github.io/2020-04-04/feature-based-approach-with-bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermelon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74d1cbe4299d6052562a3b04e5d47e1706bc319f6acfc872fba2a82cb1a428da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
