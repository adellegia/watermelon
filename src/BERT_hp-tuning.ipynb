{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import yaml\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(r\"../data/df_final_document.json\") # document\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label.132'] = le.fit_transform(df['label_132'])\n",
    "df['label.134'] = le.fit_transform(df['label_134'])\n",
    "df = df[df['submit']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, test, validation sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text_clean'], df['label.132'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label.132'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text))\n",
    "print(len(val_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      self.relu =  nn.ReLU()\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to freeze all the parameters if freeze = T\n",
    "def set_parameter_requires_grad(model, freeze):\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters\n",
    "set_parameter_requires_grad(model=bert, freeze=True)\n",
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# specify GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(config, n_epochs):\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, n_epochs))\n",
    "        # print('\\n Epoch {:} / {:}'.format(epoch + 1, config[\"n_epochs\"]))\n",
    "        train_loss, _ = train(model, criterion, config)\n",
    "        # train_loss, _ = train(model, train_dataloader, criterion, optimizer, config)\n",
    "        valid_loss, _ = evaluate(model, criterion, config)\n",
    "        # valid_loss, _ = evaluate(model, val_dataloader, criterion, config)\n",
    "        \n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights_hp-tuned.pt')\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights_hp-tuned.pt'\n",
    "model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunign with `Ray Tune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file for ray tune hyperparameter tuning\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([16, 32]),\n",
    "    \"lr\": tune.choice([2e-5, 3e-5, 5e-5])\n",
    "    # \"n_epochs\": tune.choice([[2, 3, 4]])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(config, checkpoint_dir=None, data_dir=None):\n",
    "# def train_bert(config, checkpoint_dir=None, data_dir=None, train_data_arg=None, val_data_arg=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    criterion = nn.NLLLoss() \n",
    "    optimizer = AdamW(model.parameters(), lr = config[\"lr\"])\n",
    "\n",
    "    # if checkpoint_dir:\n",
    "    #     model_state, optimizer_state = torch.load(\n",
    "    #         os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "    #     net.load_state_dict(model_state)\n",
    "    #     optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # trainset, testset = load_data(data_dir)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        # sampler=train_sampler, \n",
    "        batch_size=int(config[\"batch_size\"])\n",
    "    )\n",
    "\n",
    "    # train_dataloader = DataLoader(train_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data, \n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        # sampler = val_sampler, \n",
    "        batch_size=int(config[\"batch_size\"])\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            # push to gpu\n",
    "            inputs = [r.to(device) for r in inputs]\n",
    "            sent_id, mask, labels = inputs\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
    "            optimizer.step()\n",
    "            preds=preds.detach().cpu().numpy()\n",
    "\n",
    "            # append the model predictions\n",
    "            total_preds.append(preds)\n",
    "\n",
    "            # print statistics\n",
    "            # running_loss += loss.item()\n",
    "            # epoch_steps += 1\n",
    "            # if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            #     print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "            #                                     running_loss / epoch_steps))\n",
    "            #     running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "\n",
    "            # push to gpu\n",
    "            inputs = [t.to(device) for t in inputs]\n",
    "            sent_id, mask, labels = inputs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(sent_id, mask)\n",
    "                loss = criterion(preds,labels)\n",
    "                total_loss = total_loss + loss.item()\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                total_preds.append(preds)\n",
    "\n",
    "            val_loss = total_loss / len(val_dataloader)\n",
    "            total_preds  = np.concatenate(total_preds, axis=0)\n",
    "        # print(f\"Validation Loss: {epoch_loss:.2f}\")\n",
    "\n",
    "        \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights_hp-tuned.pt')\n",
    "        \n",
    "        # with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        #     path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        #     torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=val_loss)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-12 18:21:46</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.84        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.6/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/5.16 GiB heap, 0.0/2.0 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_bert_6f981_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/lukaswarode/ray_results/train_bert_2022-12-12_18-21-38/train_bert_6f981_00000_0_batch_size=32,lr=0.0000_2022-12-12_18-21-41/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_bert_6f981_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">3e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:21:44,028\tERROR ray_trial_executor.py:580 -- Trial train_bert_6f981_00000: Unexpected error starting runner.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 573, in start_trial\n",
      "    return self._start_trial(trial)\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 473, in _start_trial\n",
      "    runner = self._setup_remote_runner(trial)\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 414, in _setup_remote_runner\n",
      "    return full_actor_class.remote(**kwargs)\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/actor.py\", line 637, in remote\n",
      "    return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 387, in _invocation_actor_class_remote_span\n",
      "    return method(self, args, kwargs, *_args, **_kwargs)\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/actor.py\", line 844, in _remote\n",
      "    worker.function_actor_manager.export_actor_class(\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 479, in export_actor_class\n",
      "    check_oversized_function(\n",
      "  File \"/Users/lukaswarode/miniforge3/lib/python3.9/site-packages/ray/_private/utils.py\", line 742, in check_oversized_function\n",
      "    raise ValueError(error)\n",
      "ValueError: The actor ImplicitFunc is too large (419 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "2022-12-12 18:21:46,043\tWARNING util.py:244 -- The `start_trial` operation took 4.731 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_bert_6f981_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# from functools import partial\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: tune\u001b[39m.\u001b[39mchoice([\u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: tune\u001b[39m.\u001b[39mchoice([\u001b[39m2e-5\u001b[39m, \u001b[39m3e-5\u001b[39m, \u001b[39m5e-5\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# \"n_epochs\": tune.choice([[2, 3, 4]])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_bert,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# tune.with_parameters(train_bert(config=config, train_data_arg=train_data, val_data_arg=val_data)),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# partial(train_bert), \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# train_bert,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# train_bert(config=config, train_data_arg=train_data, val_data_arg=val_data),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# config\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     config \u001b[39m=\u001b[39;49m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: tune\u001b[39m.\u001b[39;49mchoice([\u001b[39m16\u001b[39;49m, \u001b[39m32\u001b[39;49m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: tune\u001b[39m.\u001b[39;49mchoice([\u001b[39m2e-5\u001b[39;49m, \u001b[39m3e-5\u001b[39;49m, \u001b[39m5e-5\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# \"n_epochs\": tune.choice([[2, 3, 4]])\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# print(\"Best config: \", analysis.get_best_config(metric=\"mean_precision\"))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Get a dataframe for analyzing trial results.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukaswarode/Desktop/Deep_Learning/watermelon/src/BERT_hp-tuning.ipynb#X65sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m df_analysis \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mdataframe()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/ray/tune/tune.py:771\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39msignal\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 771\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    772\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_bert_6f981_00000])"
     ]
    }
   ],
   "source": [
    "# from functools import partial\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([16, 32]),\n",
    "    \"lr\": tune.choice([2e-5, 3e-5, 5e-5])\n",
    "    # \"n_epochs\": tune.choice([[2, 3, 4]])\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_bert,\n",
    "    # tune.with_parameters(train_bert(config=config, train_data_arg=train_data, val_data_arg=val_data)),\n",
    "    # partial(train_bert), \n",
    "    # train_bert,\n",
    "    # train_bert(config=config, train_data_arg=train_data, val_data_arg=val_data),\n",
    "    verbose=2,\n",
    "    # config\n",
    "    config = {\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"lr\": tune.choice([2e-5, 3e-5, 5e-5])\n",
    "        # \"n_epochs\": tune.choice([[2, 3, 4]])\n",
    "    },\n",
    "    metric=\"val_loss\"\n",
    ")\n",
    "\n",
    "# print(\"Best config: \", analysis.get_best_config(metric=\"mean_precision\"))\n",
    "\n",
    "# Get a dataframe for analyzing trial results.\n",
    "df_analysis = analysis.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preds():\n",
    "    # load weights of best model\n",
    "    path = 'saved_weights_hp-tuned.pt'\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    # prediction for test set\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_data)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "613f71e825c463d7d47275488421bf9af2c365b30de846ada661b727d7d63a61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
