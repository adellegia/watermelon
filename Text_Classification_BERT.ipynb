{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "# Classification of Unstructured Documents \n",
        "## *Transfer Learning with BERT*\n",
        "### GRAD-E1394 Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "Authors:\n",
        "*   Ma. Adelle Gia Arbo, m.arbo@students.hertie-school.org\n",
        "*   Janine De Vera, j.devera@students.hertie-school.org\n",
        "*   Lorenzo Gini, l.gini@students.hertie-school.org\n",
        "*   Lukas Warode, l.warode@students.hertie-school.org | lukas.warode@gmx.de\n",
        "\n",
        "---\n",
        "\n",
        "This tutorial demonstrates the pipeline for classifying unstructured documents, particularly those in PDF format. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNv0ANr5WcD_"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Memo](#memo)\n",
        "*   [Overview](#overview)\n",
        "*   [Background & Prerequisites](#background-and-prereqs)\n",
        "*   [Software Requirements](#software-requirements)\n",
        "*   [Data Description](#data-description)\n",
        "*   [Methodology](#methodology)\n",
        "*   [Results & Discussion](#results-and-discussion)\n",
        "*   [References](#references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH81wjfsJsv1"
      },
      "source": [
        "<a name=\"memo\"></a>\n",
        "# Memo\n",
        "\n",
        "Write a memo for the leadership explaining in layman's terms why this topic is relevant for public policy. Discuss relevant research works, real-world examples of successful applications, and organizations and governments that apply such approaches for policy making. \n",
        "\n",
        "\n",
        "### *Sketch Lukas (not done)*\n",
        "\n",
        "#### Main Point / Take Away (Executive Summary???)\n",
        "\n",
        "#### Background \n",
        "\n",
        "Companies as well as public and political institutions are able and ofent required to access loads of information nowadays. In doing that, they often face the problem of unstructured information, especially when dealing with text data. Data in textual form is the most common type of unstructured, unfortunately, it is also representing the most fundamental type of documents with respect to policymakers and public institutions: Legal documents, bills and policy papers are just some examples of common text sources, which are part of the daily business in the political world. Unstructured text data entails a variety of different problems when it comes actually gaining quantitative analytical insights from them. Computers commonly have difficulties understanding textual data. Analytical and technical competences are also scarce: Only 18% of companies are able to use unstructured data, while most organizations are make their (data-driven) decisions on the basis of only 10 to 20% of their available data source. The situation for public institutions is worsend by the fact that most of the modern text data analysis frameworks and models are dominated by different industry players, while being designed and trained according to the industry-specific needs, which do not transfer to the needs of the mentioned political text sources. The EU Commission (or directly mention DMA?) …\n",
        "\n",
        "This motivates the necessity of implementing …\n",
        "\n",
        "#### Evidence (Sktech our technical solution?)\n",
        "- Text Classification Pipepline + BERT Model(ling)\n",
        "\n",
        "- Results??\n",
        "\n",
        "\n",
        "#### Conclusion and Implementation\n",
        "\n",
        "- Results??\n",
        "\n",
        "- Being specific about implementation and actual usage (maybe even specify DMA context a bit more and say who and what stakeholders/institutions are taking what position in context)\n",
        "\n",
        "*Open points*\n",
        "\n",
        "- Include practicality and gained efficiency\n",
        "- Normative perspective: \"Technical\" objectivity (but one needs to be careful for potential biases)\n",
        "\n",
        "\\\n",
        "- Directly mention DMA at beginning?\n",
        "- How structured should the memo be?\n",
        "\n",
        "\\\n",
        "- Sources need to be added for numbers of slides (e.g. 18% of companies blabla)\n",
        "\n",
        "\n",
        "*Structure*\n",
        "![](https://mitcommlab.mit.edu/broad/wp-content/uploads/sites/5/2017/12/policy-memo-struct-700x843.png)\n",
        "\n",
        "![](https://www.bu.edu/sph/files/2014/06/Policy-Memo-Organization.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u40fYe3EOL"
      },
      "source": [
        "<a name=\"overview\"></a>\n",
        "# Overview\n",
        "\n",
        "Over 80% of all data is *unstructured*. Most of the information we consume comes in a format that is not organized in a pre-defined manner (e.g. tables) or with a specific data model in mind (e.g. matrices). \n",
        "\n",
        "<u>Text</u> is the most common type of unstructured data and it comes in a variety of forms, like blogs, news articles, social media content, as well as official documents. This lack of structure that can be readily understood by machines makes it difficult to maximize text as a data source. Algorithms that efficiently and accurately process text would have a variety of applications in organisations, especially in public institutions. \n",
        "\n",
        "In this tutorial we demonstrate one such application in the context of the European Commission (EC). Whenever new legislation is proposed, the EC opens **public consultations** where various stakeholders (e.g. businesses, academia, law firms, associations, private individuals) submit documents that detail their views on the proposal. The EC receives anywhere between 10,000 to 4 million of these public consultation documents annually. Using machine learning and deep learning methods to process these documents would streamline the Commission's review of stakeholder comments, which consequently allows them to integrate more information into their policymaking process. \n",
        "\n",
        "The main goal of this tutorial is to walk you through the steps of building a <u>document classifier</u> using a deep learning model called **Bidirectional Encoder Representations from Transformers (BERT)**. By the end of this tutorial you will understand how to:  \n",
        "\n",
        "> 1. Extract, clean, and pre-process information from PDF documents\n",
        "> 2. Use the pre-processed text as input to machine learning/deep learning models\n",
        "> 3. Build a text/document classifier with BERT \n",
        "> 4. Compare BERT with text classifiers built using other models\n",
        "\n",
        "We will then apply these learnings to accomplish a research objective: \n",
        "\n",
        " > Classify public consultation documents of the recently enacted **Digital Markets Act** according to whether a stakeholder **agrees or disagrees** with the DMA proposal. \n",
        "\n",
        "The Digital Markets Act is a regulation in the European Union which came into force this 2022. It aims to promote fair competition within the digital market by defining rules for “gatekeepers” or large online platforms. Majority of the public consultation documents submitted for the DMA are from companies and business associations who will likely be affected by the law."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgijl46pYzn"
      },
      "source": [
        "<a name=\"background-and-prereqs\"></a>\n",
        "# Background & Prerequisites\n",
        "\n",
        "For this tutorial, you would need to be familiar with object oriented programming, common python libraries such as *numpy* and *pandas*, and libraries used for model building, such as *scikit-learn* and *pytorch*. Working knowledge of common language processing concepts (e.g. stemming, lemmatization, TF-IDF) would also be useful. \n",
        "\n",
        "Important concepts that need to be explained briefly:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BpQklEEIFDD"
      },
      "source": [
        "## Reading materials\n",
        "\n",
        "For detailed explanations of the topics covered in this tutorial, below is a list helpful references.\n",
        "\n",
        "**Basics of natural language processing models:**\n",
        "* Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
        "\n",
        "\n",
        "**BERT:**\n",
        "* Horev, R. (2018). BERT Explained: State of the art language model for NLP. Towards Data Science, 10.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSRCNgYzUwaf"
      },
      "source": [
        "<a name=\"software-requirements\"></a>\n",
        "# Software Requirements\n",
        "To install software requirments and dependencies, please create a new environment using the *environment.yml* file which accompanies this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda env create -f environment.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xVzk4V7qUu2R"
      },
      "outputs": [],
      "source": [
        "# Data visualization\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic and XGboost\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score\n",
        "from xgboost import XGBClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import yaml\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# specify GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXoiLncsU3pe"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "As mentioned in the [Overview](#overview), the methods discussed in this tutorial will be applied to public consultation documents of the **Digital Markets Act (DMA)**. The open public consultation for DMA received **188 responses**. In addition to document submissions, each respondent answered an accompanying survey. They were asked a series of questions about what they think about the law. \n",
        "\n",
        "To build our document classifier we need the raw **text** from the public consultation submissions, and a corresponding **label** for each document which indicates whether or not the author/s agree or disagree with the DMA proposal. \n",
        "\n",
        "**Text:**\n",
        "The submissions come in the form of **pdf files**. These documents need to be parsed in order to extract raw text. \n",
        "\n",
        "**Labels:**\n",
        "The labels are extracted from the **survey**, specifically from the question: \n",
        "\n",
        "*\"Do you consider that there is a need for the Commission to be able to intervene in gatekeeper scenarios to prevent/address structural competition problems?*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoF-BxSM5Jkc"
      },
      "source": [
        "## Data Download\n",
        "\n",
        "The pdf documents and the survey used to generate labels can be downloaded from the <a href=\"https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12416-New-competition-tool/public-consultation_en\">DMA consultation page</a>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSt6h_Q-oqjK"
      },
      "source": [
        "## Data Preprocessing\n",
        "Additionally, you can include any data preprocessing steps and exploratory data analyses (e.g. visualize data distributions, impute missing values, etc.) in this section to allow the users to better understand the dataset. \n",
        "\n",
        "In this section, you might also want to describe the different input and output variables, the train/val/test splits, and any data transformations.\n",
        "\n",
        "(Describe processing of labels here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aBJ5aLZosk-"
      },
      "outputs": [],
      "source": [
        "# Insert data pre-processing and exploratory data analysis\n",
        "# code here. Feel free to break this up into several code\n",
        "# cells, interleaved with explanatory text. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qa9Iuu2GU52Z"
      },
      "source": [
        "<a name=\"methodology\"></a>\n",
        "# Methodology\n",
        "\n",
        "This section of the tutorial is a step-by-step walkthrough of text classification pipeline, using DMA public consultation documents as described above. Below is an outline of this section: \n",
        "\n",
        "<ol type=\"A\">\n",
        "  <li>Data Preparation </li>\n",
        "  <li>Text Representation</li>\n",
        "  <li>Model Training (with hyperparameter tuning)</li>\n",
        "  <ol>\n",
        "    <li> Training baseline models (logistic, XGBoost)\n",
        "    <li> Training a DL classifiers (LSTM)\n",
        "    <li> Training a BERT model and other variants\n",
        "  </ol>\n",
        "  <li> Model Evaluation\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXsiZ5freTG"
      },
      "source": [
        "## A. Data Preparation \n",
        "Parsing PDF files, cleaning text, pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8L28axZ4NVj"
      },
      "source": [
        "## B. Text Representation\n",
        "Since this step is built inside BERT and other DL models, we can use what Lorenzo has done for the baseline models (TF-IDF & embeddings) to illustrate how text representation works. \n",
        "\n",
        "In the BERT section explain how the architecture processes text to create embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Model Training (with hyperparameter tuning)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_json(r\"./data/df_final_document.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df['label.132'] = le.fit_transform(df['label_132'])\n",
        "df['label.134'] = le.fit_transform(df['label_134'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.1 Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "dfm = vectorizer.fit_transform(df['text_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Accuracy: 62.79 \n",
            " Precision: 0.625 \n",
            " Recall: 0.962 \n",
            " F1: 0.758\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(train_text, train_labels)\n",
        "\n",
        "y_pred = clf.predict(test_text)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, y_pred) *100.0\n",
        "precision = precision_score(test_labels, y_pred, average='binary')\n",
        "recall = recall_score(test_labels, y_pred, average='binary')\n",
        "f_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
            "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
            "              early_stopping_rounds=None, enable_categorical=False,\n",
            "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
            "              grow_policy='depthwise', importance_type=None,\n",
            "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
            "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
            "              max_depth=1000, max_leaves=0, min_child_weight=1, missing=nan,\n",
            "              monotone_constraints='()', n_estimators=1000, n_jobs=0,\n",
            "              num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
            " Accuracy: 67.44 \n",
            " Precision: 0.688 \n",
            " Recall: 0.846 \n",
            " F1: 0.759\n"
          ]
        }
      ],
      "source": [
        "bst = XGBClassifier(n_estimators=1000, max_depth=1000, learning_rate=0.1, objective='binary:logistic')\n",
        "\n",
        "bst.fit(train_text, train_labels)\n",
        "\n",
        "print(bst)\n",
        "\n",
        "y_pred = bst.predict(test_text)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, y_pred) * 100.0\n",
        "precision = precision_score(test_labels, y_pred, average='binary')\n",
        "recall = recall_score(test_labels, y_pred, average='binary')\n",
        "f_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.2 Long Short-Term Memory Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.3 BERT Model and other variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into train, test, validation sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text_clean'], df['label.132'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label.132'])\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Import BERT Model and BERT Tokenizer\n",
        "\n",
        "You can specify which BERT model to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# bert-base-uncased\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# legal-bert-base-uncased\n",
        "bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tokenize the Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_workers = 2\n",
        "\n",
        "# dataLoader for train set\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_dataloader = DataLoader(train_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_dataloader = DataLoader(val_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BERT Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# method to freeze all the parameters if freeze = T\n",
        "def set_parameter_requires_grad(model, freeze):\n",
        "    if freeze:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze all parameters\n",
        "set_parameter_requires_grad(model=bert, freeze=True)\n",
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-5) # learning rate\n",
        "\n",
        "# loss function\n",
        "criterion  = nn.NLLLoss() \n",
        "\n",
        "# no. of training epochs\n",
        "epochs = 20"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Finetuning BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "  \n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  running_total_correct = 0.0\n",
        "  total_preds=[]\n",
        "  \n",
        "  for i, inputs in enumerate((dataloader)):\n",
        "    \n",
        "    # push to gpu\n",
        "    inputs = [r.to(device) for r in inputs]\n",
        "    sent_id, mask, labels = inputs\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    model.zero_grad()        \n",
        "\n",
        "    # forward + backward + optimize \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = criterion(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # epoch loss and accuracy\n",
        "  epoch_loss = total_loss / len(dataloader)\n",
        "  # reshape from (no. of batches, size of batch, no. of classes) to (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  print(f\"Train Loss: {epoch_loss:.2f}\")\n",
        "\n",
        "  return epoch_loss, total_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate(model, dataloader, criterion):\n",
        "  \n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds = []\n",
        "\n",
        "  for i, inputs in enumerate((dataloader)):\n",
        "    \n",
        "    # push to gpu\n",
        "    inputs = [t.to(device) for t in inputs]\n",
        "    sent_id, mask, labels = inputs\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = criterion(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # epoch loss and accuracy\n",
        "  epoch_loss = total_loss / len(dataloader) \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  print(f\"Validation Loss: {epoch_loss:.2f}\")\n",
        "\n",
        "  return epoch_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.70\n",
            "\n",
            " Epoch 2 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 3 / 20\n",
            "Train Loss: 0.68\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 4 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 5 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.67\n",
            "\n",
            " Epoch 6 / 20\n",
            "Train Loss: 0.69\n",
            "Validation Loss: 0.66\n",
            "\n",
            " Epoch 7 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 8 / 20\n",
            "Train Loss: 0.66\n",
            "Validation Loss: 0.64\n",
            "\n",
            " Epoch 9 / 20\n",
            "Train Loss: 0.66\n",
            "Validation Loss: 0.67\n",
            "\n",
            " Epoch 10 / 20\n",
            "Train Loss: 0.66\n",
            "Validation Loss: 0.66\n",
            "\n",
            " Epoch 11 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.65\n",
            "\n",
            " Epoch 12 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.65\n",
            "\n",
            " Epoch 13 / 20\n",
            "Train Loss: 0.68\n",
            "Validation Loss: 0.70\n",
            "\n",
            " Epoch 14 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.72\n",
            "\n",
            " Epoch 15 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 16 / 20\n",
            "Train Loss: 0.66\n",
            "Validation Loss: 0.67\n",
            "\n",
            " Epoch 17 / 20\n",
            "Train Loss: 0.68\n",
            "Validation Loss: 0.67\n",
            "\n",
            " Epoch 18 / 20\n",
            "Train Loss: 0.66\n",
            "Validation Loss: 0.71\n",
            "\n",
            " Epoch 19 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.71\n",
            "\n",
            " Epoch 20 / 20\n",
            "Train Loss: 0.67\n",
            "Validation Loss: 0.68\n"
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    train_loss, _ = train(model, train_dataloader, criterion, optimizer)\n",
        "    valid_loss, _ = evaluate(model, val_dataloader, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.59      1.00      0.74        13\n",
            "\n",
            "    accuracy                           0.59        22\n",
            "   macro avg       0.30      0.50      0.37        22\n",
            "weighted avg       0.35      0.59      0.44        22\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\watermelon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Model Evaluation\n",
        "Including comparison of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkmytKNU_Z2"
      },
      "source": [
        "<a name=\"results-and-discussion\"></a>\n",
        "# Results & Discussion\n",
        "\n",
        "In this section, describe and contextualize the results shown in the tutorial. Briefly describe the performance metrics and cross validation techniques used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LWo6UQzwj37"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIjM6eMwlXY"
      },
      "source": [
        "Finally, include a discussion on the limitations and important takeaways from the exercise.\n",
        "\n",
        "## Limitations\n",
        "*   The tutorial is focused on education and learning. Explain all the simplifications you have made compared to applying a similar approach in the real world (for instance, if you have reduced your training data and performance).\n",
        "*   ML algorithms and datasets can reinforce or reflect unfair biases. Reflect on the potential biases in the dataset and/or analysis presented in your tutorial, including its potential societal impact, and discuss how readers might go about addressing this challenge. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLCQpv14Gsx"
      },
      "source": [
        "## Next Steps\n",
        "*   What do you recommend would be the next steps for your readers after finishing your tutorial?\n",
        "*   Discuss other potential policy- and government-related applications for the method or tool discussed in the tutorial.\n",
        "*   List anything else that you would want the reader to take away as they move on from the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKqewK_-ZLD"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "# References\n",
        "\n",
        "Include all references used. \n",
        "\n",
        "For example, in this template:\n",
        "\n",
        "*   EarthCube Notebook Template: https://github.com/earthcube/NotebookTemplates\n",
        "*   Earth Engine Community Tutorials Style Guide: https://developers.google.com/earth-engine/tutorials/community/styleguide#colab\n",
        "*   Google Cloud Community Tutorial Style Guide: https://cloud.google.com/community/tutorials/styleguide\n",
        "*   Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et al. (2019) Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks. PLoS Comput Biol 15(7): e1007007. https://doi.org/10.1371/journal.pcbi.1007007\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YguJayU8RBmd"
      },
      "source": [
        "## Acknowledgement\n",
        "\n",
        "These guidelines are heavily based on the Climate Change AI template for the for the tutorials track at the [NeurIPS 2021 Workshop on Tackling Climate Change with Machine Learning](https://www.climatechange.ai/events/neurips2021). "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "watermelon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "74d1cbe4299d6052562a3b04e5d47e1706bc319f6acfc872fba2a82cb1a428da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
