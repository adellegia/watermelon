{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "# Classification of Unstructured Documents \n",
        "## *Transfer Learning with BERT*\n",
        "### GRAD-E1394 Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "Authors:\n",
        "*   Ma. Adelle Gia Arbo, m.arbo@students.hertie-school.org\n",
        "*   Janine De Vera, j.devera@students.hertie-school.org\n",
        "*   Lorenzo Gini, l.gini@students.hertie-school.org\n",
        "*   Lukas Warode, l.warode@students.hertie-school.org | lukas.warode@gmx.de\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNv0ANr5WcD_"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Memo](#memo)\n",
        "*   [Overview](#overview)\n",
        "*   [Background & Prerequisites](#background-and-prereqs)\n",
        "*   [Software Requirements](#software-requirements)\n",
        "*   [Data Description](#data-description)\n",
        "*   [Methodology](#methodology)\n",
        "*   [Results & Discussion](#results-and-discussion)\n",
        "*   [References](#references)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QH81wjfsJsv1"
      },
      "source": [
        "<a name=\"memo\"></a>\n",
        "# Memo\n",
        "\n",
        "<a name=\"memo\"></a>\n",
        "\n",
        "## Classifying Unstructured DMA Respondents' Reports  <img src=\"https://www.iccitalia.org/wp-content/uploads/2022/07/digital-markets-act.png\" width=\"80\" align=\"right\"/> <img src=\"https://ec.europa.eu/info/law/better-regulation/assets/images/ecl/logo/logo--en.svg\" width=\"200\" align=\"right\"/> \n",
        "\n",
        "\n",
        "### *Executive Summary*\n",
        "\n",
        "This tutorial shows how state-of-the-art **Deep Learning (DL)** frameworks can be used to **classify unstructured papers** and **reports** that were submitted by stakeholders that participated in the **Digital Markets Act (DMA**) public consultation survey[<sup>1</sup>](#fn1). The tutorial builds on a **strong demand** for an appropriate technical solution, while existing similar contributions show that **DL-based solutions** are **feasible**.\n",
        "\n",
        "### *Background & Relevance*\n",
        "\n",
        "Companies as well as public and political institutions are often required to access loads of information that may be **structured** or **unstructed**. Data in textual form is the **most common type of unstructured data**. Text also comprises the most fundamental type of documents for policymakers and public institutions: legal documents, bills, policy papers and reports are just some examples of common text sources, which are part of daily operations in the political world. Unstructured text data entails a variety of different problems concerning extraction of quantitative analytical insights. Computers commonly have difficulties understanding textual data. Analytical and technical competencies are also scarce: Only **18% of companies** are **able to use unstructured data**, while most organizations are make their (data-driven) **decisions on the basis of only 10 to 20%** of their available data source[<sup>1</sup>](#fn2). The situation for public institutions is worsened by the fact that most modern text data analysis frameworks and models are **industry-specific**. They are designed and trained according to the needs of certain industries, which often cannnot be generalized to the nuances of political text sources. \n",
        "\n",
        "### *Solution*\n",
        "\n",
        "There are several potential public policy applications of a DL-based text classifiers. For instance, the European Commission regularly conducts **public consultations**, such as the survey around the **DMA**, where stakeholders **submitted unstructured papers** and **reports**. **Whether stakeholders agree** or not can be predicted with **deep learning models**[<sup>3</sup>](#fn3) that are trained on the document corpus. \n",
        "\n",
        "Training a state-of-the-art **DL model** allows us to **predict stakeholder agreement** on the DMA proposal. This algorithm can support human-based analyses by producing **objective and efficient** estimates. This text classification framework can be directly implemented to obtain immediate results for existing and new documents, while also **being transferrable** to different (EC) contexts.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "---\n",
        "\n",
        "<sup id=\"fn1\">1</sup> https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12416-New-competition-tool/public-consultation_en\n",
        "\n",
        "<sup id=\"fn2\">2</sup> https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data\n",
        "\n",
        "<sup id=\"fn3\">3</sup> Minaee, S., Kalchbrenner, N., Cambria, E., Nikzad, N., Chenaghlu, M., & Gao, J. (2021). Deep learning--based text classification: a comprehensive review. ACM Computing Surveys (CSUR), 54(3), 1-40.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u40fYe3EOL"
      },
      "source": [
        "<a name=\"overview\"></a>\n",
        "# Overview\n",
        "\n",
        "Over 80% of all data is *unstructured*. Most of the information we consume come in a format that is not organized in a pre-defined manner (e.g. tables) or with a specific data model in mind (e.g. matrices). \n",
        "\n",
        "<u>Text</u> is the most common type of unstructured data and it comes in a variety of forms like blogs, news articles, social media content, as well as official documents. This lack of structure that can be readily understood by machines is what makes it difficult to maximize text as a data source. Algorithms that efficiently and accurately process text would have a variety of applications in organisations, especially public institutions that have access to different types of documents. \n",
        "\n",
        "In this tutorial we demonstrate one such application in the context of the European Commission (EC). Whenever new legislation is proposed, the EC opens **public consultations** where various stakeholders (e.g. businesses, academia, law firms, associations, private individuals) submit documents that detail their views on the proposal. The EC receives anywhere between 10,000 to 4 million of these public consultation documents annually. Using machine learning and deep learning methods to process these documents would streamline the Commission's review of stakeholder comments, which will consequently allow them to integrate more information into their policymaking process. \n",
        "\n",
        "The main goal of this tutorial is to walk you through the steps of building a <u>document classifier</u> using a deep learning model called **Bidirectional Encoder Representations from Transformers (BERT)**. By the end of this tutorial you will understand how to:  \n",
        "\n",
        "> 1. Extract, clean, and pre-process information from PDF documents\n",
        "> 2. Use the pre-processed text as input to machine learning/deep learning models\n",
        "> 3. Build a text/document classifier with BERT \n",
        "> 4. Compare BERT with text classifiers built using other models\n",
        "\n",
        "We will then apply these learnings to accomplish a research objective: \n",
        "\n",
        " > Classify public consultation documents of the recently enacted **Digital Markets Act** according to whether a stakeholder **agrees or disagrees** with the DMA proposal. \n",
        "\n",
        "The Digital Markets Act is a regulation in the European Union which came into force this 2022. It aims to promote fair competition within the digital market by defining rules for “gatekeepers” or large online platforms. Majority of the public consultation documents submitted for the DMA are from companies and business associations who will likely be affected by the law.\n",
        "\n",
        "<br>\n",
        "\n",
        "*Note: This tutorial was created primarily to demonstrate the document classification pipeline. For ease of replication, we purposely kept the dataset small. Results should therefore be taken with a grain of salt.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgijl46pYzn"
      },
      "source": [
        "<a name=\"background-and-prereqs\"></a>\n",
        "# Background & Prerequisites\n",
        "\n",
        "For this tutorial, you would need to be familiar with object oriented programming, common python libraries such as *numpy* and *pandas*, and libraries used for model building, such as *scikit-learn* and *pytorch*. Working knowledge of common language processing concepts (e.g. stemming, lemmatization, TF-IDF, embeddings) and the basics of transformer models are also required."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7BpQklEEIFDD"
      },
      "source": [
        "## Reading materials\n",
        "\n",
        "For detailed explanations of the topics covered in this tutorial, you may refer to the following reading materials:\n",
        "\n",
        "* Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
        "* Gereon, A. (2018). Hands-on Machine Learning with Scikit-Learn and Tensor Flow. O’Reily Media Inc., USA.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rSRCNgYzUwaf"
      },
      "source": [
        "<a name=\"software-requirements\"></a>\n",
        "# Software Requirements\n",
        "This tutorial requires Python 3.6 or higher version. To install software requirements and dependencies, please create a new environment using the *environment.yml* file which accompanies this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda env create -f environment.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xVzk4V7qUu2R"
      },
      "outputs": [],
      "source": [
        "# Data visualization\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parsing and pre-processing\n",
        "from glob import glob\n",
        "import os \n",
        "import re\n",
        "\n",
        "from pdfminer.high_level import extract_text\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector representations and embeddings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic and XGboost\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score\n",
        "from xgboost import XGBClassifier\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# LSTM \n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT models\n",
        "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# specify GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXoiLncsU3pe"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "As mentioned in the [Overview](#overview), the methods discussed in this tutorial will be applied to public consultation documents of the **Digital Markets Act (DMA)**. Respondents answered a survey where they were asked a series of questions about the proposed law. The public consultation received **188 survey responses**. Some respondents also provided **accompanying position papers and reports** where they discuss their views in detail. \n",
        "\n",
        "To build our document classifier we need the raw **text** from the public consultation submissions, and a corresponding **label** for each document which indicates whether or not the author/s agree or disagree with the DMA proposal. \n",
        "\n",
        "**Text:**\n",
        "The submissions come in the form of **pdf files**. These documents need to be parsed in order to extract raw text. \n",
        "\n",
        "**Labels:**\n",
        "The labels are extracted from the **survey**, specifically from the question: \n",
        "\n",
        "> *\"Do you consider that there is a need for the Commission to be able to intervene in gatekeeper scenarios to prevent/address structural competition problems?\"*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoF-BxSM5Jkc"
      },
      "source": [
        "## Data Download\n",
        "\n",
        "The pdf documents and the survey used to generate labels can be downloaded from the <a href=\"https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12416-New-competition-tool/public-consultation_en\">DMA consultation page</a>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSt6h_Q-oqjK"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A. Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, create a list of file paths of all pdf submissions: `pdf_list`. \n",
        "\n",
        "Each of the downloaded public consultation documents have a unique alpha-numeric ID. For example: the document ID for **\"F549293-Statement_on_the_New_Competition_Tool\"** is *F549293*. A regular expression is used to extract this from the file paths. The IDs are then saved in a list called `pdf_id`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_dir = \"data/reports/\"\n",
        "pdf_list = glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
        "len(pdf_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_id = [re.search('[F][0-9]{6}', i)[0] for i in pdf_list]\n",
        "pdf_id = list(set(pdf_id))\n",
        "len(pdf_id) # no. of unique pdf submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B. Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Excel file containing survey answers and details about the respondents is imported as a pandas dataframe. The survey dataset also has a reference column which corresponds to the reference numbers in `pdf_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reference</th>\n",
              "      <th>Feedback date</th>\n",
              "      <th>Language</th>\n",
              "      <th>User type</th>\n",
              "      <th>First name</th>\n",
              "      <th>Surname</th>\n",
              "      <th>Scope</th>\n",
              "      <th>Organisation name</th>\n",
              "      <th>Transparency register number</th>\n",
              "      <th>Organisation size</th>\n",
              "      <th>...</th>\n",
              "      <th>Q197</th>\n",
              "      <th>Q198</th>\n",
              "      <th>Q199</th>\n",
              "      <th>Q200</th>\n",
              "      <th>Q201</th>\n",
              "      <th>Q202</th>\n",
              "      <th>Q203</th>\n",
              "      <th>Q204</th>\n",
              "      <th>Q205</th>\n",
              "      <th>Q206</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F550828</td>\n",
              "      <td>8.09.2020 23:57</td>\n",
              "      <td>English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>See our response above.</td>\n",
              "      <td>Somewhat effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>See our response above.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No.</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F550827</td>\n",
              "      <td>8.09.2020 23:54</td>\n",
              "      <td>English</td>\n",
              "      <td>NGO (Non-governmental organisation)</td>\n",
              "      <td>Juliane</td>\n",
              "      <td>von Reppert-Bismarck</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lie Detectors</td>\n",
              "      <td>094738529674-10</td>\n",
              "      <td>Small (&lt; 50 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Not effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Somewhat effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>An additional regulatory framework imposing ob...</td>\n",
              "      <td>Lie_Detectors_-_Digital_Services_Act_-_New_Com...</td>\n",
              "      <td>Please see attached submission</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F550826</td>\n",
              "      <td>8.09.2020 23:50</td>\n",
              "      <td>German</td>\n",
              "      <td>Business Association</td>\n",
              "      <td>Antje</td>\n",
              "      <td>Woltermann</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zentralverband Deutsches Kfz-Gewerbe e.V.</td>\n",
              "      <td>71649103246-10</td>\n",
              "      <td>Medium (&lt; 250 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Somewhat effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Somewhat effective</td>\n",
              "      <td>Sufficiently effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>-</td>\n",
              "      <td>Positionspapier_Gleichberechtigter_Zugang_zum_...</td>\n",
              "      <td>-</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F550825</td>\n",
              "      <td>8.09.2020 23:45</td>\n",
              "      <td>English</td>\n",
              "      <td>Academic/Research Institution</td>\n",
              "      <td>DIANA</td>\n",
              "      <td>MONTENEGRO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Master student candidate Master in Competition...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Large (250 or more)</td>\n",
              "      <td>...</td>\n",
              "      <td>The new procedural must be agile.</td>\n",
              "      <td>Not effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>all are adequated</td>\n",
              "      <td>PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_COMMISIO...</td>\n",
              "      <td>The document I prepared was more than 1MB.</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F550824</td>\n",
              "      <td>8.09.2020 23:33</td>\n",
              "      <td>English</td>\n",
              "      <td>Business Association</td>\n",
              "      <td>Kamila</td>\n",
              "      <td>Sotomska</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zwi?zek Przedsi?biorców i Pracodawców</td>\n",
              "      <td>868073924175-77</td>\n",
              "      <td>Small (&lt; 50 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 219 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Reference    Feedback date Language                            User type  \\\n",
              "0   F550828  8.09.2020 23:57  English                                  NaN   \n",
              "1   F550827  8.09.2020 23:54  English  NGO (Non-governmental organisation)   \n",
              "2   F550826  8.09.2020 23:50   German                 Business Association   \n",
              "3   F550825  8.09.2020 23:45  English        Academic/Research Institution   \n",
              "4   F550824  8.09.2020 23:33  English                 Business Association   \n",
              "\n",
              "  First name               Surname Scope  \\\n",
              "0        NaN                   NaN   NaN   \n",
              "1    Juliane  von Reppert-Bismarck   NaN   \n",
              "2      Antje            Woltermann   NaN   \n",
              "3      DIANA            MONTENEGRO   NaN   \n",
              "4     Kamila              Sotomska   NaN   \n",
              "\n",
              "                                   Organisation name  \\\n",
              "0                                                NaN   \n",
              "1                                      Lie Detectors   \n",
              "2          Zentralverband Deutsches Kfz-Gewerbe e.V.   \n",
              "3  Master student candidate Master in Competition...   \n",
              "4              Zwi?zek Przedsi?biorców i Pracodawców   \n",
              "\n",
              "  Transparency register number         Organisation size  ...  \\\n",
              "0                          NaN                       NaN  ...   \n",
              "1              094738529674-10    Small (< 50 employees)  ...   \n",
              "2               71649103246-10  Medium (< 250 employees)  ...   \n",
              "3                          NaN       Large (250 or more)  ...   \n",
              "4              868073924175-77    Small (< 50 employees)  ...   \n",
              "\n",
              "                                Q197  \\\n",
              "0            See our response above.   \n",
              "1                                NaN   \n",
              "2                                NaN   \n",
              "3  The new procedural must be agile.   \n",
              "4                                NaN   \n",
              "\n",
              "                                                Q198  \\\n",
              "0                                 Somewhat effective   \n",
              "1                                      Not effective   \n",
              "2                                 Somewhat effective   \n",
              "3                                      Not effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q199  \\\n",
              "0                                     Very effective   \n",
              "1                                     Most effective   \n",
              "2                                     Most effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q200  \\\n",
              "0                                     Very effective   \n",
              "1                                 Somewhat effective   \n",
              "2                                 Somewhat effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q201  \\\n",
              "0                                     Most effective   \n",
              "1                                     Most effective   \n",
              "2                             Sufficiently effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q202  \\\n",
              "0                                     Very effective   \n",
              "1                                     Most effective   \n",
              "2                                     Very effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q203  \\\n",
              "0                            See our response above.   \n",
              "1  An additional regulatory framework imposing ob...   \n",
              "2                                                  -   \n",
              "3                                  all are adequated   \n",
              "4                                                NaN   \n",
              "\n",
              "                                                Q204  \\\n",
              "0                                                NaN   \n",
              "1  Lie_Detectors_-_Digital_Services_Act_-_New_Com...   \n",
              "2  Positionspapier_Gleichberechtigter_Zugang_zum_...   \n",
              "3  PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_COMMISIO...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                         Q205 Q206  \n",
              "0                                         No.  Yes  \n",
              "1              Please see attached submission  Yes  \n",
              "2                                           -  Yes  \n",
              "3  The document I prepared was more than 1MB.  Yes  \n",
              "4                                         NaN  Yes  \n",
              "\n",
              "[5 rows x 219 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dma = pd.read_excel(\"data/DMA Contributions_Clean.xlsx\", sheet_name='Results')\n",
        "dma.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we saw above, there are only 89 unique PDF submissions. We now check how many of survey respondents submitted supplementary papers and/or reports. An indicator variable is added to the survey dataframe to specify whether a response has an equivalent PDF submission. Note that we can only use observations that have both the document submission and survey response. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# count unique Reference ids with pdf submission/s\n",
        "len(dma.query('Reference in @pdf_id'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    95\n",
              "1    85\n",
              "Name: submit, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tag identifier with pdf submission = 1\n",
        "dma['submit'] = [1 if v in pdf_id else 0 for v in dma['Reference'].values]\n",
        "dma['submit'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The question that will be used as a label is Q132. \n",
        "*\"Do you consider that there is a need for the Commission to be able to intervene in gatekeeper scenarios to prevent/address structural competition problems?\"*\n",
        "\n",
        "We check how many Yes and No responses there are. Respondents who answered not applicable can be considered as **not in agreement** with the proposal. Their answers are lumped together with the No responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Yes                                                    115\n",
              "Not applicable /no relevant experience or knowledge     46\n",
              "No                                                      19\n",
              "Name: Q132, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q132 as labels\n",
        "dma.Q132.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Yes    115\n",
              "No      65\n",
              "Name: label_132, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dma['label_132'] = np.where(dma['Q132'] == 'Not applicable /no relevant experience or knowledge', 'No', dma['Q132'])\n",
        "dma.label_132.value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qa9Iuu2GU52Z"
      },
      "source": [
        "<a name=\"methodology\"></a>\n",
        "# Methodology\n",
        "\n",
        "This section of the tutorial is a step-by-step walkthrough of text classification pipeline, using DMA public consultation documents as described above. Below is an outline of this section: \n",
        "\n",
        "<ol type=\"A\">\n",
        "  <li>Data Preparation </li>\n",
        "  <li>Text Representation</li>\n",
        "  <li>Model Training (with hyperparameter tuning)</li>\n",
        "  <ol>\n",
        "    <li> Training baseline models (logistic, XGBoost)\n",
        "    <li> Training a DL classifier (LSTM)\n",
        "    <li> Transfer learning with BERT and other variants\n",
        "  </ol>\n",
        "  <li> Model Evaluation\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXsiZ5freTG"
      },
      "source": [
        "## A. Data Preparation \n",
        "In this section of the tutorial, we will (1) parse PDF submissions from the Digital Markets Act public consultation and (2) pre-process raw text to keep only the most relevant information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A.1 Parsing PDFs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PDF (Portable Document Format) documents are hard to work with because this format was not designed as a data input. Instead, the PDF contains a set of instructions that describe how characters or objects are positioned on a page. \n",
        "\n",
        "Python has text analytics libraries that convert PDFs into the required encoding format. There are several of these PDF libraries, however, they sometimes yield varying results. Here, we demonstrate one such library, `pdfminer`. We have also tried other libraries like `pdfplumber`, so feel free to also experiment on which PDF parser works best for your corpus. \n",
        "\n",
        "We create a dataframe `df_text` which contains information for each document - the reference number (unique ID), file name, and complete text of the document. The full text of the document is parsed using `extract_text` function of pdfminer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reference</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F550241</td>\n",
              "      <td>F550241-190326-Dobson_report-FINAL_VERSION.</td>\n",
              "      <td>Ref. Ares(2020)4669723 - 08/09/2020\\n\\nLEVELLI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F550737</td>\n",
              "      <td>F550737-Mediaset_-_NCT_-_Position_paper_-_fina...</td>\n",
              "      <td>New Competition Tool \\n\\nPosition Paper \\n\\nMe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F550604</td>\n",
              "      <td>F550604-LUISS_Study-Executive-summary.</td>\n",
              "      <td>Ref. Ares(2020)4713545 - 09/09/2020\\n\\nTHE EUR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F541861</td>\n",
              "      <td>F541861-ACT_-_Perspectives_on_the_NCT_-_FINAL.</td>\n",
              "      <td>ACT PERSPECTIVES ON THE NEW COMPETITION TOOL \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F549332</td>\n",
              "      <td>F549332-MCA_DSA_and_NCT_public_consultation_po...</td>\n",
              "      <td>The  Malta  Communications  Authority’s  ratio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Reference                                          file_name  \\\n",
              "0   F550241        F550241-190326-Dobson_report-FINAL_VERSION.   \n",
              "1   F550737  F550737-Mediaset_-_NCT_-_Position_paper_-_fina...   \n",
              "2   F550604             F550604-LUISS_Study-Executive-summary.   \n",
              "3   F541861     F541861-ACT_-_Perspectives_on_the_NCT_-_FINAL.   \n",
              "4   F549332  F549332-MCA_DSA_and_NCT_public_consultation_po...   \n",
              "\n",
              "                                                text  \n",
              "0  Ref. Ares(2020)4669723 - 08/09/2020\\n\\nLEVELLI...  \n",
              "1  New Competition Tool \\n\\nPosition Paper \\n\\nMe...  \n",
              "2  Ref. Ares(2020)4713545 - 09/09/2020\\n\\nTHE EUR...  \n",
              "3  ACT PERSPECTIVES ON THE NEW COMPETITION TOOL \\...  \n",
              "4  The  Malta  Communications  Authority’s  ratio...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_text = pd.DataFrame(columns = ['Reference', 'file_name', 'text'])\n",
        "\n",
        "for pdf_file in pdf_list:\n",
        "    Reference = re.search('[F][0-9]{6}', pdf_file)[0]\n",
        "    file_name = re.search('[F][0-9]{6}(.*)[\\\\>.]', pdf_file)[0]\n",
        "    text = extract_text(pdf_file)\n",
        "    row = pd.DataFrame({'Reference': Reference,'file_name': file_name, 'text': text}, index=[0])\n",
        "    df_text = pd.concat([row,df_text.loc[:]]).reset_index(drop=True)\n",
        "\n",
        "df_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`df_text` is merged with the `dma` dataframe created in the [Data Description](#data-description) section using the unique Reference code of each document and respondent. Out of the 188 survey responses, only 85 have pdf submissions and some are duplicated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reference</th>\n",
              "      <th>Feedback date</th>\n",
              "      <th>Language</th>\n",
              "      <th>User type</th>\n",
              "      <th>First name</th>\n",
              "      <th>Surname</th>\n",
              "      <th>Scope</th>\n",
              "      <th>Organisation name</th>\n",
              "      <th>Transparency register number</th>\n",
              "      <th>Organisation size</th>\n",
              "      <th>...</th>\n",
              "      <th>Q201</th>\n",
              "      <th>Q202</th>\n",
              "      <th>Q203</th>\n",
              "      <th>Q204</th>\n",
              "      <th>Q205</th>\n",
              "      <th>Q206</th>\n",
              "      <th>submit</th>\n",
              "      <th>label_132</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F550828</td>\n",
              "      <td>8.09.2020 23:57</td>\n",
              "      <td>English</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>See our response above.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No.</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F550827</td>\n",
              "      <td>8.09.2020 23:54</td>\n",
              "      <td>English</td>\n",
              "      <td>NGO (Non-governmental organisation)</td>\n",
              "      <td>Juliane</td>\n",
              "      <td>von Reppert-Bismarck</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lie Detectors</td>\n",
              "      <td>094738529674-10</td>\n",
              "      <td>Small (&lt; 50 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>Most effective</td>\n",
              "      <td>An additional regulatory framework imposing ob...</td>\n",
              "      <td>Lie_Detectors_-_Digital_Services_Act_-_New_Com...</td>\n",
              "      <td>Please see attached submission</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>F550827-Lie_Detectors_-_Digital_Services_Act_-...</td>\n",
              "      <td>8 Sept, 2020 \\n\\nDigital Services Act and New ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F550826</td>\n",
              "      <td>8.09.2020 23:50</td>\n",
              "      <td>German</td>\n",
              "      <td>Business Association</td>\n",
              "      <td>Antje</td>\n",
              "      <td>Woltermann</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zentralverband Deutsches Kfz-Gewerbe e.V.</td>\n",
              "      <td>71649103246-10</td>\n",
              "      <td>Medium (&lt; 250 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>Sufficiently effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>-</td>\n",
              "      <td>Positionspapier_Gleichberechtigter_Zugang_zum_...</td>\n",
              "      <td>-</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>F550826-Positionspapier_Gleichberechtigter_Zug...</td>\n",
              "      <td>Ref. Ares(2020)4723306 - 10/09/2020\\n\\nGemeins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F550825</td>\n",
              "      <td>8.09.2020 23:45</td>\n",
              "      <td>English</td>\n",
              "      <td>Academic/Research Institution</td>\n",
              "      <td>DIANA</td>\n",
              "      <td>MONTENEGRO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Master student candidate Master in Competition...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Large (250 or more)</td>\n",
              "      <td>...</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>Very effective</td>\n",
              "      <td>all are adequated</td>\n",
              "      <td>PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_COMMISIO...</td>\n",
              "      <td>The document I prepared was more than 1MB.</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>F550825-PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_...</td>\n",
              "      <td>Ref. Ares(2020)4723305 - 10/09/2020\\n\\nTHE LEG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F550824</td>\n",
              "      <td>8.09.2020 23:33</td>\n",
              "      <td>English</td>\n",
              "      <td>Business Association</td>\n",
              "      <td>Kamila</td>\n",
              "      <td>Sotomska</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zwi?zek Przedsi?biorców i Pracodawców</td>\n",
              "      <td>868073924175-77</td>\n",
              "      <td>Small (&lt; 50 employees)</td>\n",
              "      <td>...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>Not applicable /No relevant experience or know...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 223 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Reference    Feedback date Language                            User type  \\\n",
              "0   F550828  8.09.2020 23:57  English                                  NaN   \n",
              "1   F550827  8.09.2020 23:54  English  NGO (Non-governmental organisation)   \n",
              "2   F550826  8.09.2020 23:50   German                 Business Association   \n",
              "3   F550825  8.09.2020 23:45  English        Academic/Research Institution   \n",
              "4   F550824  8.09.2020 23:33  English                 Business Association   \n",
              "\n",
              "  First name               Surname Scope  \\\n",
              "0        NaN                   NaN   NaN   \n",
              "1    Juliane  von Reppert-Bismarck   NaN   \n",
              "2      Antje            Woltermann   NaN   \n",
              "3      DIANA            MONTENEGRO   NaN   \n",
              "4     Kamila              Sotomska   NaN   \n",
              "\n",
              "                                   Organisation name  \\\n",
              "0                                                NaN   \n",
              "1                                      Lie Detectors   \n",
              "2          Zentralverband Deutsches Kfz-Gewerbe e.V.   \n",
              "3  Master student candidate Master in Competition...   \n",
              "4              Zwi?zek Przedsi?biorców i Pracodawców   \n",
              "\n",
              "  Transparency register number         Organisation size  ...  \\\n",
              "0                          NaN                       NaN  ...   \n",
              "1              094738529674-10    Small (< 50 employees)  ...   \n",
              "2               71649103246-10  Medium (< 250 employees)  ...   \n",
              "3                          NaN       Large (250 or more)  ...   \n",
              "4              868073924175-77    Small (< 50 employees)  ...   \n",
              "\n",
              "                                                Q201  \\\n",
              "0                                     Most effective   \n",
              "1                                     Most effective   \n",
              "2                             Sufficiently effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q202  \\\n",
              "0                                     Very effective   \n",
              "1                                     Most effective   \n",
              "2                                     Very effective   \n",
              "3                                     Very effective   \n",
              "4  Not applicable /No relevant experience or know...   \n",
              "\n",
              "                                                Q203  \\\n",
              "0                            See our response above.   \n",
              "1  An additional regulatory framework imposing ob...   \n",
              "2                                                  -   \n",
              "3                                  all are adequated   \n",
              "4                                                NaN   \n",
              "\n",
              "                                                Q204  \\\n",
              "0                                                NaN   \n",
              "1  Lie_Detectors_-_Digital_Services_Act_-_New_Com...   \n",
              "2  Positionspapier_Gleichberechtigter_Zugang_zum_...   \n",
              "3  PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_COMMISIO...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                         Q205 Q206 submit label_132  \\\n",
              "0                                         No.  Yes      0       Yes   \n",
              "1              Please see attached submission  Yes      1       Yes   \n",
              "2                                           -  Yes      1       Yes   \n",
              "3  The document I prepared was more than 1MB.  Yes      1       Yes   \n",
              "4                                         NaN  Yes      0       Yes   \n",
              "\n",
              "                                           file_name  \\\n",
              "0                                                NaN   \n",
              "1  F550827-Lie_Detectors_-_Digital_Services_Act_-...   \n",
              "2  F550826-Positionspapier_Gleichberechtigter_Zug...   \n",
              "3  F550825-PREPARATIVE_DOCUMENT_FOR_THE_EUROEPAN_...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                                text  \n",
              "0                                                NaN  \n",
              "1  8 Sept, 2020 \\n\\nDigital Services Act and New ...  \n",
              "2  Ref. Ares(2020)4723306 - 10/09/2020\\n\\nGemeins...  \n",
              "3  Ref. Ares(2020)4723305 - 10/09/2020\\n\\nTHE LEG...  \n",
              "4                                                NaN  \n",
              "\n",
              "[5 rows x 223 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged = dma.merge(df_text, how='left', on='Reference')\n",
        "df_merged.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the next step, we we remove duplicate observations and use only responses with both PDF submission and label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged = df_merged[df_merged['submit']==1]\n",
        "df_merged = df_merged.drop_duplicates(subset='Reference', keep='first')\n",
        "len(df_merged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A.2 Cleaning and Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After parsing PDFs, we need to further process the raw text to ensure that most of the information we feed into our model/s is relevant to the task at hand. For instance, stopwords like \"the\", \"this\", \"and\" will not give us any indication of whether a stakeholder agrees with a law, so we can remove these words. \n",
        "\n",
        "The pre-processing steps that we apply for this tutorial are the following: \n",
        "\n",
        "1. Removal of stopwords, punctuations, and numeric characters\n",
        "2. Stemming and lemmatization\n",
        "3. Coreference resolution\n",
        "4. Language detection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what the raw text looks like before processing. White space characters (*\"/n\"*) are interspersed with the text, and there are mentions of dates and numbers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'8 Sept, 2020 \\n\\nDigital Services Act and New Competition Tool  \\n\\nLie Detectors response to public consultations \\n\\nLie  Detectors,  an  award-winning  journalist-led  media  literacy  campaign  in  Europe,  welcomes  the \\nability to respond to the Commission consultation on the Digital Services Act and the New Competition \\nTool.  \\n\\nIn relation to the Digital Services Act consultation, Lie Detectors supports the introduction of Ex Ante \\nRegulation for large online platforms with significant network effects acting as gatekeepers, and of a \\nNew Competition Tool.  \\n\\nThe  instinct  of  regulators  and  policymakers  to  hold  large  platforms  like  Facebook  and  Google  to \\naccount for their role in the proliferation of online disinformation and in the undermining of quality \\njournalism is right. With the necessary political will, solutions exist that will help rein in the epidemic \\nof  disinformation  that  is  sweeping  away  trust  in  established  facts,  in  scientific  method  and  in \\ndemocratic institutions designed to protect us. \\n\\nThe  basis  of  such  solutions  lies  in  taking  on  disinformation  at  its  source,  that  is,  by  taking  on  the \\nbusiness model of large platforms such as Facebook and Google which stoke outrage for revenue and \\nengage  in  the  “monetising  of  lies”  as  the  European  Commission  itself  has  called  it.  Following  the \\nmoney  and  applying  existing  and  new  antitrust  principles  is  a  fundamental  avenue  for  securing \\nEuropean democracy. \\n\\nOther approaches have proven incapable of denting the outrage economy and its corrosive effects on \\ndemocracy.  Fact-checking  initiatives  has  long  been  the  darling  of  conflict-shy  regulators  and \\npolicymakers looking for quick and high-profile fixes. While laudable and valuable to an extent, these \\ninitiatives have proven to touch only the tip of the iceberg. Disinformation has continued to adapt, \\nproliferate and confound the most experienced fact-checkers, evading detection in encrypted spaces \\nand image- and video-based platforms owned by the largest and most powerful platforms. With little \\nrealistic recourse to alternatives, users remain trapped in a cycle of providing data for the platforms’ \\nalgorithms,  which  exacerbate  the  reach  and  effect  of  disinformation.  Foreign  powers  continue  to \\nsubvert democratic processes via manipulative campaigns on the largest platforms. \\n\\nAt the same time, the power of the platforms is has grown exponentially, not only via their dominant \\nposition but also through widely-reported influence-buying campaigns that have gone a significant \\nway to co-opting  the very information sectors they undermine: journalism and media organisations \\nincreasingly  dependent  on  fact-checking  contracts  from  the  platforms;  academia  increasingly \\ndependent  on  data-analysis  contracts  from  the  platforms,  and  even  policymakers  who  see  no \\nalternative but depending on the platforms for rooting out illegal and borderline illegal content one \\npost at a time.   \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cThe platforms’ non-adherence to the EU Code of Practice on Disinformation, condemned repeatedly \\nby the European Commission, highlights that self-regulation will fail when a business model built on \\ntrapping users and stoking outrage is at stake. \\n\\nThe  ongoing  pandemic  has  thrown  this  into  sharp  relief.  Young  people  report  being  increasingly \\nconcerned about conspiracy theories circulating in their social media groups and have trouble finding \\nreliable information online at a time when that information saves lives. \\n\\nThe European Commission is uniquely placed to withstand the pressures of the platforms and should \\nuse competition tools to investigate links between the advertising business model of the platform and \\nthe dissemination of disinformation.  \\n\\nGatekeeper  designation  and  a  review  of  the  liability  regime  of  digital  services  acting  as  publishers \\nshould  aim  among  other  things  to  re-establishing  some  viability  of  journalism  as  a  fundamental \\ndemocratic  service  to  EU  citizens.  The  challenge  posed  by  large  platforms  requires  new  tools  in \\naddition  to  traditional  competition  law  enforcement  in  order  to  protect  consumers’  interests  and \\ndemocracy itself. \\n\\n-0- \\n\\nLie  Detectors  is  an  award-winning  journalist-led  media-literacy  campaign  in  Europe.  The  non-profit \\norganisation works with more than 200 professional journalists to secure democracy by empowering \\ntens of thousands of young people and their teachers to tackle online disinformation and by fostering \\nunderstanding of quality journalism. Our advocacy arises from the findings of our work with children \\nand teachers across Europe and focuses on tackling disinformation from the demand perspective and \\nthe supply perspective. We have advised the European Commission as a member of the High-Level \\nExpert Group on Fake News and Online Disinformation and the Media Literacy Expert Group.  \\n\\nWhen  addressing  the  demand-side  of  disinformation,  we  support  the  integration  of  critical  media \\nliteracy  into  school  rankings  gauges,  school  curricula  and  teacher-training  curricula,  and  the \\nindependence of journalism and education from corporate interest. \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged.text[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Removal of stopwords, punctuations, numeric characters\n",
        "As the first pre-processing step, we create a function that only lowercases all tokens and retains only those that are not in the dictionary of english stop words, not a punctuation mark, and not numeric characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_corpus(texts):\n",
        "    eng_stopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        token_list =  [token.lower() for token in tokens if token not in eng_stopwords and token not in punctuation and token.isdigit() == False]\n",
        "        processed_text = ' '.join(token_list)\n",
        "        return processed_text\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged['text_clean'] = preprocess_corpus(df_merged['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stemming and lemmatization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a function which stems and lemmatizes tokens. This is to retain only principal and root words and reduce them to their dictionary form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stem_lemmatize(text):\n",
        "    stemmed = [stemmer.stem(token) for token in word_tokenize(text)]\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in stemmed]\n",
        "    processed_text = ' '.join(lemmatized)\n",
        "    return processed_text\n",
        "\n",
        "df_merged['text_clean'] = [stem_lemmatize(text) for text in df_merged['text_clean']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Coreference resolution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an optional pre-processing step, we apply coreference resolution to idenfity and link multiple mentions of an entity (e.g. Google = the company, Commission = EU). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# installing neuralcoref from source\n",
        "!git clone https://github.com/huggingface/neuralcoref.git\n",
        "!cd neuralcoref\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x285839df0>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg') \n",
        "neuralcoref.add_to_pipe(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coref_res(texts):\n",
        "    doc = nlp(texts)\n",
        "    clean = doc._.coref_resolved\n",
        "    return clean\n",
        "\n",
        "df_merged['text_clean'] = [coref_res(text) for text in df_merged['text_clean']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language detection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After cleaning the text, we use language detection to subset documents that are written in English. The final corpus contains 75 public consultation documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "en    75\n",
              "de     7\n",
              "fr     2\n",
              "ro     1\n",
              "Name: lang, dtype: int64"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index, row in df_merged.iterrows():\n",
        "    df_merged.at[index, 'lang'] = detect(df_merged.at[index, 'text_clean'])\n",
        "\n",
        "df_merged.lang.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged = df_merged[df_merged['lang']==\"en\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Final dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df_merged[['Reference', 'Feedback date', 'User type', 'Scope', \n",
        "                    'Organisation name', 'Transparency register number', 'Organisation size',\n",
        "                    'label_132', 'submit', 'file_name', 'lang', 'text', 'text_clean']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final.reset_index(drop=True).to_json(r\"./data/df_final_document.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The processed text should look like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sept digit servic act new competit tool lie detector respon public consult lie detector award-win journalist-l medium literaci campaign europ welcom abil respond commiss consult digit servic act new competit tool in relat digit servic act consult lie detector support introduct ex ant regul larg onlin platform signif network effect act gatekeep new competit tool the instinct regul policymak hold larg platform like facebook googl account role prolif onlin disinform undermin qualiti journal right with necessari polit solut exist help rein epidem disinform sweep away trust establish fact scientif method democrat institut design protect u the basi solut lie take disinform sourc take busi model larg platform facebook googl stoke outrag revenu engag “ moneti lie ” european commiss call follow money appli exist new antitrust principl fundament avenu secur european democraci other approach proven incap dent outrag economi corro effect democraci fact-check initi long darl conflict-shi regul policymak look quick high-profil fix while laudabl valuabl extent initi proven touch tip iceberg disinform continu adapt prolif confound experienc fact-check evad detect encrypt space image- video-ba platform own largest power platform with littl realist recour altern user remain trap cycl provid data platform ’ algorithm exacerb reach effect disinform foreign power continu subvert democrat process via manipul campaign largest platform at time power platform grown exponenti via domin posit also widely-report influence-buy campaign gone signif way co-opt inform sector undermin journal medium organi increasingli depend fact-check contract platform academia increasingli depend data-analysi contract platform even policymak see altern depend platform root illeg borderlin illeg content one post time the platform ’ non-adh eu code practic disinform condemn repeatedli european commiss highlight self-regul fail busi model built trap user stoke outrag stake the ongo pandem thrown sharp relief young peopl report increasingli concern conspiraci theori circul social medium group troubl find reliabl inform onlin time inform save live the european commiss uniqu place withstand pressur platform use competit tool investig link adverti busi model platform dissemin disinform gatekeep design review liabil regim digit servic act publish aim among thing re-establish viabil journal fundament democrat servic eu citizen the challeng pose larg platform requir new tool addit tradit competit law enforc order protect consum ’ interest democraci -0- lie detector award-win journalist-l media-literaci campaign europ the non-profit organi work profess journalist secur democraci empow ten thousand young peopl teacher tackl onlin disinform foster understand qualiti journal our advocaci ari find work child teacher across europ focu tackl disinform demand perspect suppli perspect we advi european commiss member high-level expert group fake news onlin disinform medium literaci expert group when address demand-sid disinform support integr critic medium literaci school rank gaug school curriculum teacher-train curriculum independ journal educ corpor interest'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final.text_clean[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_json(r\"./data/df_final_document.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o8L28axZ4NVj"
      },
      "source": [
        "## B. Text Representation\n",
        "In this section of the tutorial, we demonstrate how to **convert raw text into numerical form** that can be readily fed into different machine learning and deep learning algorithms. There are two main ways to do this:\n",
        "\n",
        "1. Basic vectorization\n",
        "2. Distributed representations. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B.1 Basic Vectorization\n",
        "Basic vectorization techniques map each word of the corpus vocabulary (V) to a numeric value and represents each document as a V-dimensional vector. These methods are simple and straightforward and can be used to construct ML-based text and document classifiers with interpretable features. The most common methods for vectorization are one-hot encoding, bag of words, and the term frequency–inverse document frequency (TF-IDF) matrix. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TF-IDF\n",
        "Most basic vectorizaton approaches treat words in a text as equally important. TF-IDF introduces a weighting system which quantifies the importance of a given word relative to other words in the document and in the corpus. Below we show how to convert our corpus into a TF-IDF representation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our TF-IDF matrix has 75 documents with 10,527 features or words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(75, 10527)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "dfm = vectorizer.fit_transform(df['text_clean'])\n",
        "dfm.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B.2 Distibuted Representations\n",
        "Distributed representations or **embeddings** are dense, low-dimensional vectors which capture context and distributional similarities between words. These methods address some fundamental limitations of basic vectorization techniques. First, distributed representations are more computationally efficient since they don't retain the shape of the entire corpus vocabulary. Second, context and similarities between words are accounted for, unlike basic vectorizations where words are treated as atomic units. Finally, they provide a solution to the *out of vocabulary* problem, where a model is unable to represent a word that was not used in the training data. One of the biggest advantages of word embeddings is that they are able to generalize to unseen text.\n",
        "\n",
        "Word or document embeddings that may be pre-trained based on a big corpus (e.g. Word2Vec or GloVe) or trained based on your own set of documents (using CBOW or SkipGram).\n",
        "Below is an illustration of how to construct TF-IDF weighted word embeddings for our set of DMA consultation documents using pre-trained GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, load the GloVe embeddings which can be download from the NLP Stanford <a href = \"https://nlp.stanford.edu/projects/glove/\">website</a>. We will use the 100-dimensional GloVe embeddings and save it into a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "dims = 100\n",
        "\n",
        "z = ZipFile(\"data/glove.6B.zip\") # glove zip file saved in data folder\n",
        "f = z.open(f'glove.6B.{dims}d.txt')\n",
        "\n",
        "embed_matrix = pd.read_table(\n",
        "    f, sep = \" \", index_col = 0, \n",
        "    header = None, quoting = csv.QUOTE_NONE\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GloVe embeddings has a 400,000-word vocabulary and each word is represented by a vector of size 100. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.038194</td>\n",
              "      <td>-0.244870</td>\n",
              "      <td>0.728120</td>\n",
              "      <td>-0.399610</td>\n",
              "      <td>0.083172</td>\n",
              "      <td>0.043953</td>\n",
              "      <td>-0.391410</td>\n",
              "      <td>0.334400</td>\n",
              "      <td>-0.57545</td>\n",
              "      <td>0.087459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016215</td>\n",
              "      <td>-0.017099</td>\n",
              "      <td>-0.389840</td>\n",
              "      <td>0.87424</td>\n",
              "      <td>-0.725690</td>\n",
              "      <td>-0.510580</td>\n",
              "      <td>-0.520280</td>\n",
              "      <td>-0.145900</td>\n",
              "      <td>0.82780</td>\n",
              "      <td>0.270620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.110530</td>\n",
              "      <td>0.598120</td>\n",
              "      <td>-0.543610</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.06351</td>\n",
              "      <td>-0.094189</td>\n",
              "      <td>...</td>\n",
              "      <td>0.349510</td>\n",
              "      <td>-0.722600</td>\n",
              "      <td>0.375490</td>\n",
              "      <td>0.44410</td>\n",
              "      <td>-0.990590</td>\n",
              "      <td>0.612140</td>\n",
              "      <td>-0.351110</td>\n",
              "      <td>-0.831550</td>\n",
              "      <td>0.45293</td>\n",
              "      <td>0.082577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>-0.339790</td>\n",
              "      <td>0.209410</td>\n",
              "      <td>0.463480</td>\n",
              "      <td>-0.647920</td>\n",
              "      <td>-0.383770</td>\n",
              "      <td>0.038034</td>\n",
              "      <td>0.171270</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>0.46619</td>\n",
              "      <td>-0.019169</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063351</td>\n",
              "      <td>-0.674120</td>\n",
              "      <td>-0.068895</td>\n",
              "      <td>0.53604</td>\n",
              "      <td>-0.877730</td>\n",
              "      <td>0.318020</td>\n",
              "      <td>-0.392420</td>\n",
              "      <td>-0.233940</td>\n",
              "      <td>0.47298</td>\n",
              "      <td>-0.028803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.152900</td>\n",
              "      <td>-0.242790</td>\n",
              "      <td>0.898370</td>\n",
              "      <td>0.169960</td>\n",
              "      <td>0.535160</td>\n",
              "      <td>0.487840</td>\n",
              "      <td>-0.588260</td>\n",
              "      <td>-0.179820</td>\n",
              "      <td>-1.35810</td>\n",
              "      <td>0.425410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187120</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>-0.267570</td>\n",
              "      <td>0.72700</td>\n",
              "      <td>-0.593630</td>\n",
              "      <td>-0.348390</td>\n",
              "      <td>-0.560940</td>\n",
              "      <td>-0.591000</td>\n",
              "      <td>1.00390</td>\n",
              "      <td>0.206640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.189700</td>\n",
              "      <td>0.050024</td>\n",
              "      <td>0.190840</td>\n",
              "      <td>-0.049184</td>\n",
              "      <td>-0.089737</td>\n",
              "      <td>0.210060</td>\n",
              "      <td>-0.549520</td>\n",
              "      <td>0.098377</td>\n",
              "      <td>-0.20135</td>\n",
              "      <td>0.342410</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.131340</td>\n",
              "      <td>0.058617</td>\n",
              "      <td>-0.318690</td>\n",
              "      <td>-0.61419</td>\n",
              "      <td>-0.623930</td>\n",
              "      <td>-0.415480</td>\n",
              "      <td>-0.038175</td>\n",
              "      <td>-0.398040</td>\n",
              "      <td>0.47647</td>\n",
              "      <td>-0.159830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chanty</th>\n",
              "      <td>-0.155770</td>\n",
              "      <td>-0.049188</td>\n",
              "      <td>-0.064377</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>-0.201460</td>\n",
              "      <td>-0.038963</td>\n",
              "      <td>0.129710</td>\n",
              "      <td>-0.294510</td>\n",
              "      <td>0.00359</td>\n",
              "      <td>-0.098377</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093324</td>\n",
              "      <td>0.094486</td>\n",
              "      <td>-0.023469</td>\n",
              "      <td>-0.48099</td>\n",
              "      <td>0.623320</td>\n",
              "      <td>0.024318</td>\n",
              "      <td>-0.275870</td>\n",
              "      <td>0.075044</td>\n",
              "      <td>-0.56380</td>\n",
              "      <td>0.145010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kronik</th>\n",
              "      <td>-0.094426</td>\n",
              "      <td>0.147250</td>\n",
              "      <td>-0.157390</td>\n",
              "      <td>0.071966</td>\n",
              "      <td>-0.298450</td>\n",
              "      <td>0.039432</td>\n",
              "      <td>0.021870</td>\n",
              "      <td>0.008041</td>\n",
              "      <td>-0.18682</td>\n",
              "      <td>-0.311010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.305450</td>\n",
              "      <td>-0.011082</td>\n",
              "      <td>0.118550</td>\n",
              "      <td>-0.11312</td>\n",
              "      <td>0.339510</td>\n",
              "      <td>-0.224490</td>\n",
              "      <td>0.257430</td>\n",
              "      <td>0.631430</td>\n",
              "      <td>-0.20090</td>\n",
              "      <td>-0.105420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rolonda</th>\n",
              "      <td>0.360880</td>\n",
              "      <td>-0.169190</td>\n",
              "      <td>-0.327040</td>\n",
              "      <td>0.098332</td>\n",
              "      <td>-0.429700</td>\n",
              "      <td>-0.188740</td>\n",
              "      <td>0.455560</td>\n",
              "      <td>0.285290</td>\n",
              "      <td>0.30340</td>\n",
              "      <td>-0.366830</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044082</td>\n",
              "      <td>0.140030</td>\n",
              "      <td>0.300070</td>\n",
              "      <td>-0.12731</td>\n",
              "      <td>-0.143040</td>\n",
              "      <td>-0.069396</td>\n",
              "      <td>0.281600</td>\n",
              "      <td>0.271390</td>\n",
              "      <td>-0.29188</td>\n",
              "      <td>0.161090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zsombor</th>\n",
              "      <td>-0.104610</td>\n",
              "      <td>-0.504700</td>\n",
              "      <td>-0.493310</td>\n",
              "      <td>0.135160</td>\n",
              "      <td>-0.363710</td>\n",
              "      <td>-0.447500</td>\n",
              "      <td>0.184290</td>\n",
              "      <td>-0.056510</td>\n",
              "      <td>0.40474</td>\n",
              "      <td>-0.725830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.151530</td>\n",
              "      <td>-0.108420</td>\n",
              "      <td>0.340640</td>\n",
              "      <td>-0.40916</td>\n",
              "      <td>-0.081263</td>\n",
              "      <td>0.095315</td>\n",
              "      <td>0.150180</td>\n",
              "      <td>0.425270</td>\n",
              "      <td>-0.51250</td>\n",
              "      <td>-0.170540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sandberger</th>\n",
              "      <td>0.283650</td>\n",
              "      <td>-0.626300</td>\n",
              "      <td>-0.443510</td>\n",
              "      <td>0.217700</td>\n",
              "      <td>-0.087421</td>\n",
              "      <td>-0.170620</td>\n",
              "      <td>0.292660</td>\n",
              "      <td>-0.024899</td>\n",
              "      <td>0.26414</td>\n",
              "      <td>-0.170230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.138850</td>\n",
              "      <td>-0.228620</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>-0.43208</td>\n",
              "      <td>0.539800</td>\n",
              "      <td>-0.085806</td>\n",
              "      <td>0.032651</td>\n",
              "      <td>0.436780</td>\n",
              "      <td>-0.82607</td>\n",
              "      <td>-0.157010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 1         2         3         4         5         6    \\\n",
              "0                                                                        \n",
              "the        -0.038194 -0.244870  0.728120 -0.399610  0.083172  0.043953   \n",
              ",          -0.107670  0.110530  0.598120 -0.543610  0.673960  0.106630   \n",
              ".          -0.339790  0.209410  0.463480 -0.647920 -0.383770  0.038034   \n",
              "of         -0.152900 -0.242790  0.898370  0.169960  0.535160  0.487840   \n",
              "to         -0.189700  0.050024  0.190840 -0.049184 -0.089737  0.210060   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "chanty     -0.155770 -0.049188 -0.064377  0.223600 -0.201460 -0.038963   \n",
              "kronik     -0.094426  0.147250 -0.157390  0.071966 -0.298450  0.039432   \n",
              "rolonda     0.360880 -0.169190 -0.327040  0.098332 -0.429700 -0.188740   \n",
              "zsombor    -0.104610 -0.504700 -0.493310  0.135160 -0.363710 -0.447500   \n",
              "sandberger  0.283650 -0.626300 -0.443510  0.217700 -0.087421 -0.170620   \n",
              "\n",
              "                 7         8        9         10   ...       91        92   \\\n",
              "0                                                  ...                       \n",
              "the        -0.391410  0.334400 -0.57545  0.087459  ...  0.016215 -0.017099   \n",
              ",           0.038867  0.354810  0.06351 -0.094189  ...  0.349510 -0.722600   \n",
              ".           0.171270  0.159780  0.46619 -0.019169  ... -0.063351 -0.674120   \n",
              "of         -0.588260 -0.179820 -1.35810  0.425410  ...  0.187120 -0.018488   \n",
              "to         -0.549520  0.098377 -0.20135  0.342410  ... -0.131340  0.058617   \n",
              "...              ...       ...      ...       ...  ...       ...       ...   \n",
              "chanty      0.129710 -0.294510  0.00359 -0.098377  ...  0.093324  0.094486   \n",
              "kronik      0.021870  0.008041 -0.18682 -0.311010  ... -0.305450 -0.011082   \n",
              "rolonda     0.455560  0.285290  0.30340 -0.366830  ... -0.044082  0.140030   \n",
              "zsombor     0.184290 -0.056510  0.40474 -0.725830  ...  0.151530 -0.108420   \n",
              "sandberger  0.292660 -0.024899  0.26414 -0.170230  ...  0.138850 -0.228620   \n",
              "\n",
              "                 93       94        95        96        97        98   \\\n",
              "0                                                                       \n",
              "the        -0.389840  0.87424 -0.725690 -0.510580 -0.520280 -0.145900   \n",
              ",           0.375490  0.44410 -0.990590  0.612140 -0.351110 -0.831550   \n",
              ".          -0.068895  0.53604 -0.877730  0.318020 -0.392420 -0.233940   \n",
              "of         -0.267570  0.72700 -0.593630 -0.348390 -0.560940 -0.591000   \n",
              "to         -0.318690 -0.61419 -0.623930 -0.415480 -0.038175 -0.398040   \n",
              "...              ...      ...       ...       ...       ...       ...   \n",
              "chanty     -0.023469 -0.48099  0.623320  0.024318 -0.275870  0.075044   \n",
              "kronik      0.118550 -0.11312  0.339510 -0.224490  0.257430  0.631430   \n",
              "rolonda     0.300070 -0.12731 -0.143040 -0.069396  0.281600  0.271390   \n",
              "zsombor     0.340640 -0.40916 -0.081263  0.095315  0.150180  0.425270   \n",
              "sandberger  0.071792 -0.43208  0.539800 -0.085806  0.032651  0.436780   \n",
              "\n",
              "                99        100  \n",
              "0                              \n",
              "the         0.82780  0.270620  \n",
              ",           0.45293  0.082577  \n",
              ".           0.47298 -0.028803  \n",
              "of          1.00390  0.206640  \n",
              "to          0.47647 -0.159830  \n",
              "...             ...       ...  \n",
              "chanty     -0.56380  0.145010  \n",
              "kronik     -0.20090 -0.105420  \n",
              "rolonda    -0.29188  0.161090  \n",
              "zsombor    -0.51250 -0.170540  \n",
              "sandberger -0.82607 -0.157010  \n",
              "\n",
              "[400000 rows x 100 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We find the words in our corpus that are also present in the GloVe embedding matrix. Results below show that there are 6,487 words in common with GloVe. We take the indices of these common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6487"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_features = set(embed_matrix.index) & set(vectorizer.get_feature_names_out())\n",
        "len(common_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7897, 6866, 10396, 1494, 7119, 9995, 9804, 10335, 6570]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_ids = [vectorizer.vocabulary_[x] for x in common_features]\n",
        "vocab_ids[1:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using common features only, we multiply our 75 x 6487 TF-IDF matrix by the 6487 x 100 embedding matrix to get the document embedding matrix for all consultation documents. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rd/n9w0gpv53y72x5k9wk3hp63w0000gn/T/ipykernel_65960/3754221714.py:1: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
            "  doc_matrix = dfm[:,vocab_ids].dot(embed_matrix.loc[common_features,])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(75, 100)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_matrix = dfm[:,vocab_ids].dot(embed_matrix.loc[common_features,])\n",
        "doc_matrix.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Model Training\n",
        "In this section of the tutorial, we compare 3 different types text classification models:\n",
        "\n",
        "1. Traditional machine learning classifiers - logistic regression and gradient boosting\n",
        "2. Deep learning model using sequence networks - LSTM\n",
        "3. Deep learning transformer-based model - BERT\n",
        "\n",
        "Traditional machine learning classifiers like logistic regression and gradient boosting have the advantage of introducing \"hand-crafted\" text features that are interpretable. For instance, tree-based methods can use features such as number of characters and number of mentions of a word as nodes. On the other hand, deep learning methods based on neural network architectures can learn features of the data given just the raw text. This yields features that are more in line with the task at hand, leading to improved model performance. \n",
        "\n",
        "The discussion below will mainly highlight the use of the **Bidirectional Encoder Representations from Transformers** or **BERT** for text classification tasks. Results of other models will be shown but the methodology will not be discussed in deatil. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df['label.132'] = le.fit_transform(df['label_132'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.1 Baseline Models\n",
        "For our baseline models, we will use traditional ML classifiers - logistic regression and gradient boosting - with the document embedding matrix above as features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Logistic Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we split our data (the document embedding matrix and corresponding labels) into train and test sets. For the purpose of this demonstration, we only split our data into training and testing since the number of documents are limited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into train and test sets\n",
        "l_train_text, l_test_text, l_train_labels, l_test_labels = train_test_split(doc_matrix, df['label.132'], \n",
        "                                                                            random_state=2018, \n",
        "                                                                            test_size=0.3, \n",
        "                                                                            stratify=df['label.132'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fit a logistic regression model using embedding vectors as features. The accuracy, precision, recall, and F1 scores are shown below. The trained model is saved to a pkl file so we can later on access its results and compare with the other models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Accuracy: 56.52 \n",
            " Precision: 0.588 \n",
            " Recall: 0.769 \n",
            " F1: 0.667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(l_train_text, l_train_labels)\n",
        "with open('./models/logistic.pkl', 'wb') as f: pickle.dump(clf, f)\n",
        "\n",
        "y_pred = clf.predict(l_test_text)\n",
        "\n",
        "accuracy = accuracy_score(l_test_labels, y_pred) *100.0\n",
        "precision = precision_score(l_test_labels, y_pred, average='binary')\n",
        "recall = recall_score(l_test_labels, y_pred, average='binary')\n",
        "f_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Gradient Boosting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do the same thing for an XGBoost model. Here we see that the logistic regression yields slightly better results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
            "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
            "              early_stopping_rounds=None, enable_categorical=False,\n",
            "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
            "              grow_policy='depthwise', importance_type=None,\n",
            "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
            "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
            "              max_depth=1000, max_leaves=0, min_child_weight=1, missing=nan,\n",
            "              monotone_constraints='()', n_estimators=1000, n_jobs=0,\n",
            "              num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
            " Accuracy: 43.48 \n",
            " Precision: 0.500 \n",
            " Recall: 0.615 \n",
            " F1: 0.552\n"
          ]
        }
      ],
      "source": [
        "bst = XGBClassifier(n_estimators=1000, max_depth=1000, learning_rate=0.1, objective='binary:logistic')\n",
        "\n",
        "bst.fit(l_train_text, l_train_labels)\n",
        "\n",
        "print(bst)\n",
        "\n",
        "y_pred = bst.predict(l_test_text)\n",
        "\n",
        "accuracy = accuracy_score(l_test_labels, y_pred) * 100.0\n",
        "precision = precision_score(l_test_labels, y_pred, average='binary')\n",
        "recall = recall_score(l_test_labels, y_pred, average='binary')\n",
        "f_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.2 Long Short-Term Memory Network\n",
        "LSTM is a variant of Recurrent Neural Networks which are designed for handling sequential data such as text. Before transformer models became popular, RNNs were the go-to models for language processing. LSTM models handle long term dependencies in text through an architecture that uses three different types of gates - input, output, and forget gates. The gates operate together to decide which information to retain in the LSTM cell. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with our baseline models, we start by splitting the data into train and test sets. This time, we use raw text instead of the document embedding matrix. The LSTM architecture includes an embedding layer which learns feature representations from the data. Note that we can also use pre-trained embeddings in this layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into train and test sets\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(df['text_clean'], df['label.132'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label.132'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create train and test dataset \n",
        "train_dataset = list(zip(train_labels, train_text))\n",
        "test_dataset = list(zip(test_labels, test_text))\n",
        "\n",
        "# convert pd series to list\n",
        "train_text = train_text.tolist()\n",
        "test_text = test_text.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After splitting the data into training and test set, build the corpus vocabulary by tokenizing all texts and assigning each word to a unique index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def tokenize(datasets):\n",
        "    for dataset in datasets:\n",
        "        for text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(tokenize([train_text, test_text]), min_freq=1, specials=[\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1286, 659, 5401, 1]"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example\n",
        "tokens = tokenizer(\"This is an example.\")\n",
        "index = vocab(tokens)\n",
        "index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the vocabulary is built, we create batches of text sequences and map the tokens to indices. We also pad the sequence of words so all are of the same length. This returns a tensor of the sequence length and batch size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_classes = [\"0\", \"1\"]\n",
        "max_words = 100\n",
        "\n",
        "def vectorize_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X] # map tokens to index using vocab\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] # pad sequences\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=vectorize_batch, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=100, collate_fn=vectorize_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([23, 100]) torch.Size([23])\n"
          ]
        }
      ],
      "source": [
        "for X, Y in test_loader:\n",
        "    print(X.shape, Y.shape)\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we build a class for our LSTM classifier. For this tutorial we use an architecture with a 200-dimensional embedding layer, 3 hidden layers with 80 input features. The size of embedding and hidden layers, as well as the number of hidden layers are all hyper parameters. We set these values arbitrarily, but they can be optimzed by hyperparameter tuning techniques. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define hyperparameters\n",
        "embed_len = 200\n",
        "hidden_dim = 80\n",
        "n_layers = 3\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed_len = embed_len\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim), torch.randn(n_layers, len(X_batch), hidden_dim)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        return self.linear(output[:,-1])\n",
        "\n",
        "    def init_hidden(self):\n",
        "      return (\n",
        "               torch.zeros(n_layers, 1, self.hidden_dim, device=device),\n",
        "               torch.zeros(n_layers, 1, self.hidden_dim, device=device)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (embedding_layer): Embedding(12320, 200)\n",
              "  (lstm): LSTM(200, 80, num_layers=3, batch_first=True)\n",
              "  (linear): Linear(in_features=80, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier = LSTMClassifier()\n",
        "lstm_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer : Embedding(12320, 200)\n",
            "Parameters : \n",
            "torch.Size([12320, 200])\n",
            "\n",
            "Layer : LSTM(200, 80, num_layers=3, batch_first=True)\n",
            "Parameters : \n",
            "torch.Size([320, 200])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320])\n",
            "torch.Size([320])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320])\n",
            "torch.Size([320])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320])\n",
            "torch.Size([320])\n",
            "\n",
            "Layer : Linear(in_features=80, out_features=2, bias=True)\n",
            "Parameters : \n",
            "torch.Size([2, 80])\n",
            "torch.Size([2])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for layer in lstm_classifier.children():\n",
        "    print(\"Layer : {}\".format(layer))\n",
        "    print(\"Parameters : \")\n",
        "    for param in layer.parameters():\n",
        "        print(param.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1024, 2])"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = lstm_classifier(torch.randint(0, len(vocab), (1024, max_words)))\n",
        "out.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a function which trains that model and saves the weights of the model with the lowest validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loss_fn, val_loader):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X_test, Y_test in val_loader:\n",
        "            preds = model(X_test)\n",
        "            loss = loss_fn(preds, Y_test)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y_test)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
        "\n",
        "        return loss\n",
        "\n",
        "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    best_val_loss = 0.0  \n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds, Y) \n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad() \n",
        "            loss.backward() \n",
        "            optimizer.step() \n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        val_loss = evaluate(model, loss_fn, val_loader)\n",
        "\n",
        "        if val_loss > best_val_loss: # save model with the best accuracy\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), './models/lstm_saved_weights.pt')\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.679\n",
            "Valid Loss : 0.684\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.675\n",
            "Valid Loss : 0.682\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.670\n",
            "Valid Loss : 0.681\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.665\n",
            "Valid Loss : 0.679\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.659\n",
            "Valid Loss : 0.676\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.651\n",
            "Valid Loss : 0.672\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.639\n",
            "Valid Loss : 0.667\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.625\n",
            "Valid Loss : 0.661\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.605\n",
            "Valid Loss : 0.653\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.579\n",
            "Valid Loss : 0.645\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.547\n",
            "Valid Loss : 0.635\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.505\n",
            "Valid Loss : 0.626\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.455\n",
            "Valid Loss : 0.618\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.397\n",
            "Valid Loss : 0.615\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.333\n",
            "Valid Loss : 0.620\n",
            "Valid Acc  : 0.609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.269\n",
            "Valid Loss : 0.643\n",
            "Valid Acc  : 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.209\n",
            "Valid Loss : 0.682\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.159\n",
            "Valid Loss : 0.732\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.121\n",
            "Valid Loss : 0.786\n",
            "Valid Acc  : 0.696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.095\n",
            "Valid Loss : 0.853\n",
            "Valid Acc  : 0.652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "lstm_classifier = LSTMClassifier()\n",
        "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "train(lstm_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C.3 Transfer Learning with BERT and Other Variants"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the last four years, there have be great improvements in using neural network architectures for creating text representations. One new NLP architecture that has recently gained traction is **Transformers**. These are networks that handle long-distance dependence in sequence data using self-attention.\n",
        "\n",
        "With the `transformers` library, we can import a wide range of transformer-based pre-trained models. For the last part of the Modeling section, we will use a pre-trained **Bidirectional Encoder Representations from Transformers** or **BERT** model and fine-tune it for a text classification task. This technique is called **Transfer Learning** wherein a deep learning model trained from a very large dataset is used  “off-the-shelf” to perform similar tasks on another dataset. There are 3 different fine-tuning techniques:\n",
        "\n",
        "* Train the entire architecture which updates all pre-trained weights based on the new dataset\n",
        "* Train the pretrained model partially and freeze the weights of the initial layer and retrain only on the higher levels\n",
        "* Freeze entire architecture and attach a few neural network layers to train the new model.\n",
        "\n",
        "For this tutorial, we will implement the last approach by freezing all the layers of the BERT model and attaching a few layers to train this new model. Note that the weights of only the attached layers will be updated during model training.\n",
        "\n",
        "An example of the ideal and full pipeline using transfer learning with BERT is shown below.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"./img/bert_pipeline.png\" width=\"700\" height=\"320\">\n",
        "<p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is first split into train and test similar to what was used in LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into train and test sets\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(df['text_clean'], df['label.132'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label.132'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Import BERT Model and BERT Tokenizer\n",
        "\n",
        "BERT is trained from BookCorpus (800M words) and English Wikipedia (2.5B words). There are many variants of the BERT model including the `bert-base-uncased`, which has 12 layers, 110M parameters, and trained on lower-cased English texts. Meanwhile, a variant called `bert-large-uncased` has 345M parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# bert-base-uncased\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`legal-bert-base-uncased` is another model variant with 12 layers and 110M parameters. It is pre-trained using 12 GB of diverse English legal text from several fields such as EU legislation, UK legislation, European Court of Justice, USA court cases, and US contracts from EDGAR, the database of US Securities and Exchange Commission.\n",
        "\n",
        "For the purpose of this tutorial, we will use legalBERT given that our documents are DMA public consultations that were written with the context of legal texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# legal-bert-base-uncased\n",
        "bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tokenize the Sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a nutshell, BERT reads the input sequence and generates meaningful text representations, which it feeds into the encoder. This can then be augmented with additional neural network layers to fit a classification task. Unlike other deep learning NLP models, BERT has three embedding layers: token embedding layer, segment embedding layer, and position embedding layer. The element-wise sum of these three layers gives the final input representation.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"./img/bert_embeddings.png\" width=\"250\" height=\"320\">\n",
        "</p>\n",
        "\n",
        "Let's check how the BERT tokenizer works using a sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 532, 261, 2178, 116, 21558, 145, 219, 188, 190, 1955, 102], [101, 532, 261, 2397, 439, 5793, 102, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "text = [\"we will fine-tune a bert model\", \"we will implement transfer learning\"]\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
        "print(sent_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is a dictionary of three items.\n",
        "\n",
        "* input_ids -  token indices, numerical representations of tokens building the sequences that will be used as input by the model\n",
        "* token_type_ids - a binary mask identifying the two types of sequence in the model to do classification on pairs of sentences\n",
        "* attention_mask - a binary tensor indicating the position of the padded indices so that the model does not attend to them\n",
        "\n",
        "More details [here](https://huggingface.co/docs/transformers/glossary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will convert the integer sequences to tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_workers = 2\n",
        "\n",
        "# dataLoader for train set\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_dataloader = DataLoader(train_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# dataLoader for test set\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_dataloader = DataLoader(test_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BERT Model Architecture"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will define a class for our BERT model architecture. As mentioned earlier, we will freeze all layers of the BERT pre-trained model, `legal-bert-base-uncased`, and attach a few neural network layers and a softmax layer in the end to convert the output to probabilities for our binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the method to freeze all layers and keep the weights of the pre-trained BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# method to freeze all the parameters if freeze = T\n",
        "def set_parameter_requires_grad(model, freeze):\n",
        "    if freeze:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_parameter_requires_grad(model=bert, freeze=True)\n",
        "bert_classifier = BERT_Arch(bert)\n",
        "bert_classifier = bert_classifier.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Fine-tuning BERT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we will define our methods to train (fine-tune) and evaluate the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  total_preds=[]\n",
        "  \n",
        "  for inputs in tqdm(dataloader):\n",
        "    \n",
        "    # push to gpu\n",
        "    inputs = [r.to(device) for r in inputs]\n",
        "    sent_id, mask, labels = inputs\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    model.zero_grad()        \n",
        "\n",
        "    # forward + backward + optimize \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = criterion(preds, labels)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()   \n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # epoch loss and model predictions\n",
        "  epoch_loss = total_loss / len(dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return epoch_loss, total_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  total_preds = []\n",
        "\n",
        "  for inputs in tqdm(dataloader):\n",
        "    \n",
        "    # push to gpu\n",
        "    inputs = [t.to(device) for t in inputs]\n",
        "    sent_id, mask, labels = inputs\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = criterion(preds,labels)\n",
        "      total_loss += loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # epoch loss and model predictions\n",
        "  epoch_loss = total_loss / len(dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return epoch_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(model, criterion, train_loader, val_loader, epochs):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    train_losses=[]\n",
        "    valid_losses=[]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "        train_loss, _ = train(model, train_loader, criterion, optimizer)\n",
        "        valid_loss, _ = evaluate(model, val_loader, criterion)\n",
        "        \n",
        "        # save best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), './models/bert_saved_weights.pt')\n",
        "        \n",
        "        # append training and validation loss\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        \n",
        "        print(f\"Train Loss: {train_loss:.2f}\")\n",
        "        print(f\"Validation Loss: {valid_loss:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use AdamW as our optimizer which is an [improved version](https://arxiv.org/abs/1711.05101) of the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "epochs = 20\n",
        "learning_rate = 1e-5\n",
        "\n",
        "optimizer = AdamW(bert_classifier.parameters(), lr = learning_rate)\n",
        "criterion  = nn.NLLLoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 20\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.70it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.70\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 2 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.73it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.69\n",
            "Validation Loss: 0.68\n",
            "\n",
            " Epoch 3 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:04<00:00,  5.21it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:03<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.68\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 4 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  7.36it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 5 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.50it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 6 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.99it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.66\n",
            "Validation Loss: 0.70\n",
            "\n",
            " Epoch 7 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  8.56it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.67\n",
            "Validation Loss: 0.71\n",
            "\n",
            " Epoch 8 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:04<00:00,  5.24it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:04<00:00,  2.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.66\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 9 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.81it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.66\n",
            "Validation Loss: 0.72\n",
            "\n",
            " Epoch 10 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  8.00it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.68\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 11 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  7.83it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.65\n",
            "Validation Loss: 0.72\n",
            "\n",
            " Epoch 12 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:02<00:00,  8.76it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.68\n",
            "Validation Loss: 0.72\n",
            "\n",
            " Epoch 13 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.77it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.67\n",
            "Validation Loss: 0.70\n",
            "\n",
            " Epoch 14 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.57it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.69\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 15 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:04<00:00,  5.60it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.65\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 16 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  6.88it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.67\n",
            "Validation Loss: 0.69\n",
            "\n",
            " Epoch 17 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  8.00it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.68\n",
            "Validation Loss: 0.71\n",
            "\n",
            " Epoch 18 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  7.77it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.64\n",
            "Validation Loss: 0.71\n",
            "\n",
            " Epoch 19 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  7.50it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.68\n",
            "Validation Loss: 0.70\n",
            "\n",
            " Epoch 20 / 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:03<00:00,  8.32it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  7.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.63\n",
            "Validation Loss: 0.70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "fit(bert_classifier, criterion, train_dataloader, test_dataloader, epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Model Evaluation\n",
        "In this section of the tutorial, we compare the results of three types of text classification models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Logistic Regression\n",
        "The baseline logitic regression model, which used TF-IDF weighted GloVe embeddings, gives a good starting point to evaluate how good the final BERT model is. The accuracy of the logistic regression is 57%, with a precision score of 59% and recall score 77%. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.30      0.37        10\n",
            "           1       0.59      0.77      0.67        13\n",
            "\n",
            "    accuracy                           0.57        23\n",
            "   macro avg       0.54      0.53      0.52        23\n",
            "weighted avg       0.55      0.57      0.54        23\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the model from the file\n",
        "with open('./models/logistic.pkl', 'rb') as f:\n",
        "  clf = pickle.load(f)\n",
        "\n",
        "y_pred = clf.predict(l_test_text)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(l_test_labels, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LSTM\n",
        "The LSTM model uses 3 hidden layers, with 80 input features from a 200-dimensional embedding layer. Even without hyperparameter tuning, the accuracy score of the RNN-based model (65%) is higher than that of the logistic regression. Unlike the logistic regression model, the LSTM has a higher precision score (73%) compared to recall (62%). Results were optimized at the 14th out of 20 epochs. After that, the model started overfitting to the training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.70      0.64        10\n",
            "           1       0.73      0.62      0.67        13\n",
            "\n",
            "    accuracy                           0.65        23\n",
            "   macro avg       0.66      0.66      0.65        23\n",
            "weighted avg       0.66      0.65      0.65        23\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load weights of best model\n",
        "path = './models/lstm_saved_weights.pt'\n",
        "lstm_classifier.load_state_dict(torch.load(path))\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = lstm_classifier.forward(X.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(Y, preds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BERT\n",
        "The BERT model performed just as well as the logistic regression and a slightly poorer than LSTM in terms of accuracty. However, it outperformed both models in terms of recall. It correctly classifies as \"agree\" a high percentage of documents whose authors actually agree with the legislation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        10\n",
            "           1       0.57      1.00      0.72        13\n",
            "\n",
            "    accuracy                           0.57        23\n",
            "   macro avg       0.28      0.50      0.36        23\n",
            "weighted avg       0.32      0.57      0.41        23\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/janinedevera/opt/miniconda3/envs/watermelon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "path = './models/bert_saved_weights.pt'\n",
        "bert_classifier.load_state_dict(torch.load(path))\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = bert_classifier(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkmytKNU_Z2"
      },
      "source": [
        "<a name=\"results-and-discussion\"></a>\n",
        "# Results & Discussion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the context of the public consultations of the European Commission, a document clasifier can be used as a pre-screening tool that facilitates an efficient and streamlined review of information from stakeholders. Instead of staff members going through each submission individually, the classifier can categorize the documents according to whether a stakeholder agrees or disagrees with a proposed law. \n",
        "\n",
        "Documents classified as \"**agree**\" can be archived and taken as is, while documents classified as \"**disagree**\" will need to be manually checked by staff for in order to understand the negative sentiment of the stakeholder. These comments and insights will be valuable for improving legislative proposals. \n",
        "\n",
        "With this in mind, it is important to ensure that our document classifier not only makes accurate predictions, but also minimizes false positives and maximizes precision. Out of all documents classified by the algorthim as agreeing with the legislation, the number who *actually* agree should be as high as possible. This way, more valuable information can be integrated into the EC's policy and decision making process. \n",
        "\n",
        "Based on the results of the 3 models, the RNN-based LSTM classifier currently works the best both in terms of accuracy and precision. We note, however, that the results may vary when the models are refined further (as discussed in the **Next Steps** section).\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIjM6eMwlXY"
      },
      "source": [
        "## Limitations\n",
        "\n",
        "This tutorial is focused on education and learning. Its main purpose is to present different models and approaches that can be implemented to classify unstructured documents. The following limitations should be kept in mind: \n",
        "\n",
        "*   The models can satisfactorily classify DMA public consultation documents, but they are not enough to understand how different stakeholders in digital markets respond to the proposal. \n",
        "*   The use of a small dataset yielded results that are not optimal. Thus, the findings of our models cannot be directly used to infer policy implications.\n",
        "* The models are trained on DMA public consultation documents, which likely contain terminologies specific to digital markets. As such, there is no guarantee that the models can generalize to public consultations in other industries.\n",
        "\n",
        "Nonetheless, this tutorial provides a structured project pipeline for classifying unstructured documents using machine learning and NLP-based models, which can be adopted by future researchers to a similar project.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLCQpv14Gsx"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "For future research and studies involving the classification of unstructured document and/or analyzing EU public consultation documents, we recommend the following:\n",
        "1. Data collection\n",
        "*   First, collect more textual data from different public consultations in the EU or any similar dataset. We highlight the importance of using a large enough dataset to train and fine-tune a pre-trained model to reduce overfitting.\n",
        "*   Another limitation of our current data is that our labels initially comprised of three classes but we lumped the answers \"No\" and \"Not applicable\" together and regarded them as not agreeing to the DMA proposal. We advise to rethink this approach and the assess whether the these two responses are different. \n",
        "*   Not all datasets especially unstructured documents come with labels. We highly recommend to allot time to annotating labels as early as possible to answer specific research questions that you have in mind.\n",
        "2. Pre-processing techniques\n",
        "*   Our parsing and pre-processing approach only utilized a simple parsing technique which reads all the texts in a document together. Other NLP methods like document layout analysis can help identify different sections of the document including headings, body, tables, footnotes. This way we can tag or keep the relevant parts or texts to include in the model. \n",
        "* Techniques such as Named Entity Recognition can also be as an advanced pre-processing step. Names of businesses and companies can be taken out of the text that is fed into the algorithms, given that proper nouns will likely not provide any information regarding stakeholder sentiments.\n",
        "*   Image-to-text method for scanned pdfs as well as language translation for non-English documents can be used to avoid reducing the number of the training data.\n",
        "3. Modeling\n",
        "*   The entire architechture of the pre-trained NLP model can be trained to update all pre-trained weights based on the new (large) dataset. \n",
        "*   Moreover, other BERT variants can be utilized but LegalBERT seems to be the most appropriate so far when it comes to training data involving legal texts.\n",
        "*   Another best-practice is to define a validation dataset to conduct hyperparameter tuning and cross validation such as grid search and parameter search.\n",
        "*   Future research can also explore the use of unsupervised machine learning and deep learning models to identify clusters and textual features from the documents. Analysis including topic models such as Latent Dirichlet Analysis, Structural Topic Modeling, and BERTtopic can be conducted to understand the relationship of type of stakeholder (e.g., large companies, MSMEs, public institutions), the proposed bill, and other variables to the position and/or emotions of the stakeholder to a given law."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKqewK_-ZLD"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "# References\n",
        "\n",
        "* Hugging Face LEGAL-BERT https://huggingface.co/nlpaueb/legal-bert-base-uncased\n",
        "* Hugging Face Transformers https://huggingface.co/docs/transformers/glossary\n",
        "* Transfer Learning for NLP: Fine-tuning BERT for Text Classification https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
        "* LSTM for Text Classification https://www.kaggle.com/code/mehmetlaudatekman/lstm-text-classification-pytorch/notebook\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YguJayU8RBmd"
      },
      "source": [
        "## Acknowledgement\n",
        "\n",
        "The transfer learning with BERT tutorial was inspired by Analytics Vidhya tutorial on [Transfer Learning for NLP: Fine-tuning BERT for Text Classification](https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "watermelon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d937f221309e89928364c8f95c616932ec42e1a35308f59658ba617ce90f29fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
